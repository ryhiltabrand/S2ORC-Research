id,title,year,abstract
10.1.1.99.6904,APPLICATION OF ARC IN SYSTEM DESIGN,,"Current and future mobile communication systems require the combination of high performance technology from very diverse engineering disciplines. Consequently, these systems are complex by nature, which complicates system-wide Quality of Service optimisation. In this paper we consider system-wide optimisation of resource distribution within the framework of a given (static) architecture but with components that do support multiple modes of operation. We take into account variable type of applications, quality requirements, resource availability, and external context. In this paper we use the concepts of Adaptive resource contracts (ARC) to capture and exploit the capabilities of individual components in the system. A relevant case study demonstrates the added value of the ARC concepts for system design. Eventually the method yields a specification of individual system components and an indication of the optimised overall QoS. 1"
10.1.1.98.4162,Ten benchmark database queries for location-based services,2003,"Location-based services (l-services for short) compose an emerging application involving spatiotemporal databases. In this paper, we discuss this type of application, in terms of database requirements, and provide a set of ten benchmark database queries (plus two operations for loading and updating data). The list includes selection queries on stationary and moving reference objects, join queries and unary operations on trajectories of moving objects. We also survey recent work in query processing for those query types, with emphasis on indexing of moving objects, and suggest candidates for efficiently supporting databases for l-services."
10.1.1.97.9144,Improving Automatic Indexing through Concept Combination and Term Enrichment,,"Although indexes may overlap, the output of an automatic indexer is generally presented as a flat and unstructured list of terms. Our pur-pose is to exploit term overlap and embed-ding so as to yield a substantial qualitative and quantitative improvement in automatic in-dexing through concept combination. The in-crease in the volume of indexing is 10.5 % for free indexing and 52.3 % for controlled indexing. The resulting structure of the indexed corpus is a partial conceptual analysis. 1"
10.1.1.97.5765,A new approach to real-time checkpointing,2006,"The progress towards programming methodologies that simplify the work of the programmer involves automating, whenever possible, activities that are secondary to the main task of designing algorithms and developing applications. Automatic memory management, using garbage collection, and automatic persistence, using checkpointing, are both examples of mechanisms that operate behind the scenes, simplifying the work of the programmer. Implementing such mechanisms in the presence of real-time constraints, however, is particularly difficult. In this paper we review the behavior of traditional copy-onwrite implementations of checkpointing in the context of real-time systems, and we show how such implementations may, in pathological cases, seriously impair the ability of the user code to meet its deadlines. We discuss the source of the problem, supply benchmarks, and discuss possible remedies. We subsequently propose a novel approach that does not rely on copy-on-write and that, while more expensive in terms of CPU time overhead, is unaffected by pathological user code. We also describe our implementation of the proposed solution, based on the Ovm RTSJ Java Virtual Machine, and we discuss our experimental results."
10.1.1.96.9361,Abstract GenIc: A Single Pass Generalized Incremental Algorithm for Clustering,,In this paper we introduce a new single pass clustering algorithm called GenIc designed with the objective of having low overall cost. We examine some of the properties of GenIc and compare it to windowed k-means. We also study its performance using experimental data sets obtained from network monitoring. 1
10.1.1.96.6534,Climate,2007,change and trace gases
10.1.1.96.4674,Ishikawa & Geiger Mapping Image Restoration to a Graph Problem,,"We propose a graph optimization method for the restoration of gray-scale images. We consider an arbitrary noise model for each pixel location. We also consider a smooth constraint where the potentials between neighbor pixels are convex functionals. We show how to map this problem to a directed flow graph. Then, a global optimal solution is obtained via the use of the maximumflow algorithm. The algorithm runs in a polynomial time with respect to the size of the image. 1."
10.1.1.93.7066,Quincunx fundamental refinable functions and quincunx biorthogonal wavelets,1999,"Abstract. We analyze the approximation and smoothness properties of quincunx fundamental refinable functions. In particular, we provide a general way for the construction of quincunx interpolatory refinement masks associated with the quincunx lattice in R 2. Their corresponding quincunx fundamental refinable functions attain the optimal approximation order and smoothness order. In addition, these examples are minimally supported with symmetry. For two special families of such quincunx interpolatory masks, we prove that their symbols are nonnegative. Finally, a general way of constructing quincunx biorthogonal wavelets is presented. Several examples of quincunx interpolatory masks and quincunx biorthogonal wavelets are explicitly computed. 1."
10.1.1.93.6900,Qualitative Reasoning Group,,"The finite representation of infinite behaviors provided by total envisionments has been shown to be useful for tasks such as model development, explanation, and monitoring. Unfortunately, generating total envisionments of complex systems can be intractable, or at least excessive for highly-focused tasks. Such shortcomings have encouraged some researchers to favor alternative schemes, such as attainable envisioning and history generation. However, we argue in this paper for an incremental means of envisioning that can realizemany ofthe practical advantages oftotal envisioning while supporting more-focused search to address issues of tractability and relevance. In this paper, we describe our theory of incremental envisioning and compare it with QPC [3] and QPE [9]. We emphasize how IQE attempts to reason about states at low (i.e., abstract) levels of detail when that is sufficient for the overall task. Such reasoning helps avoid the inefficiency of making state distinctions that are irrelevant for the overall task- a problemcommonamong most other current qualitative simulators, including QPC and QPE. I"
10.1.1.93.478,Abstract Type-Based Race Detection for Java,,"This paper presents a static race detection analysis for multithreaded Java programs. Our analysis is based on a formal type system that is capable of capturing many common synchronization patterns. These patterns include classes with internal synchronization, classes that require client-side synchronization, and thread-local classes. Experience checking over 40,000 lines of Java code with the type system demonstrates that it is an effective approach for eliminating races conditions. On large examples, fewer than 20 additional type annotations per 1000 lines of code were required by the type checker, and we found a number of races in the standard Java libraries and other test programs. 1"
10.1.1.93.4031,The Bulk Index Join: A Generic Approach to Processing Non-Equijoins,1999,"Efficient join algorithms have been developed for processing different types of non-equijoins like spatial join, band join, temporal join or similarity join. Each of these previously proposed join algorithms is tailor-cut for a specific type of join, and a generalization of these algorithms to other join types is not obvious. We present an efficient algorithm called bulk index join that can be easily applied to a broad class of non-equijoins. Similar to the well-known hash join algorithms, the bulk index join performs in two phases. In the build-phase, an appropriate index structure is created that serves as a partitioning function on the first relation. In the probing-phase, the records of the second relation are probed against the first relation by using the index structure of the build-phase. In order to support both phases efficiently, we adopt a technique recently proposed for bulk loading index structures. We show that this technique can also be exploited for probing the tuples of the second relation in bulk. Similar to the generic bulk loading approach, only a predefined set of routines of the index structure is used for implementing our join algorithm. This set is generally available in tree-based index structures. The so-called band join serves as an example in this paper. We first discuss in detail how to apply our generic approach to the band join. Thereafter, we present a worst-case analysis and experimental results. Moreover, we show in our experiments that the well-known index nested loops join can benefit from performing queries in bulk as it is proposed for the probing-phase of the bulk index join. 1"
10.1.1.90.6865,Abstract,2003,"In this paper we present a sublinear time (1+ɛ)-approximation randomized algorithm to estimate the weight of the minimum spanning tree of an n-point metric space. The running time of the algorithm is � O(n/ɛ O(1)). Since the full description of an n-point metric space is of size Θ(n 2), the complexity of our algorithm is sublinear with respect to the input size. Our algorithm is almost optimal as it is not possible to approximate in o(n) time the weight of the minimum spanning tree to within any factor. Furthermore, it has been previously shown that no o(n 2) algorithm exists that returns a spanning tree whose weight is within a constant times the optimum."
10.1.1.89.9207,Write Markers for Probabilistic Quorum Systems,2007,"Probabilistic quorum systems can tolerate a larger fraction of faults than can traditional (strict) quorum systems, while guaranteeing consistency with an arbitrarily high probability for a system with enough replicas. However, they are hampered in that, like strict quorum systems, they allow for Byzantine-faulty servers to collude maximally to provide incorrect values to clients. We present a technique based on write markers that prevents faulty servers from colluding unless they are all also selected to be participants in the same update operations. We show that write markers increase the maximum fraction of faults that can be tolerated to b < n/2 from b < n/2.62, where n is the total number of replicas, for probabilistic masking quorum systems (compared with b < n/4 for strict masking quorum systems) and to b < n/2.62 from b < n/3.15 for probabilistic opaque quorum systems (compared with b < n/5 for strict opaque quorum systems). In addition, with write markers, probabilistic masking quorums no longer require write quorums of large or maximal size in order to tolerate the maximum fraction of faults. We describe an implementation of write markers that is effective even if Byzantine clients collude with faulty servers. This work was partially supported by NSF grant CCF-0424422."
10.1.1.89.6320,Speculative execution and instruction-level parallelism,1994,"research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,"
10.1.1.88.1359,Write Markers for Probabilistic Quorum Systems,2007,"Probabilistic quorum systems can tolerate a larger fraction of faults than can traditional (strict) quorum systems, while guaranteeing consistency with an arbitrarily high probability for a system with enough replicas. However, they are hampered in that, like strict quorum systems, they allow for Byzantine-faulty servers to collude maximally to provide incorrect values to clients. We present a technique based on write markers that prevents faulty servers from colluding unless they are all also selected to be participants in the same update operations. We show that write markers increase the maximum fraction of faults that can be tolerated to b < n/2 from b < n/2.62, where n is the total number of replicas, for probabilistic masking quorum systems (compared with b < n/4 for strict masking quorum systems) and to b < n/2.62 from b < n/3.15 for probabilistic opaque quorum systems (compared with b < n/5 for strict opaque quorum systems). In addition, with write markers, probabilistic masking quorums no longer require write quorums of large or maximal size in order to tolerate the maximum fraction of faults. We describe an implementation of write markers that is effective even if Byzantine clients collude with faulty servers. This work was partially supported by NSF grant CCF-0424422."
10.1.1.87.9361,Chapter 7 Depth estimation via sampling,2008,"“Maybe the Nazis told the truth about us. Maybe the Nazis were the truth. We shouldn’t forget that: perhaps they were the truth. The rest, just beautiful lies. We’ve sung many beautiful lies about ourselves. Perhaps that’s what I’m trying to do- to sing another beautiful lie.” – –The roots of heaven, Romain Gary In this chapter, we introduce a “trivial ” but yet powerful idea. Given a set S of objects, a point p that is contained in some of the objects, and let its weight be the number of objects that contains it. We can estimate the depth/weight of p by counting the number of objects that contains it in a random sample of the objects. In fact, by considering points induced by the sample, we can bound the number of “light ” vertices induced by S. This idea can be extended to bounding the number of “light ” configurations induced by a set of objects. This approach leads to a sequence of short, beautiful, elegant and correct  proofs of several hallmark results in discrete geometry. While the results in this chapter are not directly related to approximation algorithms, the insights and general approach would be useful for us later, or so one hopes. 7.1 The at most k-levels Let L be a set of n lines in the plane. A point p ∈  � ℓ∈L ℓ is of level k, if there are k lines of L strictly below it. The k-level is the closure of set of points of level k. Namely, the k-level is an x-monotone curve along the lines of L."
10.1.1.87.808,Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications,2001,"Efficiently determining the node that stores a data item in a distributed network is an important and challenging problem. This paper describes the motivation and design of the Chord system, a decentralized lookup service that stores key/value pairs for such networks. The Chord protocol takes as input an ¡-bit identifier (derived by hashing a higher-level applicationspecific key), and returns the node that stores the value corresponding to the key. Each Chord node is identified by an ¡-bit identifier and each node stores the key identifiers in the system closest to the node’s identifier. Each node maintains an ¡-entry routing table that allows it to look up keys efficiently. Results from theoretical analysis, simulations, and experiments show that Chord is incrementally scalable, with insertion and lookup costs scaling logarithmically with the number of Chord nodes. 1"
10.1.1.87.7740,Is MPI Suitable for a Generative Design-Pattern System?,,"Generative parallel design patterns is a proven technique to improve the productivity of parallel program development. However many of the generative design-pattern systems are developed for target languages that are not widely used by the high performance computing community. This paper describes an initial effort to develop a system that will hopefully answer the question in the title in the affirmative. This new system is ostensively based on, and build upon the experience with, the successful CO2P3S. Significant challenges must be overcome to implement the features of a system that generates frameworks conceived for an object-oriented programming language (Java) into a parallel-annotated procedural language (MPI/C). 1"
10.1.1.87.2959,Graph kernels versus graph representations: a case study in parse ranking,,"Abstract. Recently, several kernel functions designed for a data that consists of graphs have been presented. In this paper, we concentrate on designing graph representations and adapting the kernels for these graphs. In particular, we propose graph representations for dependency parses and analyse the applicability of several variations of the graph kernels for the problem of parse ranking in the domain of biomedical texts. The parses used in the study are generated with the link grammar (LG) parser from annotated sentences of BioInfer corpus. The results indicate that designing the graph representation is as important as designing the kernel function that is used as the similarity measure of the graphs. 1"
10.1.1.86.6095,and Reasoning about Programs,,"In this paper, we present a new algorithm for partial program verification that runs in polynomial time and space. We are interested in checking that a program satisfies a given temporal safety property. Our insight is that by accurately modeling only those branches in a program for which the property-related behavior differs along the arms of the branch, we can design an algorithm that is accurate enough to verify the program with respect to the given property, without paying the potentially exponential cost of full pathsensitive analysis. We have implemented this “property simulation ” algorithm as part of a partial verification tool called ESP. We present the results of applying ESP to the problem of verifying the file I/O behavior of a version of the GNU C compiler (gcc, 140,000 LOC). We are able to prove that all of the 646 calls to fprintf in the source code of gcc are guaranteed to print to valid, open files. Our results show that property simulation scales to large programs and is accurate enough to verify meaningful properties."
10.1.1.86.5219,The analytic theory of matrix orthogonal polynomials,2008,We survey the analytic theory of matrix orthogonal polynomials.
10.1.1.85.9375,Detecting Causal Relationships in Distributed Computations,1994,"Abstract: The paper shows that characterizing the causal relationship between significant events is an important but non-trivial aspect for understanding the behavior of distributed programs. An introduction to the notion of causality and its relation to logical time is given; some fundamental results concerning the characterization of causality are presented. Recent work on the detection of causal relationships in distributed computations is surveyed. The relative merits and limitations of the different approaches are discussed, and their general feasibility is analyzed."
10.1.1.85.5694,Joint nonparametric alignment for analyzing spatial gene expression patterns in drosophila imaginal discs,,"To compare spatial patterns of gene expression, one must analyze a large number of images as current methods are only able to measure a small number of genes at a time. Bringing images of corresponding tissues into alignment is a critical first step in making a meaningful comparative analysis of these spatial patterns. Significant image noise and variability in the shapes make it hard to pick a canonical shape model. In this paper, we address these problems by combining segmentation and unsupervised shape learning algorithms. We first segment images to acquire structures of interest, then jointly align the shapes of these acquired structures using an unsupervised nonparametric maximum likelihood algorithm along the lines of ‘congealing’ [12], while simultaneously learning the underlying shape model and associated transformations. The learned transformations are applied to corresponding images to bring them into alignment in one step. We demonstrate the results for images of various classes of Drosophila imaginal discs and discuss the methodology used for a quantitative analysis of spatial gene expression patterns. 1."
10.1.1.84.9931,"2 Microsoft Research,",,"Abstract. Many approaches to object recognition are founded on probability theory, and can be broadly characterized as either generative or discriminative according to whether or not the distribution of the image features is modelled. Generative and discriminative methods have very different characteristics, as well as complementary strengths and weaknesses. In this chapter we introduce new generative and discriminative models for object detection and classification based on weakly labelled training data. We use these models to illustrate the relative merits of the two approaches in the context of a data set of widely varying images of non-rigid objects (animals). Our results support the assertion that neither approach alone will be sufficient for large scale object recognition, and we discuss techniques for combining the strengths of generative and discriminative approaches. 1"
10.1.1.84.497,Pacific Symposium on Biocomputing 8:565-576(2003) FUNCTIONAL DISCRIMINATION OF GENE EXPRESSION PATTERNS IN TERMS OF THE GENE ONTOLOGY,,"The ever-growing amount of experimental data in molecular biology and genetics requires its automated analysis, by employing sophisticated knowledge discovery tools. We use an Inductive Logic Programming (ILP) learner to induce functional discrimination rules between genes studied using microarrays and found to be differentially expressed in three recently discovered subtypes of adenocarcinoma of the lung. The discrimination rules involve functional annotations from the Proteome HumanPSD database in terms of the Gene Ontology, whose hierarchical structure is essential for this task. While most of the lower levels of gene expression data (pre)processing have been automated, our work can be seen as a step toward automating the higher level functional analysis of the data. We view our application not just as a prototypical example of applying more sophisticated machine learning techniques to the functional analysis of genes, but also as an incentive for developing increasingly more sophisticated functional annotations and ontologies, that can be automatically processed by such learning algorithms. 1 Introduction and"
10.1.1.83.5547,An RPC mechanism for transportable agents,1996,"Transportable agents are autonomous programs that migrate from machine to machine, performing complex processing at each step to satisfy client requests. As part of their duties agents often need to communicate with other agents. We propose to use remote procedure call (RPC) along with a exible interface de nition language (IDL), to add structure to inter-agent communication. The real power of our Agent RPC comes from a client-server binding mechanism based on exible IDL matching and from support for multiple simultaneous bindings. Our agents are programmed in Agent Tcl�we describe how the Tcl implementation made RPC particularly easy to implement. Finally, although our RPC is designed for Agent Tcl programs, the concepts would also work for standard Tcl programs."
10.1.1.81.7810,BUREAU OF ECONOMICS,2004,"The authors are economists in the Bureau of Economics, Federal Trade Commission. The views expressed in this report are those of the authors and do not necessarily represent the views of the Federal Trade Commission or any individual Commissioner. ii ACKNOWLEDGMENTS The authors greatly thank Kata Mihaly who contributed tremendously to the data management, computer programming, and statistical analysis for this study. The authors also thank everyone who provided helpful suggestions on the study design, valuable assistance in the data analysis, or thoughtful comments on the"
10.1.1.81.6808,Managing Change in Information Systems: Technological Challenges,,"Abstract: Information systems and other computer-based systems must continuously undergo change in order to reflect change in their environments. The present technology used to implement such systems, including models, methods, tools and languages, does not have an inherent understanding of the nature of evolution. The rigidity of existing systems is a hindrance for user requested enhancements. Propagating changes correctly is a particular problem. It is common to find that necessary changes consequent on some other change have not been made, so that the system is inconsistent and will eventually fail to operate correctly. The paper discusses tools for system maintenance and focuses on the issue of automation. A tool that automatically generates and maintains all the information it needs is presented. To provide more information about the form and extent of the evolution in real-world systems, the same tool was instructed to collect change measurements. Information about the evolution of a large health management system was recorded over a period of 18 months. Methods for and problems of automatic change measurements collection are discussed. 1"
10.1.1.81.6408,Abstract,2006,"In this paper we use a computer model of social capital to explore and develop a computational social theory of the anthropological or interpretive notion of the sub jectivity (Ortner 2005). The cultural sub jectivity is a social theory of the reflexive actor that is historically situated in a material and cultural context. It is important for computational social science to begin developing tools to represent the dimensions of the actor in terms of the sub jectivity because it recognizes and seeks to explain the complexity of human feelings and fears in creating meaning and in taking action. Theories of sub jectivity also offer a model of human action and play that is neither universal nor individual. Developing a computational social theory of the sub jectivity is an impossible and improbable task, however, in seeking to develop this theory in code we might be able to posit clearer questions in social research regarding the sub jectivity and explore the operational limits of computational social science, which is a theory in itself. Also accounting for how complex structures of feeling and experienceanxieties, fears, dreams, hopes and the likedevelop as a component inside complex social organizations, ones that can be rigorously explored with computational methods, a foundation can be created for a critical computational social science. Contact:"
10.1.1.81.4430,TOPOLOGICAL CONJUGACY BETWEEN APERIODIC TILING DYNAMICAL SYSTEMS,,We extend to certain aperiodic tiling dynamical systems associated with an underlying substitution a geometric invariant for topological conjugacy.
10.1.1.81.3129,Using phrasal patterns to identify discourse relations,2006,"This paper describes a system which identifies discourse relations between two successive sentences in Japanese. On top of the lexical information previously proposed, we used phrasal pattern information. Adding phrasal information improves the system's accuracy 12%, from 53 % to 65%. 1"
10.1.1.81.3109,Contents,1995,"Printed copies available from the following: The use of trademarks or names of manufacturers in this report is for accurate reporting and does not constitute an official endorsement, either expressed or implied, of such products or manufacturers by the National Aeronautics and Space Administration. Arvind R. Prabhu, Analytical Services & Materials, Inc., Hampton, Virginia, has also contributed to the development of the Thermal Imaging Application. Work was supported under contracts NAS1-"
10.1.1.81.1567,Non-Leftmost Unfolding in Partial Evaluation of Logic Programs with Impure Predicates,2006,"Abstract. Partial evaluation of logic programs which contain impure predicates poses non-trivial challenges. Impure predicates include those which produce side-effects, raise errors (or exceptions), and those whose truth value varies according to the degree of instantiation of arguments 4. In particular, non-leftmost unfolding steps can produce incorrect results since the independence of the computation rule no longer holds in the presence of impure predicates. Existing proposals allow non-leftmost unfolding steps, but at the cost of accuracy: bindings and failure are not propagated backwards to predicates which are potentially impure. In this work we propose a partial evaluation scheme which substantially reduces the situations in which such backpropagation has to be avoided. With this aim, our partial evaluator takes into account the information about purity of predicates expressed in terms of assertions. This allows achieving some optimizations which are not feasible using existing partial evaluation techniques. We argue that our proposal goes beyond existing ones in that it is a) accurate, since the classification of pure vs impure is done at the level of atoms instead of predicates, b) extensible, as the information about purity can be added to programs using assertions without having to modify the partial evaluator itself, and c) automatic, since (backwards) analysis can be used to automatically infer the required assertions. Our approach has been implemented in the context of CiaoPP, the abstract interpretation-based preprocessor of the Ciao logic programming system. 1"
10.1.1.81.1247,'One is a Lonely Number': on the logic of communication,,"Abstract Logic is not just about single-agent notions like reasoning, or zero-agent notions like truth, but also about communication between two or more people. What we tell and ask each other can be just as 'logical ' as what we infer in Olympic solitude. We show how such interactive phenomena can be studied systematically by merging epistemic and dynamic logic. 1 Logic in a social setting 1.1 Questions and answers Consider the simplest type of communication: a question–answer episode between two agents. Here is a typical example. Being a Batavian soldier – a German tribe in the Rhine delta of proverbial valour – I approach you in a busy Roman street, A.D. 160, intent on contacting my revered general Maximus, now a captive, and ask: 1 Q Is this the road to the Colosseum? As a well-informed and helpful Roman citizen, you answer A Yes. This is the sort of thing that we all do competently millions of times in our lives."
10.1.1.80.6347,MEAD - a platform for multidocument multilingual text summarization,2004,"This paper describes the functionality of MEAD, a comprehensive, public domain, open source, multidocument multilingual summarization environment that has been thus far downloaded by more than 500 organizations. MEAD has been used in a variety of summarization applications ranging from summarization for mobile devices to Web page summarization within a search engine and to novelty detection. 1."
10.1.1.80.4504,Fundamental Theorem of Algebra 1 MML Identifier:POLYNOM5.,,"provide the notation and terminology for this paper. One can prove the following propositions: 1. PRELIMINARIES (1) For all natural numbers n, m such that n � = 0 and m � = 0 holds (n · m − n − m)+1 ≥ 0. (2) For all real numbers x, y such that y> 0 holds min(x,y) max(x,y)  ≤ 1. (3) For all real numbers x, y such that for every real number c such that c> 0 and c < 1 holds c · x ≥ y holds y ≤ 0. (4) Let p be a finite sequence of elements of R. Suppose that for every natural number n such that n ∈ dom p holds p(n)  ≥ 0. Let i be a natural number. If i ∈ dom p, then ∑ p ≥ p(i). (5) For all real numbers x, y holds −(x+yiCF)  = −x+(−y)iCF. (6) For all real numbers x1, y1, x2, y2 holds (x1 + y1iCF)  − (x2 + y2iCF)  = (x1 − x2) + (y1 − y2)iCF. In this article we present several logical schemes. The scheme ExDHGrStrSeq deals with a non empty groupoidA and a unary functorF yielding an element ofA, and states that: There exists a sequence S ofA such that for every natural number n holds S(n) = F (n) for all values of the parameters. The scheme ExDdoubleLoopStrSeq deals with a non empty double loop structureA and a unary functorF yielding an element ofA, and states that: There exists a sequence S ofA such that for every natural number n holds S(n) = F (n) for all values of the parameters. The following proposition is true (8) 1 For every element z of CF such that z � = 0CF |power CF (z, n) |  = |z|n."
10.1.1.80.2858,Attacks on Bresson-Chevassut-Essiari-Pointcheval’s Group Key Agreement Scheme for Low-Power Mobile Devices. Available at http://eprint.iacr.org/2004/251,2004,"Abstract. In this paper, we show that Bresson-Chevassut-Essiari-Pointcheval’s group key agreement scheme does not meet the main security properties: implicit key authentication, forward secrecy, and known key security. Also, we propose an improved version which fixes the security flaws found in the scheme."
10.1.1.8.817,Predictive Models for the Breeder Genetic Algorithm I. Continuous Parameter Optimization,1993,In this paper a new genetic algorithm called the Breeder Genetic Algorithm  (BGA) is introduced. The BGA is based on artificial selection  similar to that used by human breeders. A predictive model for the BGA is  presented which is derived from quantitative genetics. The model is used to  predict the behavior of the BGA for simple test functions. Different mutation  schemes are compared by computing the expected progress to the solution.
10.1.1.79.838,Optimistic Active Learning using Mutual Information,,"An “active learning system ” will sequentially decide which unlabeled instance to label, with the goal of efficiently gathering the information necessary to produce a good classifier. Some such systems greedily select the next instance based only on properties of that instance and the few currently labeled points — e.g., selecting the one closest to the current classification boundary. Unfortunately, these approaches ignore the valuable information contained in the other unlabeled instances, which can help identify a good classifier much faster. For the previous approaches that do exploit this unlabeled data, this information is mostly used in a conservative way. One common property of the approaches in the literature is that the active learner sticks to one single query selection criterion in the whole process. We propose a system, MM+M, that selects the query instance that is able to provide the maximum conditional mutual information about the labels of the unlabeled instances, given the labeled data, in an optimistic way. This approach implicitly exploits the discriminative partition information contained in the unlabeled data. Instead of using one selection criterion, MM+M also employs a simple on-line method that changes its selection rule when it encounters an “unexpected label”. Our empirical results demonstrate that this new approach works effectively. 1"
10.1.1.79.7931,Abstract Automated Hoarding for Mobile Computers,,"common problem facing mobile computing is disconnected operation, or computing in the absence of a network. Hoarding eases disconnected operation by selecting a subset of the user’s files for local storage. We describe a hoarding system that can operate without user intervention, by observing user activity and predicting future needs. The system calculates a new measure, semantic distance, between individual files, and uses this to feed a clustering algorithm that chooses which files should be hoarded. A separate replication system manages the actual transport of data; any of a number of replication systems may be used. We discuss practical problems encountered in the real world and present usage statistics showing that our system outperforms previous approaches by factors that can exceed 10:1."
10.1.1.79.6953,BUREAU OF ECONOMICS,2004,"The authors are economists in the Bureau of Economics, Federal Trade Commission. The views expressed in this report are those of the authors and do not necessarily represent the views of the Federal Trade Commission or any individual Commissioner. ii ACKNOWLEDGMENTS The authors greatly thank Kata Mihaly who contributed tremendously to the data management, computer programming, and statistical analysis for this study. The authors also thank everyone who provided helpful suggestions on the study design, valuable assistance in the data analysis, or thoughtful comments on the"
10.1.1.79.46,Design for a decentralized security system for network-attached storage,2000,"This paper describes an architecture for a secure file system based on network-attached storage that guarantees end-to-end encryption for all user data. We describe the design of this system, focusing on the features that allow it to ensure that data is written and read only by authorized users, even in the face of attacks such as network snooping and physically capturing the storage media. Our work shows that such a system is feasible given the speeds of today’s microprocessors, and we discuss benchmark results using several popular encryption and authentication algorithms that could be used on storage devices in such a system. Based on these calculations, we present the overall performance of the system, showing that it is nearly as fast as the non-encrypted file systems in wide use today."
10.1.1.79.4446,Symbolic Computational Techniques for Solving Games,,Software Tools for Technology Transfer manuscript No. (will be inserted by the editor)
10.1.1.78.9124,"Mobility Helps Peer-to-Peer Security Srdjan Capkun, Member, IEEE Computer Society,",,"Abstract—We propose a straightforward technique to provide peer-to-peer security in mobile networks. We show that far from being a hurdle, mobility can be exploited to set up security associations among users. We leverage on the temporary vicinity of users, during which appropriate cryptographic protocols are run. We illustrate the operation of the solution in two scenarios, both in the framework of mobile ad hoc networks. In the first scenario, we assume the presence of an offline certification authority and we show how mobility helps to set up security associations for secure routing; in this case, the security protocol runs over one-hop radio links. We further show that mobility can be used for the periodic renewal of vital security information (e.g., the distribution of hash chain/Merkle tree roots). In the second scenario, we consider fully self-organized security: Users authenticate each other by visual contact and by the activation of an appropriate secure side channel of their personal device; we show that the process can be fuelled by taking advantage of trusted acquaintances. We then show that the proposed solution is generic: It can be deployed on any mobile network and it can be implemented either with symmetric or with asymmetric cryptography. We provide a performance analysis by studying the behavior of the solution in various scenarios. Index Terms—Mobile ad hoc networks, network-level security and protection. 1"
10.1.1.78.833,Knowledge-level reflection,1992,"This paper presents an overview of the reflect project. It defines the notion of knowledge level reflection that has been central to the project, it compares this notion with existing approaches to reflection in related fields, and investigates some of the consequences of the concept of knowledge level reflection: what is a general architecture for knowledge level reflection, how to model the object component in such an architecture, what is the nature of reflective theories, how can we design such architectures, and what are the results of our actual experiments with such systems? Keywords: Knowledge-based systems, reflection, meta-reasoning."
10.1.1.78.7325,Managing Deadline Miss Ratio and Sensor Data Freshness in Real-Time Databases,2004," The demand for real-time data services is increasing in many applications including e-commerce, agile manufacturing, and telecommunications network management. In these applications, it is desirable to execute transactions within their deadlines, i.e., before the real-world status changes, using fresh (temporally consistent) data. However, meeting these fundamental requirements is challenging due to dynamic workloads and data access patterns in these applications. Further, transaction timeliness and data freshness requirements may conflict. In this paper, we define average/transient deadline miss ratio and new data freshness metrics to let a database administrator specify the desired quality of real-time data services for a specific application. We also present a novel QoS management architecture for real-time databases to support the desired QoS even in the presence of unpredictable workloads and access patterns. To prevent overload and support the desired QoS, the presented architecture applies feedback control, admission control, and flexible freshness management schemes. A simulation study shows that our QoS-aware approach can achieve a near zero miss ratio and perfect freshness, meeting basic requirements for real-time transaction processing. In contrast, baseline approaches fail to support the desired miss ratio and/or freshness in the presence of unpredictable workloads and data access patterns.  "
10.1.1.78.6911,Describing groups,,"Abstract. Two ways of describing a group are considered. 1. A group is finiteautomaton presentable if its elements can be represented by strings over a finite alphabet, in such a way that the set of representing strings and the group operation can be recognized by finite automata. 2. An infinite f.g. group is quasi-finitely axiomatizable if there is a description consisting of a single first-order sentence, together with the information that the group is finitely generated. In the first part of the paper we survey examples of FA-presentable groups, but also discuss theorems restricting this class. In the second part, we give examples of quasi-finitely axiomatizable groups, consider the algebraic content of the notion, and compare it to the notion of a group which is a prime model. We also show that if a structure is bi-interpretable in parameters with the ring of integers, then it is prime and quasi-finitely axiomatizable. Contents"
10.1.1.78.6412,On the Group of Inner Automorphisms MML Identifier:AUTGROUP.,,"the notation and terminology for this paper. For simplicity, we use the following convention: G denotes a strict group, H denotes a subgroup of G, a, b, x denote elements of G, and h denotes a homomorphism from G to G. We now state the proposition (1) For all a, b such that b is an element of H holds b a ∈ H iff H is normal. Let us consider G. The functor Aut(G) yielding a non empty set of functions from the carrier of G to the carrier of G is defined as follows: (Def. 1) Every element of Aut(G) is a homomorphism from G to G and for every h holds h ∈ Aut(G) iff h is one-to-one and an epimorphism. We now state several propositions: (3) 1 Aut(G)  ⊆ (the carrier of G) the carrier of G. (4) idthe carrier of G is an element of Aut(G). (5) For every h holds h ∈ Aut(G) iff h is an isomorphism. (6) For every element f of Aut(G) holds f −1 is a homomorphism from G to G. (7) For every element f of Aut(G) holds f −1 is an element of Aut(G)."
10.1.1.78.1488,Design of large-scale polylingual systems,2004,"Abstract. Building systems from existing applications written in two or more languages is common practice. Such systems are polylingual. Polylingual systems are relatively easy to build when the number of APIs needed to achieve language interoperability is small. However, when the number of distinct APIs become large, maintaining and evolving polylingual systems becomes a notoriously difficult task. In this paper, we present a simple, practical, and effective way to develop, maintain, and evolve large-scale polylingual systems. Our approach relies on recursive type systems whose instances can be manipulated by reflection. Foreign objects (i.e. objects that are not defined in a host programming language) are abstracted as graphs and path expressions are used for accessing and manipulating data. Path expressions are implemented by type reification — turning foreign type instances into first-class objects and enabling access to and manipulation of them in a host programming language. Doing this results in multiple benefits, including coding simplicity and uniformity that we demonstrate in a complex commercial project. 1"
10.1.1.77.8783,Trading Memory for Randomness \Lambda,,"Abstract Strategies in repeated games can be classified as towhether or not they use memory and/or randomization. We consider Markov decision processes and 2-player graphgames, both of the deterministic and probabilistic varieties. We characterize when memory and/or randomization arerequired for winning with respect to various classes of!-regular objectives, noting particularly when the use of memory can be traded for the use of randomization. In partic-ular, we show that Markov decision processes allow randomized memoryless optimal strategies for all M&quot;uller ob-jectives. Furthermore, we show that 2-player probabilistic graph games allow randomized memoryless strategies forwinning with probability 1 those M&quot;uller objectives which are upward-closed. Upward-closure means that if a set ff ofinfinitely repeating vertices is winning, then all supersets of ff are also winning. 1"
10.1.1.77.5470,Fingerprint classification by combination of flat and structural approaches,2001,"Abstract. This paper investigates the advantages of the combination of flat and structural approaches for fingerprint classification. A novel structural classification method is described and compared with the “multichannel ” flat method recently proposed by Jain et al. [1]. Performances and complementarity of the two methods are evaluated using NIST-4 Database. A simple approach based on the concept of “metaclassification ” is proposed for the combination of the two fingerprint classification methods. Reported results point out the potential advantages of the combination of flat and structural fingerprint-classification approaches. In particular, such results show that the exploitation of structural information allows increasing classification performances. 1."
10.1.1.77.2480,Genetic Process Mining: A Basic Approach and its Challenges,2006,"Abstract. One of the aims of process mining is to retrieve a process model from a given event log. However, current techniques have problems when mining processes that contain non-trivial constructs and/or when dealing with the presence of noise in the logs. To overcome these problems, we try to use genetic algorithms to mine process models. The non-trivial constructs are tackled by choosing an internal representation that supports them. The noise problem is naturally tackled by the genetic algorithm because, per definition, these algorithms are robust to noise. The definition of a good fitness measure is the most critical challenge in a genetic approach. This paper presents the current status of our research and the pros and cons of the fitness measure that we used so far. Experiments show that the fitness measure leads to the mining of process models that can reproduce all the behavior in the log, but these mined models may also allow for extra behavior. In short, the current version of the genetic algorithm can already be used to mine process models, but future research is necessary to always ensure that the mined models do not allow for extra behavior. Thus, this paper also discusses some ideas for future research that could ensure that the mined models will always only reflect the behavior in the log."
10.1.1.76.5251,Creditflow -controlled ATM for MP interconnection: the ATLAS I single-chip ATM switch,1998,"Multiprocessing (MP) on networks of workstations (NOW) is a high-performance computing architecture ofgrowing importance. Intraditional MP’s, wormhole routing interconnection networks use fixed-size flits and backpressure. In NOW’s, ATM −one of the major contending interconnection technologies − uses fixed-size cells, while backpressure can be added to it. We argue that ATM with backpressure has interesting similarities with wormhole routing. Weare implementing ATLAS I, a single-chip gigabit ATM switch, which includes credit flow control (backpressure), according to a protocol resembling Quantum Flow Control (QFC). We show by simulation that this protocol performs better than the traditional multi-lane wormhole protocol: high throughput and low latency are pro vided with less buffer space. Also, ATLAS I demonstrates little sensitivity to bursty traffic, and, unlike wormhole, itisfair in terms of latency in hotspot configurations. We use detailed switch models, operating at clock-cycle granularity."
10.1.1.76.2965,On the Group of Inner Automorphisms MML Identifier: AUTGROUP.,,"the notation and terminology for this paper. For simplicity, we use the following convention: G denotes a strict group, H denotes a subgroup of G, a, b, x denote elements of G, and h denotes a homomorphism from G to G. We now state the proposition (1) For all a, b such that b is an element of H holds b a ∈ H iff H is normal. Let us consider G. The functor Aut(G) yielding a non empty set of functions from the carrier of G to the carrier of G is defined as follows: (Def. 1) Every element of Aut(G) is a homomorphism from G to G and for every h holds h ∈ Aut(G) iff h is one-to-one and an epimorphism. We now state several propositions: (3) 1 Aut(G)  ⊆ (the carrier of G) the carrier of G. (4) idthe carrier of G is an element of Aut(G). (5) For every h holds h ∈ Aut(G) iff h is an isomorphism. (6) For every element f of Aut(G) holds f −1 is a homomorphism from G to G. (7) For every element f of Aut(G) holds f −1 is an element of Aut(G)."
10.1.1.76.2175,Paravirtualization for HPC Systems,2006,"  In this work, we investigate the efficacy of using paravirtualizing software for performance-critical HPC kernels and applications. We present a comprehensive performance evaluation of Xen, a low-overhead, Linux-based, virtual machine monitor, for paravirtualization of HPC cluster systems at LLNL. We investigate subsystem and overall performance using a wide range of benchmarks and applications. We employ statistically sound methods to compare the performance of a paravirtualized kernel against three Linux operating systems: RedHat Enterprise 4 for build versions 2.6.9 and 2.6.12 and the LLNL CHAOS kernel. Our results indicate that Xen is very efficient and practical for HPC systems.  "
10.1.1.75.4229,ContextContacts: re-designing Smartphone’s contact book to support mobile awareness and collaboration,2005,"Acontextuality of the mobile phone often leads to a caller’s uncertainty over a callee’s current state, which in turn often hampers mobile collaboration. We are interested in re-designing a Smartphone’s contact book to provide cues of the current situations of others. ContextContacts presents several meaningful, automatically communicated situation cues of trusted others. Its interaction design follows social psychological findings on how people make social attributions based on impoverished cues, on how self-disclosure of cues is progressively and interactionally managed, and on how mobility affects interaction through cues. We argue how our design choices support mobile communication decisions and group coordinations by promoting awareness. As a result, the design is very minimal and integrated, in an “unremarkable ” manner, to previously learned usage patterns with the phone. First laboratory and field evaluations indicate important boundary conditions for and promising avenues toward more useful and enjoyable mobile awareness applications."
10.1.1.75.3023,L. Stephen Young G-DIMENSIONAL THEORY THE SMARANDACHE QUANTUM PARADOXES: Comparative Logic and Modern Quantum Theory Table 1A. Elliptic Parameters of S'. {θ = arcsin(v)},,S ' a b f1 v a / b σe a1 a2 cos(θ) (4) sec(θ) (4) cos 2 (θ) (4) 1 (4) sin(θ)cos(θ) (5.1) tan(θ) (5.1) sin(θ) (2) sec(θ) (3.1)
10.1.1.74.4531,A Component- and Message-Based Architectural Style for GUI Software,,"Abstract-- While a large fraction of application code is devoted to graphical user interface (GUI) functions, support for reuse in this domain has largely been confined to the creation of GUI toolkits (“widgets”). We present a novel architectural style directed at supporting larger grain reuse and flexible system composition. Moreover, the style supports design of distributed, concurrent applications. Asynchronous notification messages and asynchronous request messages are the sole basis for inter-component communication. A key aspect of the style is that components are not built with any dependencies on what typically would be considered lower-level components, such as user interface toolkits. Indeed, all components are oblivious to the existence of any components to which notification messages are sent. While our focus has been on applications involving graphical user interfaces, the style has the potential for broader applicability. Several trial applications using the style are described. 1 Index Terms-- architectural styles, message-based architectures, graphical user interfaces (GUIs), heterogeneity, concurrency. I."
10.1.1.74.3296,Abstract,2006,"In this paper we use a computer model of social capital to explore and develop a computational social theory of the anthropological or interpretive notion of the sub jectivity (Ortner 2005). The cultural sub jectivity is a social theory of the reflexive actor that is historically situated in a material and cultural context. It is important for computational social science to begin developing tools to represent the dimensions of the actor in terms of the sub jectivity because it recognizes and seeks to explain the complexity of human feelings and fears in creating meaning and in taking action. Theories of sub jectivity also offer a model of human action and play that is neither universal nor individual. Developing a computational social theory of the sub jectivity is an impossible and improbable task, however, in seeking to develop this theory in code we might be able to posit clearer questions in social research regarding the sub jectivity and explore the operational limits of computational social science, which is a theory in itself. Also accounting for how complex structures of feeling and experienceanxieties, fears, dreams, hopes and the likedevelop as a component inside complex social organizations, ones that can be rigorously explored with computational methods, a foundation can be created for a critical computational social science. Contact:"
10.1.1.74.3274,MultilevelAlgorithmsforGeneratingCoarseGridsfor,,"Geometric Multigrid methods have gained widespread acceptance for solving large systems of linear equations, especially for structured grids. One of the challenges in successfully extending these methods to unstructured grids is the problem of generating an appropriate set of coarse grids. The focus of this paper is the development of robust algorithms, both serial and parallel, for generating a sequence of coarse grids from the original unstructured grid. Our algorithms treat the problem of coarse grid construction as an optimization problem that tries to optimize the overall quality of the resulting fused elements. We solve this problem using the multilevel paradigm that has been very successful in solving the related grid/graph partitioning problem. The parallel formulation of our algorithm incurs a very small communication overhead, achieves high degree of concurrency, and maintains the high quality of the coarse grids obtained by the serial algorithm. ∗ This work was supported by NSF CCR-9972519, EIA-9986042,"
10.1.1.73.915,"Reprinted from Perspectives on Adaptation in Natural and Artificial Systems—Essays in honor of John Holland,",,"What you always wanted to know about genetic algorithms but were afraid to hear In spite of their seemingly “obvious ” virtues as a search strategy, genetic algorithms have ended up playing only a modest role as design tools in science and engineering. We review the reasons for this apparent failure, and we suggest a more relaxed view of their utility. 1"
10.1.1.73.9101,Fundamental Theorem of Algebra 1 MML Identifier: POLYNOM5.,,"provide the notation and terminology for this paper. One can prove the following propositions: 1. PRELIMINARIES (1) For all natural numbers n, m such that n � = 0 and m � = 0 holds (n · m − n − m) + 1 ≥ 0. (2) For all real numbers x, y such that y> 0 holds min(x,y) max(x,y)  ≤ 1. (3) For all real numbers x, y such that for every real number c such that c> 0 and c < 1 holds c · x ≥ y holds y ≤ 0. (4) Let p be a finite sequence of elements of R. Suppose that for every natural number n such that n ∈ dom p holds p(n)  ≥ 0. Let i be a natural number. If i ∈ dom p, then ∑ p ≥ p(i). (5) For all real numbers x, y holds −(x + yiCF)  = −x + (−y)iCF. (6) For all real numbers x1, y1, x2, y2 holds (x1 + y1iCF)  − (x2 + y2iCF)  = (x1 − x2) + (y1 − y2)iCF. In this article we present several logical schemes. The scheme ExDHGrStrSeq deals with a non empty groupoid A and a unary functor F yielding an element of A, and states that: There exists a sequence S of A such that for every natural number n holds S(n) = F (n) for all values of the parameters. The scheme ExDdoubleLoopStrSeq deals with a non empty double loop structure A and a unary functor F yielding an element of A, and states that: There exists a sequence S of A such that for every natural number n holds S(n) = F (n) for all values of the parameters. The following proposition is true (8) 1 For every element z of CF such that z � = 0CF |power CF (z, n) |  = |z|n."
10.1.1.73.7467,Revisiting R-tree construction principles,2002,"Abstract. Spatial indexing is a well researched field that benefited computer science with many outstanding results. Our effort in this paper can be seen as revisiting some outstanding contributions to spatial indexing, questioning some paradigms, and designing an access method with globally improved performance characteristics. In particular, we argue that dynamic R-tree construction is a typical clustering problem which can be addressed by incorporating existing clustering algorithms. As a working example, we adopt the well-known k-means algorithm. Further, we study the effect of relaxing the “two-way split procedure and propose a “multiway” split, which inherently is supported by clustering techniques. We compare our clustering approach to two prominent examples of spatial access methods, the R- and the R*-tree. 1"
10.1.1.73.7192,Attacks on Bresson-Chevassut-Essiari-Pointcheval’s Group Key Agreement Scheme for Low-Power Mobile Devices. Available at http://eprint.iacr.org/2004/251,2004,"Abstract. In this paper, we show that Bresson-Chevassut-Essiari-Pointcheval’s group key agreement scheme does not meet the main security properties: implicit key authentication, forward secrecy, and known key security. Also, we propose an improved version which fixes the security flaws found in the scheme."
10.1.1.73.6942,Processor acceleration through automated instruction set customization,2003,"Application-specific extensions to the computational capabilities of a processor provide an efficient mechanism to meet the growing performance and power demands of embedded applications. Hardware, in the form of new function units (or co-processors), and the corresponding instructions, are added to a baseline processor to meet the critical computational demands of a target application. The central challenge with this approach is the large degree of human effort required to identify and create the custom hardware units, as well as porting the application to the extended processor. In this paper, we present the design of a system to automate the instruction set customization process. A dataflow graph design space exploration engine efficiently identifies profitable computation subgraphs from which to create custom hardware, without artificially constraining their size or shape. The system also contains a compiler subgraph matching framework that identifies opportunities to exploit and generalize the hardware to support more computation graphs. We demonstrate the effectiveness of this system across a range of application domains and study the applicability of the custom hardware across the domain. 1."
10.1.1.73.5079,An Empirical Evaluation of Context-Sensitive Pose Estimators in an Urban Outdoor Environment  ,,"  When a mobile robot is executing a navigational task in an urban outdoor environment, accurate localization information is often essential. The difficulty of this task is compounded by sensor drop-out and the presence of non-linear error sources over the span of the mission. We have observed that certain motions of the robot and environmental conditions affect pose sensors in different ways. In this paper, we propose a computational method for localization that systematically integrates and evaluates contextual information that affects the quality of sensors, and utilize the information in order to improve the output of sensor fusion. Our method was evaluated in comparison with conventional probabilistic localization methods (namely, the extended Kalman filter and Monte Carlo localization) in a set of outdoor experiments. The results of the experiment are also reported in this paper."
10.1.1.73.4626,X-trace: A pervasive network tracing framework,2007,"Modern Internet systems often combine different applications (e.g., DNS, web, and database), span different administrative domains, and function in the context of network mechanisms like tunnels, VPNs, NATs, and overlays. Diagnosing these complex systems is a daunting challenge. Although many diagnostic tools exist, they are typically designed for a specific layer (e.g., traceroute) or application, and there is currently no tool for reconstructing a comprehensive view of service behavior. In this paper we propose X-Trace, a tracing framework that provides such a comprehensive view for systems that adopt it. We have implemented X-Trace in several protocols and software systems, and we discuss how it works in three deployed scenarios: DNS resolution, a three-tiered photo-hosting website, and a service accessed through an overlay network. 1"
10.1.1.73.3144,Towards a computational theory of rat navigation,1994,"A century of behavioral studies has generated an abundance of proposalsfor how animals represent and navigate through space. Recently, neurophysiological recording in freely-behaving animals has begun to reveal cellular correlates of these cognitive processes, such as the existence of place cells in hippocampus and head direction cells in postsubiculum and parietal cortex. We propose computational mechanisms to explain these phenomena. A variety of computer models have demonstrated place cell-like responses, given inputs that encode distance and/or bearing to one or more landmarks. These models utilize machine learning algorithms such as competitive learning [Sharp 1991], recurrent backpropagation/Elman nets [Shapiro & Hetherington 1993, Hetherington & Shapiro 1993], genetic algorithms[Treves et al. 1992], competitive learningwith radial basis units [Burgess et al. 1993], and specialized architectures employing a combination of delta rule and radial basis units [Schmajuk & Blair 1993]. The problem with all of these models is that their processing is mainly a function of visual input. The experimental literature clearly shows that hippocampal processing is not that simple. Specifically, although place fields are sensitive to visual input (they rotate in agreement with rotation of distal visual cues), place cells remain active when the lights are turned out, and place fields can form when the animal explores novel environments in the dark. Place cells also continue to fire when distal landmarks are removed, but permutation of landmarks causes the animal to behave as if it were in an unfamiliar environment. Finally, place cell firing may be dependent on head direction, at least under certain"
10.1.1.72.7133,Towards usable VR: An empirical study of user interfaces for immersive virtual environments,1999,"This paper reports empirical results from a study into the use of 2D widgets in 3D immersive virtual environments. Several researchers have proposed the use of 2D interaction techniques in 3D environments, however little empirical work has been done to test the usability of such approaches. We present the results of two experiments conducted on low-level 2D manipulation tasks within an immersive virtual environment. We empirically show that the addition of passive-haptic feedback for use in precise UI manipulation tasks can significantly increase user performance. Furthermore, users prefer interfaces that provide a physical surface, and that allow them to work with interface widgets in the same visual field of view as the objects they are modifying."
10.1.1.72.4054,Restructuring Partitioned Normal Form Relations without Loss of Information,1997,"Nested relations in partitioned normal form (PNF) are an important subclass of nested relations that are useful in many applications. In this paper we address the question of determining when every PNF relation stored under one nested relation scheme can be transformed into another PNF relation stored under a different nested relation scheme without loss of information, referred to as the two schemes being data equivalent. This issue is important in many database application areas such as view processing, schema integration and schema evolution. The main result of the paper provides two characterisations of data equivalence for nested schemes. The first is that two schemes are data equivalent if and only if the two sets of multivalued dependencies induced by the two corresponding scheme trees are equivalent. The second is that the schemes are equivalent if and only if the corresponding scheme trees can be transformed into the other by a sequence of applications of a local restructuring operator and its inverse. 1."
10.1.1.71.5737,Boolean Analyzer -- An Algorithm That Uses A Probabilistic Interestingness Measure to find Dependency/Association Rules In A Head Trauma Data ,,"A new, binary-based technique is presented for finding dependency/association rules called the Boolean Analyzer (BA). With initial guidance from a domain user or domain expert, BA is given one or more metrics to partition the entire data set. This leads to analyzing the implicit domain knowledge and creatingweighted rules in the form of boolean expressions. To augment the analysis of the rules produced, we can additionally apply a probabilistic interestingness measure (PIM) to order the generated rules based on event dependency, where events are combinations of primed and unprimed variables. Following our discussion of the basic BA algorithm, our paper will present a case study on clinical head trauma data. BA able to find rules, where the most significant rules were those that had a high PIM. We believe that BA has broad applicability in the medical domain, and hope that our presentation here can stimulate other creative applications of the technique."
10.1.1.71.2130,Detecting Causal Relationships in Distributed Computations,1994,"Abstract: The paper shows that characterizing the causal relationship between significant events is an important but non-trivial aspect for understanding the behavior of distributed programs. An introduction to the notion of causality and its relation to logical time is given; some fundamental results concerning the characterization of causality are presented. Recent work on the detection of causal relationships in distributed computations is surveyed. The issue of observing distributed computations in a causally consistent way and the basic problems of detecting global predicates are discussed. To illustrate the major difficulties, some typical monitoring and debugging approaches are assessed, and it is demonstrated how their feasibility is severely limited by the fundamental problem to master the complexity of causal relationships."
10.1.1.70.2080,"PocketSphinx: A free, real-time continuous speech recognition system for hand-held devices",2006,"The availability of real-time continuous speech recognition on mobile and embedded devices has opened up a wide range of research opportunities in human-computer interactive applications. Unfortunately, most of the work in this area to date has been confined to proprietary software, or has focused on limited domains with constrained grammars. In this paper, we present a preliminary case study on the porting and optimization of CMU SPHINX-II, a popular open source large vocabulary continuous speech recognition (LVCSR) system, to hand-held devices. The resulting system operates in an average 0.87 times real-time on a 206MHz device, 8.03 times faster than the baseline system. To our knowledge, this is the first hand-held LVCSR system available under an open-source license. 1."
10.1.1.7.9457,Data Swapping:,2003,"this paper, we describe:  "
10.1.1.7.4123,The Bulk Index Join:,1999,"Efficient join algorithms have been developed for processing different types of non-equijoins like spatial  join, band join, temporal join or similarity join. Each of these previously proposed join algorithms is tailor  -cut for a specific type of join, and a generalization of these algorithms to other join types is not obvious."
10.1.1.7.1072,Data Swapping:A Risk-Utility,,"this paper, we describe:  "
10.1.1.69.8113,Backup path allocation based on a correlated link failure probability model in overlay networks,2002,"Communication reliability is a desired property in computer networks. One key technology to increase the reliability of a communication path is to provision a disjoint backup path. One of the main challenges in implementing this technique is that two paths that are disjoint at the IP or overlay layer may share the same physical links. As a result, although we may select a disjoint backup path at the overlay layer, one physical link failure may cause the failure of both the primary and the backup paths. In this paper, we propose a solution to address this problem. The main idea is to take into account the correlated link failure at the overlay layer. More precisely, our goal is to find a route for the backup path to minimize the joint path failure probability between the primary and the backup paths. To demonstrate the feasibility of our approach, we perform extensive evaluations under both single and double link failure models. Our results show that, in terms of robustness, our approach is near optimal and is up to ¢¤£¦ ¥ better than no backup path reservation and is up to §¨£¦ ¥ better than using the traditional shortest disjoint path algorithm to select the backup path. 1."
10.1.1.69.7541,Memory ordering in modern microprocessors,2005,"Linux R ○ has supported a large number of SMP systems based on a variety of CPUs since the 2.0 kernel. Linux has done an excellent job of abstracting away differences among these CPUs, even in kernel code. One important difference is how CPUs allow memory accesses to be reordered in SMP systems. SMMP Hardware Memory accesses are among the slowest of a CPU’s operations, due to the fact that Moore’s law has increased CPU instruction performance at a much greater rate than it has increased memory performance. This difference in performance increase means that memory operations have been getting increasingly expensive compared to simple register-to-register instructions. Modern CPUs sport increasingly large caches in order to reduce the"
10.1.1.69.7312,Backup path allocation based on a correlated link failure probability model in overlay networks,2002,"Communication reliability is a desired property in computer networks. One key technology to increase the reliability of a communication path is to provision a disjoint backup path. One of the main challenges in implementing this technique is that two paths that are disjoint at the IP or overlay layer may share the same physical links. As a result, although we may select a disjoint backup path at the overlay layer, one physical link failure may cause the failure of both the primary and the backup paths. In this paper, we propose a solution to address this problem. The main idea is to take into account the correlated link failure at the overlay layer. More precisely, our goal is to find a route for the backup path to minimize the joint path failure probability between the primary and the backup paths. To demonstrate the feasibility of our approach, we perform extensive evaluations under both single and double link failure models. Our results show that, in terms of robustness, our approach is near optimal and is up to ¢¤£¦¥ better than no backup path reservation and is up to §¨£©¥ better than using the traditional shortest disjoint path algorithm to select the backup path. 1."
10.1.1.68.9507,Integrating Transaction Services into Web-based Software Development Environments,,"Software Development Environments (SDE) require sophisticated database transaction models due to the long-duration, interactive, and cooperative nature of the software engineering activities. Such Extended Transaction Models (ETM) have been proposed and implemented by building application-specific databases for the SDEs. With the development of World Wide Web (WWW), there have been a number of efforts to build SDEs on top of the WWW. Using web servers as the databases to store the software artifacts provided us with a new challenge: how to implement the ETMs in such web-based SDEs without requiring the web servers to be customized specifically according to the application domains of the SDEs. This paper presents our experiences of integrating transaction services into web based SDEs. We evolved from the traditional approach of building a transaction management component that operated on top of a dedicated database to the external transaction server approach. A transaction server, called JPernLite, was built to operate independently of the web servers and provide the necessary extensibility for SDEs to implement their ETMs. The transaction server can be integrated into the SDE via a number of interfaces, and we discuss the pros and cons of each alternative in detail."
10.1.1.68.8931,The Click modular router,1999,"Click is a new software architecture for building flexible and configurable routers. A Click router is assembled from packet processing modules called elements. Individual elements implement simple router functions like packet classification, queueing, scheduling, and interfacing with network devices. Complete configurations are built by connecting elements into a graph; packets flow along the graph's edges. Several features make individual elements more powerful and complex configurations easier to write, including pull processing, which models packet flow driven by transmitting interfaces, and flow-based router context, which helps an element locate other interesting elements. We"
10.1.1.68.8428,"Algorithm, implementation and application of the simdl similarity server",2007,"Abstract. Semantic similarity measurement gained attention as a methodology for ontology-based information retrieval within GIScience over the last years. Several theories explain how to determine the similarity between entities, concepts or spatial scenes, while concrete implementations and applications are still missing. In addition, most existing similarity theories use their own representation language while the majority of geoontologies is annotated using the Web Ontology Language (OWL). This paper presents a context and blocking aware semantic similarity theory for the description logic ALCHQ as well as its prototypical implementation within the open source SIM-DL similarity server. An application scenario is introduced showing how the Alexandria Digital Library Gazetteer can benefit from similarity in terms of improved search and annotation capabilities. Directions for further work are discussed. 1"
10.1.1.68.8100,The evolution of subtle manoeuvres in simulated hockey,1998,"A simulated hockey environment is introduced as a test bed for studying adaptive behavior and evolution of robot controllers. A near-frictionless playing surface is employed, partially mimicking zero gravity conditions. We show how a neural network using a simple evolutionary algorithm can develop nimble strategies for moving about the rink and scoring goals quickly and e�ectively. 1."
10.1.1.67.9544,On flat-state connectivity of chains with fixed acute angles,2002,"We prove that two classes of fixed-angle, open chains with acute angles are “flat-state connected. ” A chain is flatstate connected if it can be reconfigured between any two of its planar realizations without self-crossing. In a companion paper (under preparation) [ADD +], several fixed-angle linkages will be proved flat-state connected or disconnected. In particular, all orthogonal or obtuse-angle open chains are flat-state connected. But it remains open whether this holds for acute-angle open chains. In this paper, we prove that two classes of such chains are indeed flat-state connected: those with equal acute angles, and those with equal edge lengths and angles in (60 ◦ , 90 ◦]. We claim, but do not prove, an extension of the latter result to the range [45 ◦ , 90 ◦ ] without length restriction. 1"
10.1.1.67.2374,"PocketSphinx: A free, real-time continuous speech recognition system for hand-held devices",2006,"The availability of real-time continuous speech recognition on mobile and embedded devices has opened up a wide range of research opportunities in human-computer interactive applications. Unfortunately, most of the work in this area to date has been confined to proprietary software, or has focused on limited domains with constrained grammars. In this paper, we present a preliminary case study on the porting and optimization of CMU SPHINX-II, a popular open source large vocabulary continuous speech recognition (LVCSR) system, to hand-held devices. The resulting system operates in an average 0.87 times real-time on a 206MHz device, 8.03 times faster than the baseline system. To our knowledge, this is the first hand-held LVCSR system available under an open-source license. 1."
10.1.1.66.1366,An FPGA Design Flow for Reconfigurable Network-Based Multi-Processor Systems on Chip,,"Multi-Processor System on Chip (MPSoC) platforms are becoming increasingly more heterogeneous and are shifting towards a more communication-centric methodology. Networks on Chip (NoC) have emerged as the design paradigm for scalable on-chip communication architectures. As the system complexity grows, the problem emerges as how to design and instantiate such a NoC-based MPSoC platform in a systematic and automated way. In this paper we present an integrated flow to automatically generate a highly configurable NoC-based MPSoC for FPGA instantiation. The system specification is done on a high level of abstraction, relieving the designer of errorprone and time consuming work. The flow uses the state-ofthe-art Æthereal NoC, and Silicon Hive processing cores, both configurable at design- and run-time. We use this flow to generate a range of sample designs whose functionality has been verified on a Celoxica RC300E development board. The board, equipped with a Xilinx Virtex II 6000, also offers a huge number of peripherals, and we show how their insertion is automated in the design for easy debugging and prototyping. 1."
10.1.1.65.7908,Towards a real time panoramic depth sensor,2003,Abstract. Recently we have presented a system for panoramic depth imaging with a single standard camera. One of the problems of such a system is the fact that we cannot generate a stereo pair of images in real time. This paper presents a possible solution to this problem. Based on a new sensor setup simulations were performed to establish the quality of new results in comparison to results obtained with the old sensor setup. The goal of the paper is to reveal whether the new setup can be used for real time capturing of panoramic depth images and consequently for autonomous navigation of a mobile robot in a room. 1
10.1.1.65.5825,Integrating Transaction Services into Web-based Software Development Environments,,"Software Development Environments (SDE) require sophisticated database transaction models due to the long-duration, interactive, and cooperative nature of the software engineering activities. Such Extended Transaction Models (ETM) have been proposed and implemented by building application-specific databases for the SDEs. With the development of World Wide Web (WWW), there have been a number of efforts to build SDEs on top of the WWW. Using web servers as the databases to store the software artifacts provided us with a new challenge: how to implement the ETMs in such web-based SDEs without requiring the web servers to be customized specifically according to the application domains of the SDEs. This paper presents our experiences of integrating transaction services into web based SDEs. We evolved from the traditional approach of building a transaction management component that operated on top of a dedicated database to the external transaction server approach. A transaction server, called JPernLite, was built to operate independently of the web servers and provide the necessary extensibility for SDEs to implement their ETMs. The transaction server can be integrated into the SDE via a number of interfaces, and we discuss the pros and cons of each alternative in detail."
10.1.1.65.3765,Non-Leftmost Unfolding in Partial Evaluation of Logic Programs with Impure Predicates,2006,"Abstract. Partial evaluation of logic programs which contain impure predicates poses non-trivial challenges. Impure predicates include those which produce side-effects, raise errors (or exceptions), and those whose truth value varies according to the degree of instantiation of arguments 4. In particular, non-leftmost unfolding steps can produce incorrect results since the independence of the computation rule no longer holds in the presence of impure predicates. Existing proposals allow non-leftmost unfolding steps, but at the cost of accuracy: bindings and failure are not propagated backwards to predicates which are potentially impure. In this work we propose a partial evaluation scheme which substantially reduces the situations in which such backpropagation has to be avoided. With this aim, our partial evaluator takes into account the information about purity of predicates expressed in terms of assertions. This allows achieving some optimizations which are not feasible using existing partial evaluation techniques. We argue that our proposal goes beyond existing ones in that it is a) accurate, since the classification of pure vs impure is done at the level of atoms instead of predicates, b) extensible, as the information about purity can be added to programs using assertions without having to modify the partial evaluator itself, and c) automatic, since (backwards) analysis can be used to automatically infer the required assertions. Our approach has been implemented in the context of CiaoPP, the abstract interpretation-based preprocessor of the Ciao logic programming system. 1"
10.1.1.64.8428,Learning a model of a web user’s interests,2003,"Abstract. There are many recommender systems that are designed to help users find relevant information on the web. To produce recommendations that are relevant to an individual user, many of these systems first attempt to learn a model of the user’s browsing behavior. This paper presents a novel method for learning such a model from a set of annotated web logs — i.e., web logs that are augmented with the user’s assessment of whether each webpage is an information content (IC) page (i.e., contains the information required to complete her task). Our systems use this to learn what properties of a webpage, within a sequence, identify such IC-pages, and similarly what ”browsing properties ” characterize the words on such pages (“IC-words”). As these methods deal with properties of webpages (or of words), rather than specific URLs (words), they can be used anywhere throughout the web; i.e., they are not specific to a particular website, or a particular task. This paper also describes the enhanced browser, AIE, that we designed and implemented for collecting these annotated web logs, and an empirical study we conducted to investigate the effectiveness of our approach. This empirical evidence shows that our approach, and our algorithms, work effectively. 1"
10.1.1.64.641,Thread Transparency in Information Flow,,"Abstract. Applications that process continuous information flows are challenging to write because the application programmer must deal with flow-specific concurrency and timing requirements, necessitating the explicit management of threads, synchronization, scheduling and timing. We believe that middleware can ease this burden, but middleware that supports control-flow centric interaction models such as remote method invocation does not match the structure of these applications. Indeed, it abstracts away from the very things that the information-flow centric programmer must control. We are defining Infopipes as a high-level abstraction for information flow, and we are developing a middleware framework that supports this abstraction directly. Infopipes handle the complexities associated with control flow and multi-threading, relieving the programmer of this task. Starting from a high-level description of an information flow configuration, the framework determines which parts of a pipeline require separate threads or coroutines, and handles synchronization transparently to the application programmer. The framework also gives the programmer the freedom to write or reuse components in a passive style, even though the configuration will actually require the use of a thread or coroutine. Conversely, it is possible to write a component using a thread and know that the thread will be eliminated if it is not needed in a pipeline. This allows the most appropriate programming model to be chosen for a given task, and existing code to be reused irrespective of its activity model. 1"
10.1.1.64.349,CHAPTER 34 KNOWLEDGE MANAGEMENT: ARE WE MISSING SOMETHING? *,,"As commercial organisations face up to modern pressures to downsize and outsource they have lost knowledge as people leave and take with them what they know. This knowledge is increasingly being recognised as an important resource and organisations are now taking steps to manage it. In addition, as the pressures for globalisation increase, collaboration and co-operation are becoming more distributed and international. Knowledge sharing in a distributed international environment is becoming an essential part of Knowledge Management (KM). In this paper we make a distinction between hard and soft knowledge within an organisation and argue that much of what is called KM deals with hard knowledge and emphasises capture-codify-store. This is a major weakness of the current approach to KM. This paper addresses this weakness by exploring the sharing of ‘soft ’ knowledge using the concept of communities of practice."
10.1.1.64.1303,An Empirical Evaluation of Context-Sensitive Pose Estimators in an Urban Outdoor Environment   ,,"  When a mobile robot is executing a navigational task in an urban outdoor environment, accurate localization information is often essential. The difficulty of this task is compounded by sensor drop-out and the presence of non-linear error sources over the span of the mission. We have observed that certain motions of the robot and environmental conditions affect pose sensors in different ways. In this paper, we propose a computational method for localization that systematically integrates and evaluates contextual information that affects the quality of sensors, and utilize the information in order to improve the output of sensor fusion. Our method was evaluated in comparison with conventional probabilistic localization methods (namely, the extended Kalman filter and Monte Carlo localization) in a set of outdoor experiments. The results of the experiment are also reported in this paper."
10.1.1.63.8929,Differential complexes and numerical stability,2002,"Differential complexes such as the de Rham complex have recently come to play an important role in the design and analysis of numerical methods for partial differential equations. The design of stable discretizations of systems of partial differential equations often hinges on capturing subtle aspects of the structure of the system in the discretization. In many cases the differential geometric structure captured by a differential complex has proven to be a key element, and a discrete differential complex which is appropriately related to the original complex is essential. This new geometric viewpoint has provided a unifying understanding of a variety of innovative numerical methods developed over recent decades and pointed the way to stable discretizations of problems for which none were previously known, and it appears likely to play an important role in attacking some currently intractable problems in numerical PDE."
10.1.1.63.7305,Describing groups,,"Abstract. Two ways of describing a group are considered. 1. A group is finiteautomaton presentable if its elements can be represented by strings over a finite alphabet, in such a way that the set of representing strings and the group operation can be recognized by finite automata. 2. An infinite f.g. group is quasi-finitely axiomatizable if there is a description consisting of a single first-order sentence, together with the information that the group is finitely generated. In the first part of the paper we survey examples of FA-presentable groups, but also discuss theorems restricting this class. In the second part, we give examples of quasi-finitely axiomatizable groups, consider the algebraic content of the notion, and compare it to the notion of a group which is a prime model. We also show that if a structure is bi-interpretable in parameters with the ring of integers, then it is prime and"
10.1.1.63.7017,Abstract,,"In this paper we present a sublinear time (1 + ɛ)-approximation randomized algorithm to estimate the weight of the minimum spanning tree of an n-point metric space. The running time of the algorithm is � O(n/ɛ O(1)). Since the full description of an n-point metric space is of size Θ(n 2), the complexity of our algorithm is sublinear with respect to the input size. Our algorithm is almost optimal as it is not possible to approximate in o(n) time the weight of the minimum spanning tree to within any factor. Furthermore, it has been previously shown that no o(n 2) algorithm exists that returns a spanning tree whose weight is within a constant times the optimum."
10.1.1.63.3138,Multi-value-functions: Efficient automatic action hierarchies for multiple goal MDPs,1999,"If you have planned to achieve one particular goal in a stochastic delayed rewards problem and then someone asks about a di erent goal what should you do? What if you need to be ready to quickly supply an answer for any possible goal? This paper shows that by using a new kind of automatically generated abstract action hierarchy that with N states, preparing for all of N possible goals can be much much cheaper than N times the work of preparing for one goal. In goal-based Markov Decision Problems, it is usual to generate a policy (x), mapping states to actions, and a value function J(x), mapping states to an estimate of minimum expected cost-to-goal, starting at x. In this paper we will use the terminology that a multi-policy? (x; y) (for all state-pairs (x; y)) maps a state x to the rst action it should take in order to reach y with expected minimum cost and a multi-value function J? (x; y) is a de nition of this minimum cost. Building these objects quickly and with little memory is the main purpose of this paper, but a secondary result is a natural, automatic, way to actions for MDPs. The paper concludes with a set of empirical results on increasingly large MDPs.  "
10.1.1.63.2506,Run-time support for distributed sharing in safe languages,2003,"We present a new run-time system that supports object sharing in a distributed system. The key insight in this system is that a handle-based implementation of such a system enables efficient and transparent sharing of data with both fine- and coarse-grained access patterns. In addition, it supports efficient execution of garbage-collected programs. In contrast, conventional distributed shared memory (DSM) systems are limited to providing only one granularity with good performance, and have experienced difficulty in efficiently supporting garbage collection. A safe language, in which no pointer arithmetic is allowed, can transparently be compiled into a handle-based system and constitutes its preferred mode of use. A programmer can also directly use a handle-based programming model that avoids pointer arithmetic on the handles, and achieve the same performance but without the programming benefits of a safe programming language. This new run-time system, DOSA (Distributed Object Sharing Architecture), provides a shared object space abstraction rather than a shared address space abstraction. The key to its efficiency is the observation that a handle-based distributed implementation permits VM-based access and modification detection without suffering false sharing for fine-grained access patterns. We compare DOSA to TreadMarks, a conventional DSM system that is efficient at handling coarse-grained sharing. The performance of fine-grained applications and garbage-collected applications is considerably better than in TreadMarks, and the performance of coarse-grained applications is nearly as good as in TreadMarks. Inasmuch as the performance of such applications is already good in TreadMarks, we consider this an acceptable performance penalty."
10.1.1.63.2502,TOPOLOGICAL CONJUGACY BETWEEN APERIODIC TILING DYNAMICAL SYSTEMS,,We extend to certain aperiodic tiling dynamical systems associated with an underlying substitution a geometric invariant for topological conjugacy.
10.1.1.62.8148,Inductance 101: Analysis and design issues,2001,"With operating frequencies approaching the gigahertz range, inductance is becoming an increasingly important consideration in the design and analysis of on-chip interconnect. In this paper, we give a tutorial overview of the analysis and design issues related to on-chip inductance effects.We explain the complexity of the current flow in VLSI circuits. We discuss the applicability of the PEEC approach in a detailed circuit model of the signal and power grid interconnect, switching devices, power pads and the package. Further, we explain techniques that can be used to speed-up simulation of the large PEEC model. We then discuss a simplified model that uses the so-called loop inductance approach, and compare it with the detailed model.We present experimental results, obtained from simulations of industrial circuits, for both the PEEC and loop models. We also cover design techniques that can help tackle the on-chip inductance issues."
10.1.1.62.1099,Exploratory study of a new model of evolving networks,2006,"Abstract. The study of social networks has gained new importance with the recent rise of large on-line communities. Most current approaches focus on deterministic (descriptive) models and are usually restricted to a preset number of people. Moreover, the dynamic aspect is often treated as an addendum to the static model. Taking inspiration from real-life friendship formation patterns, we propose a new generative model of evolving social networks that allows for birth and death of social links and addition of new people. Each person has a distribution over social interaction spheres, which we term ”contexts. ” We study the robustness of our model by examining statistical properties of simulated networks relative to well known properties of real social networks. We discuss the shortcomings of this model and problems that arise during learning. Several extensions are proposed. 1"
10.1.1.60.9804,Dynamic Resource Allocation in Communication Networks,2006,"E#cient dynamic resource provisioning algorithms are  necessary to the development and automation of Quality of Service  (QoS) networks. The main goal of these algorithms is to o#er services  that satisfy the QoS requirements of individual users while guaranteeing  at the same time an e#cient utilization of network resources. In this  paper we introduce a new service model that provides quantitative  per-flow bandwidth guarantees, where users subscribe for a guaranteed  rate; moreover, the network periodically individuates unused bandwidth  and proposes short-term contracts where extra-bandwidth is allocated  and guaranteed exclusively to users who can exploit it to transmit at  a rate higher than their subscribed rate. To implement this service  model we propose a dynamic provisioning architecture for intra-domain  Quality of Service networks. We develop an e#cient bandwidth allocation  algorithm that takes explicitly into account tra#c statistics to  increase the users' benefit and the network revenue simultaneously. We  demonstrate through simulation in realistic network scenarios that the  proposed dynamic provisioning model is superior to static provisioning  in providing resource allocation both in terms of total accepted load  and network revenue."
10.1.1.60.5900,Indexing Data-Oriented Overlay Networks,2005,"The application of structured overlay networks  to implement index structures for data-oriented  applications such as peer-to-peer databases or  peer-to-peer information retrieval, requires highly  efficient approaches for overlay construction,  as changing application requirements frequently  lead to re-indexing of the data and hence (re-  )construction of overlay networks. This problem  has so far not been addressed in the literature  and thus we describe an approach for the  efficient construction of data-oriented, structured  overlay networks from scratch in a self-organized  way. Standard maintenance algorithms for overlay  networks cannot accomplish this efficiently, as  they are inherently sequential. Our proposed algorithm  is completely decentralized, parallel, and  can construct a new overlay network with short  latency. At the same time it ensures good loadbalancing  for skewed data key distributions which  result from preserving key order relationships as  necessitated by data-oriented applications. We  provide both a theoretical analysis of the basic algorithms  and a complete system implementation  that has been tested on PlanetLab. We use this implementation  to support peer-to-peer information  retrieval and database applications."
10.1.1.60.1268,Dynamic Resource Allocation in Communication Networks,2006,"E#cient dynamic resource provisioning algorithms are necessary  to the development and automation of Quality of Service (QoS)  networks. The main goal of these algorithms is to o#er services that satisfy  the QoS requirements of individual users while guaranteeing at the  same time an e#cient utilization of network resources. In this paper we  introduce a new service model that provides quantitative per-flow bandwidth  guarantees, where users subscribe for a guaranteed rate; moreover,  the network periodically individuates unused bandwidth and proposes  short-term contracts where extra-bandwidth is allocated and guaranteed  exclusively to users who can exploit it to transmit at a rate higher than  their subscribed rate. To implement this service model we propose a  dynamic provisioning architecture for intra-domain Quality of Service  networks. We develop an e#cient bandwidth allocation algorithm that  takes explicitly into account tra#c statistics to increase the users' benefit  and the network revenue simultaneously. We demonstrate through  simulation in realistic network scenarios that the proposed dynamic provisioning  model is superior to static provisioning in providing resource  allocation both in terms of total accepted load and network revenue."
10.1.1.6.624,Packing Square Tiles into One Texture,2004,"This paper deals with the packing of square tiles of the same size into one texture. Texture size is constrained by the graphics hardware. In particular, width and height resolutions must be powers of two. To cover the whole texture and avoid space loss, common schemes pack a number of tiles that is a power of two. We show that numbers of tiles like 5, 13, 17, 25, 34 and others can also be reached without wasting memory. To achieve this, our scheme takes advantage of texture rotation and the wrapping capability of texture-addressing, which gives a torus topology to the texture space."
10.1.1.6.285,Methodologies for Target Selection in Structural Genomics,2000,"As the number of complete genomes that have been sequenced keeps growing, unknown areas of the protein space are revealed and new horizons open up. Most of this information will be fully appreciated only when the structural information about the encoded proteins becomes available. The goal of structural genomics is to direct large-scale e#orts of protein structure determination, so as to increase the impact of these e#orts. This review focuses on current approaches in structural genomics aimed at selecting representative proteins as targets for structure determination. We will discuss the concept of representative structures/folds, the current methodologies for identifying those proteins, and computational techniques for identifying proteins which are expected to adopt new structural folds. # 2000 Elsevier Science Ltd. All rights reserved."
10.1.1.6.1860,High-Level Synthesis of Nonprogrammable Hardware Accelerators,2000,"The PICO-N system automatically synthesizes embedded nonprogrammable accelerators to be used as co-processors for functions expressed as loop nests in C. The output is synthesizable VHDL that defines the accelerator at the register transfer level (RTL). The system generates a synchronous array of customized VLIW (very-long instruction word) processors, their controller, local memory, and interfaces. The system also modifies the user's application software to make use of the generated accelerator. The user indicates the throughput to be achieved by specifying the number of processors and their initiation interval. In experimental comparisons, PICO-N designs are slightly more costly than hand-designed accelerators with the same performance."
10.1.1.6.1455,Plutarch: An Argument for Network Pluralism,2003,"It is widely accepted that the current Internet architecture is insu#cient for the future: problems such as address space scarcity, mobility and non-universal connectivity are already with us, and stand to be exacerbated by the explosion of wireless, ad-hoc and sensor networks. Furthermore, it is far from clear that the ubiquitous use of standard transport and name resolution protocols will remain practicable or even desirable."
10.1.1.59.9133,ContextContacts: Re-Designing SmartPhone's Contact Book to Support Mobile Awareness and Collaboration,2005,"Acontextuality of the mobile phone often leads to a caller's uncertainty over a callee's current state, which in turn often hampers mobile collaboration. We are interested in re-designing a Smartphone's contact book to provide cues of the current situations of others. ContextContacts presents several meaningful, automatically communicated situation cues of trusted others. Its interaction design follows social psychological findings on how people make social attributions based on impoverished cues, on how self-disclosure of cues is progressively and interactionally managed, and on how mobility affects interaction through cues. We argue how our design choices support mobile communication decisions and group coordinations by promoting awareness. As a result, the design is very minimal and integrated, in an ""unremarkable"" manner, to previously learned usage patterns with the phone. First laboratory and field evaluations indicate important boundary conditions for and promising avenues toward more useful and enjoyable mobile awareness applications."
10.1.1.59.7332,Indexing Data-Oriented Overlay Networks,2005,"The application of structured overlay networks  to implement index structures for data-oriented  applications such as peer-to-peer databases or  peer-to-peer information retrieval, requires highly  efficient approaches for overlay construction,  as changing application requirements frequently  lead to re-indexing of the data and hence (re-  )construction of overlay networks. This problem  has so far not been addressed in the literature  and thus we describe an approach for the  efficient construction of data-oriented, structured  overlay networks from scratch in a self-organized  way. Standard maintenance algorithms for overlay  networks cannot accomplish this efficiently, as  they are inherently sequential. Our proposed algorithm  is completely decentralized, parallel, and  can construct a new overlay network with short  latency. At the same time it ensures good loadbalancing  for skewed data key distributions which  result from preserving key order relationships as  necessitated by data-oriented applications. We  provide both a theoretical analysis of the basic algorithms  and a complete system implementation  that has been tested on PlanetLab. We use this implementation  to support peer-to-peer information  retrieval and database applications."
10.1.1.59.6213,Robust and Efficient Skeletal Graphs,2000,"There has recently been significant interest in using representations based on abstractions of Blum's skeleton into a graph, for qualitative shape matching. The application of these techniques to large databases of shapes hinges on the availability of numerical algorithms for computing the medial axis. Unfortunately, this computation can be extremely subtle. Approaches based on Voronoi techniques preserve topology, but heuristic pruning measures are introduced to remove unwanted edges. Methods based on Euclidean distance functions can localize skeletal points accurately, but often at the cost of altering the object's topology. In this paper we introduce a new algorithm for computing subpixel skeletons which is robust and accurate, has low computational complexity, and preserves topology. The key idea is to measure the net outward flux of a vector field per unit area, and to detect locations where a conservation of energy principle is violated. This is done in conjunction with a thinning process applied in a rectangular lattice. We illustrate the approach with several examples of skeletal graphs for biological and man-made silhouettes."
10.1.1.59.6201,Is MPI Suitable for a Generative Design-Pattern System?,2005,Generative parallel design patterns is a proven technique to improve the productivity of parallel  program development. However many of the generative design-pattern systems are developed for target  languages that are not widely used by the high performance computing community. This paper describes  an initial e#ort to develop a system that will hopefully answer the question in the title in the a#rmative.
10.1.1.59.5519,Adaptive Routing for Intermittently Connected Mobile Ad Hoc Networks,2005,"The vast majority of mobile ad hoc networking research makes a very large assumption: that communication can only take place between nodes that are simultaneously accessible within in the same connected cloud (i.e., that communication is synchronous). In reality, this assumption is likely to be a poor one, particularly for sparsely or irregularly populated environments."
10.1.1.58.9190,The Chromatic Number of Random Regular Graphs,2004,"Given any integer d    1, let k be the smallest integer such that d  2k log k. We prove that with  high probability the chromatic number of a random d-regular graph is k, k + 1, or k + 2."
10.1.1.58.6683,A Parameterized Type System for Race-Free Java Programs  ,2001,"... programs; any well-typed program in our system is free of data races. Our type system is significantly more expressive than previous such type systems. In particular, our system lets programmers write generic code to implement a class, then create dierent objects of the same class that have different protection mechanisms. This flexibility enables programmers to reduce the number of unnecessary synchronization operations in a program without risking data races. We also support default types which reduce the burden of writing the extra type annotations. Our experience indicates that our system provides a promising approach to make multithreaded programs more reliable and efficient."
10.1.1.57.2784,Performance Analysis of a WWW Server,1996,"This paper focuses on the performance analysis of Web servers. In the first part of the paper, we discuss the main steps to carry out a WWW performance analysis effort. Then, we show some experimental results. Using a synthetic benchmark (WebStone) and performance monitor tools, we analyze performance of three different Web server software running on top of a Windows NT platform."
10.1.1.56.4045,Recurrences and Legendre Transform,1992,"A binomial identity ((1) below), which relates the famous Ap'ery numbers and the sums of cubes of binomial coefficients (for which Franel has established a recurrence relation almost 100 years ago), can be seen as a particular instance of a Legendre transform between sequences. A proof of this identity can be based on the more general fact that the Ap'ery and Franel recurrence relations themselves are conjugate via Legendre transform. This motivates a closer look at conjugacy of sequences satisfying linear recurrence relations with polynomial coefficients. The role of computer-aided proof and verification in the study of binomial identities and recurrence relations is illustrated, and potential applications of conjugacy in diophantine approximation are mentioned. This article is an expanded version of a talk given at the 29. meeting of the  S'eminaire Lothringien de Combinatoire, Thurnau, september 1992. 1 Introduction  In this article I will discuss some general aspects related to the..."
10.1.1.56.3828,Complexity of Computing Semi-algebraic Descriptions of the Connected Components of a Semi-algebraic Set,1998,"Given Q 2 R[X1 ; : : : ; Xk ] with deg(Q)  d; we give an algorithm that outputs a semi-algebraic description for each of the semi-algebraically connected components of  Z(Q) ae R  k  : The complexity of the algorithm as well as the size of the output are bounded by d  O(k  3  )  :  More generally, given any semi-algebraic set S defined by a quantifier-free formula involving a family of polynomials, P = fP1 ; : : : ; Psg ae R[X1 ; : : : ; Xk ] whose degrees are at most d; we give an algorithm that outputs a semi-algebraic description for each of the semialgebraically connected components of S: The complexity of the algorithm as well as the size of the output is bounded by s  k+1  d  O(k  3  )  : This improves the previously best known bound of (sd)  k O(1)  for this problem due to Canny, Grigor'ev, Vorobjov and Heintz, Roy and Solern`o [9, 14].  1 Introduction  Let R be a real closed field. A semi-algebraic set in R  k  is the set of points which satisfy a boolean combination of polynom..."
10.1.1.55.9607,Continuity Of The Hausdorff Dimension For Invariant Subsets Of Interval Maps,1994,". Let T : [0; 1] ! [0; 1] be an expanding piecewise monotonic map, and consider the set R of all points, whose orbits omit a certain finite union of open intervals. It is shown that the Hausdorff dimension HD (R) depends continuously on small perturbations of the endpoints of these open intervals. A similar result for the topological pressure is also obtained. Furthermore it is shown that for every  t 2 [0; 1] there exists a closed, T -invariant R t ` [0; 1] with HD (R t ) = t. Finally it is proved that the Hausdorff dimension of the set of all points, whose orbit is not dense, is 1. Introduction  Let T : [0; 1] ! [0; 1] be an expanding piecewise monotonic map, that means there exists a finite partition Z of [0; 1] into pairwise disjoint open intervals with  [Z2Z Z = [0; 1], T jZ is continuous and strictly monotone for all Z 2 Z , T  0  jZ  can be extended to a continuous function on Z for all Z 2 Z , and there exists an n 2 N with inf x2[0;1] j(T  n  )  0  (x)j ? 1. Fix a K 2 N. Let (..."
10.1.1.55.858,"PI/OT, Parallel I/O Templates",1997,"This paper presents a novel, top-down, high-level approach to parallelizing file I/O. Each parallel file descriptor is annotated with a high-level specification, or template, of the expected parallel behaviour. The annotations are external to and independent of the source code. At run-time, all I/O using a parallel file descriptor adheres to the semantics of the selected template. By separating the parallel I/O specifications from the code, a user can quickly change the I/O behaviour without rewriting code. Templates can be composed hierarchically to construct more complex access patterns. Two sample parallel programs using these templates are compared against versions implemented in an existing parallel I/O system (PIOUS). The sample programs show that the use of parallel I/O templates are beneficial from both the performance and software engineering points of view.  1. Introduction  The development of parallel applications has focused on computational parallelism. However, the corres..."
10.1.1.55.8210,RTP: A Transport Protocol for Real-Time Applications,1994,"This memorandum describes the real-time transport protocol, RTP. RTP provides end-toend network transport functions suitable for applications transmitting real-time data, such as audio, video or simulation data over multicast or unicast network services. RTP does not address resource reservation and does not guarantee quality-of-service for real-time services. The data transport is augmented by a control protocol (RTCP) designed to provide minimal control and identification functionality, particularly in multicast networks. RTP and RTCP are designed to be independent of the underlying transport and network layers. The protocol supports the use of RTP-level translators and bridges. This specification is a product of the Audio/Video Transport working group within the Internet Engineering Task Force. Comments are solicited and should be addressed to the working group's mailing list at rem-conf@es.net and/or the authors. The protocol is under development and changes to aspects of the proto..."
10.1.1.55.6678,"Plurisubharmonic Extremal Functions, Lelong Numbers And Coherent Ideal Sheaves",1998,". We introduce a new type of pluricomplex Green function which has a logarithmic pole along a complex subspace A of a complex manifold X. It is the largest negative plurisubharmonic function on X whose Lelong number is at least the Lelong number of log maxfjf 1 j; : : : ; jf m jg, where f 1 ; : : : ; fm are local generators for the ideal sheaf of A. The pluricomplex Green function with a single logarithmic pole or a finite number of weighted poles is a very special case of our construction. We give several equivalent definitions of this function and study its properties, including boundary behaviour, continuity, and uniqueness. This is based on and extends our previous work on disc functionals and their envelopes. 1. Introduction  Let X be a complex manifold. For each function ff : X ! [0; +1) we let  F ff = fu 2 PSH(X) ; u  0; u  ffg  and  G ff = sup F ff ;  where PSH(X) is the class of plurisubharmonic functions on X (including the constant function \Gamma1) and u denotes the Lelong ..."
10.1.1.55.3287,From a Formal Dynamic Semantics of Sisal to a Sisal Environment,1995,"We present a formal definition of the dynamic semantics of a significant part of the language Sisal 2.0 in the structural operational style of Natural Semantics, using Typol inference rules within the Centaur system, a generic specification environment. Sisal is a strongly typed, applicative, single assignment language in use on a variety of parallel processors, including conventional multiprocessors, vector machines and data-flow machines. The motivations of our work are, with a formal semantic description of Sisal, to provide a firm foundation for understanding and evaluating language design issues, aid the elimination of ambiguities in the language, provide a valuable reference for both implementors and programmers, and facilitate comparison of Sisal with other parallel functional languages. At the same time, Centaur specifications automatically yield a structure editor and an interpreter for Sisal, which can be developed into an interactive environment for Sisal programming. Also, ..."
10.1.1.54.9972,Amoeba - A Distributed Operating System for the 1990s,1990,"Amoeba is the distributed system developed at the Free University (VU) and Centre for Mathematics and Computer Science (CWI), both in Amsterdam. Throughout the project's ten-year history, a major concern of the designers has been to combine the research themes of distributed systems, such as high availability, use of parallelism and scalability, with simplicity and high performance. Distributed systems are necessarily more complicated than centralized systems, so they have a tendency to be much slower. Amoeba was always designed to be used, so it was deemed essential to achieve extremely high performance. We are working hard to achieve this goal --- Amoeba is already one of the fastest distributed systems (on its class of hardware) reported so far in the scientific literature and future versions will be even faster. The Amoeba software is based on objects. An object is a piece of data on which well-defined operations may be performed by authorized users, independent of where the user a..."
10.1.1.54.8822,Tight Analyses of Two Local Load Balancing Algorithms,1995,".  This paper presents an analysis of the following load balancing algorithm. At each step, each node in a network examines the number of tokens at each of its neighbors and sends a token to each neighbor with at least 2d + 1 fewer tokens, where d  is the maximum degree of any node in the network. We show that within O(\Delta=ff) steps, the algorithm reduces the maximum difference in tokens between any two nodes to at most O((d  2  log n)=ff), where \Delta is the maximum difference between the number tokens at any node initially and the average number of tokens, n is the number of nodes in the network, and ff is the edge expansion of the network. The time bound is tight in the sense that for any graph with edge expansion ff, and for any value \Delta, there exists an initial distribution of tokens with imbalance \Delta for which the time to reduce the imbalance to even \Delta=2 is at least \Omega\Gammaa  =ff). The bound on the final imbalance is tight in the sense that there exists a cl..."
10.1.1.54.6389,Applicability of Reinforcement Learning,1998,"We describe our experiences in trying to implement a hierarchical reinforcement learning system, and follow with conclusions that we have drawn from the difficulties that we encountered. We present our objectives before we started, the problems we encountered along the way, the solutions we devised for some of these problems, and our conclusions afterward about the class of problems for which reinforcement learning may be suitable. Our experience has made it clearer to us when and when not to select a reinforcement learning method. Introduction  We describe our experiences in trying to implement a hierarchical reinforcement learning system. This includes our design objectives, the problems we encountered, the solutions we devised for some of these problems, and our conclusions afterward. These conclusions are somewhat lengthy, making the paper fall into two larger parts: the technical aspects of what we tried to do, and the thinking that our difficulties led us to do. Before proceeding..."
10.1.1.54.5535,Tight Analyses of Two Local Load Balancing Algorithms,1995,"This paper presents an analysis of the following load balancing algorithm. At each step, each node in a network examines the number of tokens at each of its neighbors and sends a token to each neighbor with at least 2d + 1 fewer tokens, where d is the maximum degree of any node in the network. We show that within O(\Delta=ff) steps, the algorithm reduces the maximum difference in tokens between any two nodes to at most O((d  2  log n)=ff), where \Delta is the maximum difference between the number tokens at any node initially and the average number of tokens, n is the number of nodes in the network, and  ff is the edge expansion of the network. The time bound is tight in the sense that for any graph with edge expansion ff, and for any value \Delta, there exists an initial distribution of tokens with imbalance \Delta for which the time to reduce the imbalance to even \Delta=2 is at least\Omega\Gammaa  =ff). The bound on the final imbalance is tight in the sense that there exists a class ..."
10.1.1.54.2933,Evolving Algebras and Partial Evaluation,1994,"We describe an automated partial evaluator for evolving algebras implemented at the University of Michigan. 1 Introduction to Sequential Evolving Algebras  A fuller discussion of evolving algebras (or ealgebras) can be found in [1]; to make this paper self-contained, we recall briefly the main concepts. A sequential ealgebra A is an abstract machine. The signature of A is a (finite) collection of function names, each of a fixed arity. A state of A is a set, the superuniverse, together with interpretations of the function names in the signature. These interpretations are called basic functions of the state. The superuniverse does not change as A evolves; the basic functions may. Formally, a basic function of arity r (i.e. the interpretation of a function name of arity r) is an r-ary  operation on the superuniverse. (We often use basic functions with r = 0; such basic functions will be called  distinguished elements.) But functions naturally arising in applications may be defined only on..."
10.1.1.54.1855,Automatic Synthesis of Control Programs in Polynomial Time for an Assembly Line,1996,"The industry wants provably correct and fast formal methods for handling combinatorial dynamical systems. One example of such problems is error recovery in industrial processes. We have used a provably correct, polynomial-time planning algorithm to plan for a miniature assembly line, which assembles toy cars. Although somewhat limited, this process has many similarities with real industrial processes. By exploring the structure of this assembly line we have extended a previously presented algorithm, thus extending the class of problems that can be handled in polynomial time. The planning tool presented here contains general-purpose algorithms that generate plans in the form of GRAFCET charts that are automatically translated into PLC code using a commercial PLC compiler.  Keywords: Planning, GRAFCET, sequential control, automated manufacturing 1 Introduction  The majority of all hardware and software developed for industrial control purposes is devoted to sequential control and only a ..."
10.1.1.54.1667,Computer Learning of Subjectivity,1996,"ing with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. However, rarely can models with semantic control knobs be found. Even when they exist, it is an effort to know how to optimally set them. Moreover, usually a person does not make a single query, but a succession of queries, with slight variations each time. Therefore, she not only needs to know how to set the knobs when initiating a query session, but also how to adjust them with each new query. What I have described is the current trend in content-based retrieval and annotation systems, and it needs to change. The system must recognize that the user's goals evolve while they browse; subjectivity, mood-dependence, and fickleness are to be expected. Furthermore, a system that tracks the evolving goals of a subjective human will also be helpful for the difficult but common query sessions best described as ""I'll know it when I see ..."
10.1.1.53.689,A Formal Specification of dMARS,1997,"The Procedural Reasoning System (PRS) is the best established agent architecture currently available. It has been deployed in many major industrial applications, ranging from fault diagnosis on the space shuttle to air traffic management and business process control. The theory of PRS-like systems has also been widely studied: within the intelligent agents research community, the belief-desire-intention (BDI) model of practical reasoning that underpins PRS is arguably the dominant force in the theoretical foundations of rational agency. Despite the interest in PRS and BDI agents, no complete attempt has yet been made to precisely specify the behaviour of real PRS systems. This has led to the development of a range of systems that claim to conform to the PRS model, but which differ from it in many important respects. Our aim in this paper is to rectify this omission. We provide an abstract formal model of an idealised dMARS system (the most recent implementation of the PRS architecture)..."
10.1.1.53.442,Performance Analysis Of A WWW Server,,"... This paper focuses on the performance analysis of Web servers. In the first part of the paper, we discuss the main steps to carry out a WWW performance analysis effort. Then, we show some experimental results. Using a synthetic benchmark (WebStone) and performance monitor tools, we analyze performance of three different Web server software running on top of a Windows NT platform. "
10.1.1.53.3337,Program Slicing,1992,"The concept of static program slicing was first introduced by Weiser. Ottenstein et al. indicated that an intraprocedural slice can be found in linear time by traversing a suitable graph representation of the program referred to as the program dependence graph (PDG). Horwitz et al. introduced algorithms to construct interprocedural slices by extending the program dependence graph to a supergraph of the PDG referred to as the system dependence graph (SDG). This extension captures the calling context of procedures.  In a previous paper, we demonstrated that a parse-tree-based SDG provides us with ""smaller"" and therefore more precise slices than a statement-based SDG. Furthermore, we described extensions to the SDG in order to handle particular constructs in a language that is a subset of ANSI C. In this paper, we will describe a new method for the calculation of transitive dependences and therefore build a SDG that does require neither the calculation of the GMOD and GREF sets nor the co..."
10.1.1.52.6558,Accurate SVDs of Structured Matrices,1997,"We present new O(n  3  ) algorithms to compute very accurate SVDs of Cauchy matrices, Vandermonde matrices, and related ""unit-displacement-rank"" matrices. These algorithms compute all the singular values with guaranteed relative accuracy, independent of their dynamic range. In contrast, previous O(n  3  ) algorithms can potentially lose all relative accuracy in the tiniest singular values.  LAPACK Working Note 130 University of Tennessee Computer Science Report ut-cs-97-375 1 Introduction  The singular value decomposition (SVD) of a real matrix G is the factorization G = U \SigmaV  T  where  U and V are orthogonal matrices and \Sigma is nonnegative and diagonal. If G is m-by-n, with m  n  (otherwise transpose G), then U is m-by-n, \Sigma = diag(oe 1 ; :::; oe n ) with oe 1 \Delta \Delta \Delta  oe n  0, and V  is n-by-n. We call the columns u i of U = [u 1 ; :::; u n ] the left singular vectors, the columns v i of  V = [v 1 ; :::; v n ] the right singular vectors, and the oe i the sing..."
10.1.1.52.2810,Intelligent Built-in Torque Sensor for Harmonic Drive Systems,1997,"A harmonic drive is a compact, light--weight and high--ratio torque transmission device which is used in many electrically actuated robot manipulators. In this paper a built--in torque sensor for harmonic drive systems is examined in detail. The method proposed by Hashimoto, in which strain--gauges are directly mounted on the flexspline, is employed and improved in this paper. To minimize sensing inaccuracy, four Rosette strain gauges are used employing an accurate positioning method. To cancel the torque ripples, the oscillation observed on the measured torque and caused mainly by gear teeth meshing, Kalman filter estimation is used. A simple forth order harmonic oscillator proved to accurately model the torque ripples. Moreover, the error model is extended to incorporate any misalignment torque. By on line implementation of the Kalman filter, it has been shown that this method is a fast and accurate way to filter torque ripples and misalignment torque. Keywords  Harmonic drive, torqu..."
10.1.1.52.2486,QoS Routing Mechanisms and OSPF Extensions,1998,"This memo describes extensions to the OSPF [Moy98] protocol to support QoS routes. The focus of this document is on the algorithms used to compute QoS routes and on the necessary modifications to OSPF to support this function, e.g., the information needed, its format, how it is distributed, and how it is used by the QoS path selection process. Aspects related to how QoS routes are established and managed are also briefly discussed. The goal of this document is to identify a framework and possible approaches to allow deployment of QoS routing capabilities with the minimum possible impact to the existing routing infrastructure. In addition, experience from an implementation of the proposed extensions in the GateD environment [Con], along with performance measurements is presented. Guerin, et al. Experimental [Page i]  RFC XXX QoS Routing Mechanisms December 1998 Contents Status of This Memo i Abstract i 1. Introduction 1 1.1. Overall Framework . . . . . . . . . . . . . . . . . . . . 1 1..."
10.1.1.51.8438,Mining Knowledge in Geographical Data,1998,"this article, a short overview is provided to summarize recent studies on spatial data mining, including spatial data mining techniques, their strengths and weaknesses, how and when to apply them, and what are the challenges yet to be faced."
10.1.1.51.7371,Continuity Of The Hausdorff Dimension For Invariant Subsets Of Interval Maps,39,". Let T : [0; 1] ! [0; 1] be an expanding piecewise monotonic map, and consider the set R of all points, whose orbits omit a certain finite union of open intervals. It is shown that the Hausdorff dimension HD (R) depends continuously on small perturbations of the endpoints of these open intervals. A similar result for the topological pressure is also obtained. Furthermore it is shown that for every  t 2 [0; 1] there exists a closed, T -invariant R t ` [0; 1] with HD (R t ) = t. Finally it is proved that the Hausdorff dimension of the set of all points, whose orbit is not dense, is 1. Introduction  Let T : [0; 1] ! [0; 1] be an expanding piecewise monotonic map, that means there exists a finite partition Z of [0; 1] into pairwise disjoint open intervals with  [Z2ZZ = [0; 1], T jZ is continuous and strictly monotone for all Z 2 Z, T  0  jZ  can be extended to a continuous function on Z for all Z 2 Z, and there exists an n 2 N with inf x2[0;1] j(T  n  )  0  (x)j ? 1. Fix a K 2 N. Let (a 1..."
10.1.1.51.7088,The Shape Of A Typical Boxed Plane Partition,1996,". Using a calculus of variations approach, we determine the shape of a typical plane partition in a large box (i.e., a plane partition chosen at random according to the uniform distribution on all plane partitions whose solid Young diagrams fit inside the box). Equivalently, we describe the distribution of the three different orientations of lozenges in a random lozenge tiling of a large hexagon. We prove a generalization of the classical formula of MacMahon for the number of plane partitions in a box; for each of the possible ways in which the tilings of a region can behave when restricted to certain lines, our formula tells the number of tilings that behave in that way. When we take a suitable limit, this formula gives us a functional which we must maximize to determine the asymptotic behavior of a plane partition in a box. Once the variational problem has been set up, we analyze it using a modification of the methods employed by Logan and Shepp and by Vershik and Kerov in their stud..."
10.1.1.51.6399,The Evolution of Subtle Manoeuvres in Simulated Hockey,1998,"A simulated hockey environment is introduced as a test bed for studying adaptive behavior and evolution of robot controllers. A near-frictionless playing surface is employed, partially mimicking zero gravity conditions. We show how a neural network using a simple evolutionary algorithm can develop nimble strategies for moving about the rink and scoring goals quickly and effectively.  1. Introduction  In recent years, there has been growing interest in using machine learning and evolutionary techniques for developing software controllers that play competitive or cooperative games in some kind of physical environment, either real or simulated. These have included: avoiding obstacles (Xiao et al., 1997; Lee et al., 1996; Brooks, 1986), foraging for ""food"" (Werger & Mataric, 1996), playing pursuit/evasion games (Miller & Cliff, 1994), discriminating objects (Beer, 1996), fighting for control of a cube (Sims, 1995) and RoboCup soccer (Kitano et al., 1995). Following in these traditions, we ..."
10.1.1.51.6359,Automatic Learning for Semantic Collocation,1992,"The real difficulty in development of practical NLP systems comes from the fact that we do not have effective means for gathering ""knowledge "". In this paper, we propose an algorithm which acquires automatically knowledge of semantic collocations among ""words"" from sample corpora. The algorithm proposed in this paper tries to discover semantic collocations which will be useful for disambiguating structurally ambiguous sentences, by a statistical approach. The algorithm requires a corpus and minimum linguistic knowledge (parts-of-speech of words, simple inflection rules, and a small number of general syntactic rules). We conducted two experiments of applying the algorithm to different corpora to extract different types of semantic collocations. Though there are some unsolved problems, the results showed the effectiveness of the proposed algorithm.  1 Introduction  Quite a few grammatical formalisms have been proposed by computational linguists, which are claimed to be ""good"" (declarativ..."
10.1.1.51.4463,Competition for Attention,1998,"This paper introduces 'Competition for Attention' as a conceptual design paradigm for agent-based multi-media applications. Our aim is to allow for the relative independent development of information services, on the one hand, and value-added agent-based services on the other, and to enable their smooth integration in a particular on-line application. The underlying and long-term objective is to deal in a principled way with the scaling problem, allowing for agentbased applications with hundreds to millions of active agents. Although we have not yet fully realized these objectives, the paper describes three phases of an existing practical application that demonstrates the step that we have taken toward this goal. The application is an on-line WWW service for Brussels summer movie festival Ecran Total. In three consecutive years, three agent-based applications have been developed with increasingly enhanced methodological and technical support for realizing the Competition for Attentio..."
10.1.1.50.9145,Computer Learning of Subjectivity,1996,"tic control knobs."" For example, the Wold model for retrieving perceptually similar visual patterns has knobs corresponding to periodicity, directionality, and randomness [2]. However, rarely can models with semantic control knobs be found. Even when they exist, it is an effort to know how to optimally set them. Moreover, usually a person does not make a single query, but a succession of queries, with slight variations each time. Therefore, she not only needs to know how to set the knobs when initiating a query session, but also how to adjust them with each new query. What I have described is the current trend in content-based retrieval and annotation systems, and it needs to change. The system must recognize that the user's goals evolve while they browse; subjectivity, mood-dependence, and fickleness are to be expected. Furthermore, a system that tracks the evolving goals of a subjective human will also be helpful for the difficult but common query sessions best described as ""I'll kno"
10.1.1.50.2773, 	 A Logic-based Knowledge Representation for Authorization with Delegation ,1999,"  We introduce Delegation Logic (DL), a logic-based knowledge representation (i.e., language) that deals with authorization in large-scale, open, distributed systems. Of central importance in any system for deciding whether requests should be authorized in such a system are delegation of authority, negation of authority, and conflicts between authorities. DL's approach to these issues and to the interplay among them borrows from previous work on delegation and trust management in the computer-security literature and previous work on negation and conflict handling in the logic-programming and non-monotonic reasoning literature, but it departs from previous work in some crucial ways. In this introductory paper, we present the syntax and semantics of DL and explain our novel design choices. This first paper focuses on delegation, including explicit treatment of delegation depth and delegation to complex principals; a forthcoming companion paper focuses on negation. Compared to previous lo..."
10.1.1.5.932,Lead Field Basis for FEM Source Localization  ,1999,"In recent years, significant progress has been made in the area of EEG/MEG source imaging. Source imaging on simple spherical models has become increasingly e#cient, with consistently reported accuracy of within 5mm. In contrast, source localization on realistic head models remains slow, with sub-centimeter accuracy being the exception rather than the norm. A primary reason for this discrepancy is that most source imaging techniques are based on lead-fields. While the lead-field for simplified geometries can be easily computed analytically, an e#cient method for computing realistic domain lead-fields has, until now, remained elusive. In this paper, we propose two e#cient methods for computing realistic EEG lead-field bases: the first is elementoriented, and the second is node-oriented. We compare these two bases, discuss how they can be used to apply recent source imaging methods to realistic models, and report timings for constructing the bases."
10.1.1.5.7843,Subtyping Patterns for Active Objects,2000,Subtyping relations for object-oriented formalisms describe relationships between super- and subclasses which satisfy the substitutability requirement imposed on types and their subtypes. For active objects with an associated behaviour description subtyping relations also have to guarantee substitutability with respect to the dynamic behaviour of classes. In this
10.1.1.5.4182,QoS Routing Mechanisms and OSPF Extensions,1997,"This memo describes extensions to the OSPF [Moy98] protocol to  support QoS routes. The focus of this document is on the algorithms used to compute QoS routes and on the necessary modifications to OSPF to support this function, e.g., the information needed, its format, how it is distributed, and how it is used by the QoS path selection process. Aspects related to how QoS routes are established and  managed are also briefly discussed. The goal of this document is to  identify a framework and possible approaches to allow deployment of QoS routing capabilities with the minimum possible impact to the existing routing infrastructure."
10.1.1.5.319,The Role of Trust Management in Distributed Systems Security,0,"Existing authorization mechanisms fail to provide powerful  and robust tools for handling security at the scale necessary for today's  Internet. These mechanisms are coming under increasing strain from the  development and deployment of systems that increase the programmability  of the Internet. Moreover, this #increased #exibility through programmability  "" trend seems to be accelerating with the advent of proposals  such as Active Networking and Mobile Agents."
10.1.1.5.2276,Advanced Polymorphic Worms: Evading IDS by Blending in with Normal Traffic,2004,"Normal traffic can provide worms with a very good source of information  to camouflage themselves. In this paper, we explore the concept of polymorphic  worms that mutate based on normal traffic. We assume that a worm has  already penetrated a system and is trying to hide its presence and propagation attempts  from an IDS. We focus on stealthy worms that cannot be reliably detected  by increases in traffic because of their low propagation factor. We first give an example  of a simple polymorphic worm. Such worms can evade a signature-based  IDS but not necessarily an anomaly-based IDS. We then show that it is feasible  for an advanced polymorphic worm to gather a normal traffic profile and use it to  evade an anomaly-based IDS. We tested the advanced worm implementation with  three anomaly IDS approaches: NETAD, PAYL and Service-specific IDS. None  of the three IDS approaches were able to detect the worm reliably. We found that  the mutated worm can also evade other detection methods, such as the Abstract  Payload Execution."
10.1.1.49.9700,Itinerant Agents for Mobile Computing,, 
10.1.1.49.7849,Resolving Ambiguity for Cross-language Retrieval,1998,"One of the main hurdles to improved CLIR effectiveness is resolving ambiguity associated with translation. Availability of resources is also a problem. First we present a technique based on co-occurrence statistics from unlinked corpora which can be used to reduce the ambiguity associated with phrasal and term translation. We then combine this method with other techniques for reducing ambiguity and achieve more than 90% monolingual effectiveness. Finally, we compare the co-occurrence method with parallel corpus and machine translation techniques and show that good retrieval effectiveness can be achieved without complex resources. 1 Introduction  Research in the area of cross-language information retrieval (CLIR) has focused mainly on methods for translating queries. Full document translation for large collections is impractical, thus query translation is a viable alternative. Methods for translation have focused on three areas: dictionary translation, parallel  or comparable corpora fo..."
10.1.1.49.6857,Competition for Attention,1998,"This paper introduces 'Competition for Attention' as a conceptual design paradigm for agent-based multi-media applications. Our aim is to allow for the relative independent development of information services, on the one hand, and value-added agent-based services on the other, and to enable their smooth integration in a particular on-line application. The underlying and long-term objective is to deal in a principled way with the scaling problem, allowing for agentbased applications with hundreds to millions of active agents. Although we have not yet fully realized these objectives, the paper describes three phases of an existing practical application that demonstrates the step that we have taken toward this goal. The application is an on-line WWW service for Brussels summer movie festival Ecran Total. In three consecutive years, three agent-based applications have been developed with increasingly enhanced methodological and technical support for realizing the Competition for Attention paradigm."
10.1.1.49.6041,Data Movement and Control Substrate for Parallel Scientific Computing,1997,"In this paper, we describe the design and implementation of a datamovement and control substrate (DMCS) for network-based, homogeneous communication within a single multiprocessor. DMCS is an implementation of an API for communication and computation that has been proposed by the PORTS consortium. One of the goals of this consortium is to define an API that can support heterogeneous computing without undue performance penalties for homogeneous computing. Preliminary results in our implementation suggest that this is quite feasible. The DMCS implementation seeks to minimize the assumptions made about the homogeneous nature of its target architecture. Finally, we present some extensions to the API for PORTS that will improve the performance of sparse, adaptive and irregular type of numeric computations. Keywords: parallel processing, runtime systems, communication, threads, networks    Appears in the Proceedings of Workshop on Communication and Architectural Support for Network-based Par..."
10.1.1.49.5842,Scheduling Dynamic Graphs,1998,"In parallel and distributed computing scheduling low level tasks on the available hardware is a fundamental problem. Traditionally, one has assumed that the set of tasks to be executed is known beforehand. Then the scheduling constraints are given by a precedence graph. Nodes represent the elementary tasks and edges the dependencies among tasks. This static approach is not appropriate in situations where the set of tasks is not known exactly in advance, for example, when different options how to continue a program may be granted. In this paper a new model for parallel and distributed programs, the dynamic process graph, will be introduced, which represents all possible executions of a program in a compact way. The size of this representation is small -- in many cases only logarithmically with respect to the size of any execution. An important feature of our model is that the encoded executions are directed acyclic graphs having a ""regular"" structure that is typical of parallel programs..."
10.1.1.49.565,"Topologies, Migration Rates, and Multi-Population Parallel Genetic Algorithms",1999,"This paper presents an analysis of parallel genetic algorithms (GAs) with multiple populations (demes). The analysis makes explicit the relation between the probability of reaching a desired solution with the deme size, the migration rate, and the degree of the connectivity graph. The analysis considers arbitrary topologies with a fixed number of neighbors per deme. The demes evolve in isolation until each converges to a unique solution. Then, the demes exchange an arbitrary number of individuals and restart their execution. An accurate deme-sizing equation is derived, and it is used to determine the optimal configuration of an arbitrary number of demes that minimizes the execution time of the parallel GA. 1 Introduction  Parallel genetic algorithms (GAs) with multiple populations are difficult to configure because they are controlled by many parameters that affect their efficiency and accuracy. Among other things, one must decide the number and the size of the populations (demes), the..."
10.1.1.49.5558,Abductive Plan Recognition and Diagnosis: A Comprehensive Empirical Evaluation,1992,"While it has been realized for quite some time within AI that abduction is a general model of explanation for a variety of tasks, there have been no empirical investigations into the practical feasibility of a general, logic-based abductive approach to explanation. In this paper we present extensive empirical results on applying a general abductive system, Accel, to moderately complex problems in plan recognition and diagnosis. In plan recognition,  Accel has been tested on 50 short narrative texts, inferring characters' plans from actions described in a text. In medical diagnosis,  Accel has diagnosed 50 real-world patient cases involving brain damage due to stroke (previously addressed by set-covering methods). Accel also uses abduction to accomplish model-based diagnosis of logic circuits (a full adder) and continuous dynamic systems (a temperature controller and the water balance system of the human kidney). The results indicate that general purpose abduction is an effective and ef..."
10.1.1.49.2865,Insertion of an Articulated Human into a Networked Virtual Environment,1994,This paper outlines the network and software architecture of the system.
10.1.1.49.1559,FDTD and Related Publications,1999,"this document, the BibT E X file used to generate it, and other descriptive files are available via anonymous ftp from ftp.eecs.wsu.edu in the directory /pub/FDTD. These files can also be obtained using WWW. The URL's are:"
10.1.1.48.930,FDTD and Related Publications,,"this document, the BibT E X file used to generate it, and other descriptive files are available via anonymous ftp from ftp.eecs.wsu.edu in the directory /pub/FDTD. These files can also be obtained using WWW. The URL's are:"
10.1.1.48.2833,Managing Change in Information Systems: Technological Challenges,,": Information systems and other computer-based systems must continuously undergo change in order to reflect change in their environments. The present technology used to implement such systems, including models, methods, tools and languages, does not have an inherent understanding of the nature of evolution. The rigidity of existing systems is a hindrance for user requested enhancements. Propagating changes correctly is a particular problem. It is common to find that necessary changes consequent on some other change have not been made, so that the system is inconsistent and will eventually fail to operate correctly. The paper discusses tools for system maintenance and focuses on the issue of automation. A tool that automatically generates and maintains all the information it needs is presented. To provide more information about the form and extent of the evolution in real-world systems, the same tool was instructed to collect change measurements. Information about the evolution of a lar..."
10.1.1.48.1387,"A Mixed Linear and Non-Linear Logic: Proofs, Terms and Models  ",1994,"Intuitionistic linear logic regains the expressive power of intuitionistic logic through the ! (`of course') modality. Benton, Bierman, Hyland and de Paiva have given a term assignment system for ILL and an associated notion of categorical model in which the ! modality is modelled by a comonad satisfying certain extra conditions. Ordinary intuitionistic logic is then modelled in a cartesian closed category which arises as a full subcategory of the category of coalgebras for the comonad. This paper attempts to explain the connection between ILL and IL more directly and symmetrically by giving a logic, term calculus and categorical model for a system in which the linear and non-linear worlds exist on an equal footing, with operations allowing one to pass in both directions. We start from the categorical model of ILL given by Benton, Bierman, Hyland and de Paiva and show that this is equivalent to having a symmetric monoidal adjunction between a symmetric monoidal closed category and a ca..."
10.1.1.47.9075,Itinerant Agents for Mobile Computing,1995,"This paper describes an abstract framework for itinerant agents that can be used to implement secure, remote applications in large, public networks such as the Internet or the IBM Global Network. Itinerant agents are programs, dispatched from a source computer, that roam among a set of networked servers until they accomplish their task. This is an extension to the client / server model in which the client sends a portion of itself to the server for execution. An additional feature of itinerant agents is their ability to migrate from server to server, perhaps seeking one that can help with the user's task or perhaps collecting information from all of them. A major focus of the paper is the Agent Meeting Point, an abstraction that supports the interaction of agents with each other and server based resources. Why is this extended form of client-server computing desirable or valuable? There are many detailed motivations for using itinerant agents [6]. They fall broadly into two categories: 1) support for mobile computers or lightweight devices and 2) the emerging need in rapidly evolving networks for an asynchronous method of searching for information or transaction services. For example: 1. The reduction of overall communication traffic over the low-bandwidth, high-latency, high-cost access networks typically employed by mobile computers. 2. The ability of the agent to engage in high-bandwidth communication (with a server, for example) to search through large, free text databases. 3. The ability of lightweight mobile computers to interact with heavyweight applications without prior, detailed knowledge of the remote server's capabilities. 4. The ability of the agent to integrate knowledge from the client and server and perform inferencing at the server. 5. The ability of th..."
10.1.1.47.8594,Blue - A Language for Teaching Object-Oriented Programming,1996,"Teaching object-oriented programming has clearly become an important part of computer science education. We agree with many others that the best place to teach it is in the CS1 introductory course. Many problems with this have been reported in the literature. These mainly result from inadequate languages and environments. Blue is a new language and integrated programming environment, currently under development explicitly for object-oriented teaching. We expect clear advantages from the use of Blue for first year teaching compared to using other available languages. This paper describes the design principles on which the language was based and the most important aspects of the language itself. 1 INTRODUCTION  Object-oriented languages are becoming increasingly widely used in software projects. Their importance for stateof -the-art software development is now generally accepted, and they have achieved popularity with academics and practitioners alike. As this trend has become clear, man..."
10.1.1.47.843,Blue -- A Language For Teaching Object-Oriented Programming,1996,"Teaching object-oriented programming has clearly become an important part of computer science education. We agree with many others that the best place to teach it is in the CS1 introductory course. Many problems with this have been reported in the literature. These mainly result from inadequate languages and environments. Blue is a new language and integrated programming environment, currently under development explicitly for object-oriented teaching. We expect clear advantages from the use of Blue for first year teaching compared to using other available languages. This paper describes the design principles on which the language was based and the most important aspects of the language itself.  "
10.1.1.46.8264,A Declarative Semantics for Behavioral Inheritance and Conflict Resolution,1995,"We propose a novel semantics for object-oriented deductive databases in the direction of F-logic to logically account for behavioral inheritance, conflict resolution in multiple inheritance hierarchies, and overriding. We introduce the ideas of withdrawal,  locality, and inheritability of properties (i.e., methods and signatures). Exploiting these ideas, we develop a declarative semantics of behavioral inheritance and overriding without having to resort to non-monotonic reasoning. Conflict resolution in our model can be achieved both via specification and by detection. The possibility of specification based conflict resolution through withdrawal allows users to define inheritance preference. We present a formal account of the semantics of our language by defining a model theory, proof theory and a fixpoint theory. We also show that the different characterizations of our language are equivalent.  Key Words: object-orientation, behavioral inheritance, overriding, conflict resolution, ded..."
10.1.1.46.7869,A Declarative Semantics for Behavioral Inheritance and Conflict Resolution,1995,"We propose a novel semantics for object-oriented deductive databases in the direction of F-logic to logically account for behavioral inheritance, conflict resolution in multiple inheritance hierarchies, and overriding. We introduce the ideas of withdrawal,  locality, and inheritability of properties (i.e., methods and signatures). Exploiting these ideas, we develop a declarative semantics of behavioral inheritance and overriding without having to resort to non-monotonic reasoning. Conflict resolution in our model can be achieved both via specification and by detection. The possibility of specification based conflict resolution through withdrawal allows users to define inheritance preference. We present a formal account of the semantics of our language by defining a model theory, proof theory and a fixpoint theory. We also show that the different characterizations of our language are equivalent.  "
10.1.1.46.7208,Attempto Controlled English (ACE),1999,Contents 1 What is Attempto Controlled English (ACE)? .................................................................1 2 ACE in a Nutshell.............................................................................................................2 3 Grammar of ACE..............................................................................................................9  3.1 Some Terminology............................................................................................................................. 9 3.2 Simple Sentences ............................................................................................................................. 10 3.2.1 Basic Form .......................................................................................................................... 10 3.2.2 Modifying Nouns and Noun Phrases .................................................................................. 12 3.2.2.1 Adjectives ...................
10.1.1.46.6466,Decoupled Simulation in Virtual Reality with The MR Toolkit,1993,"The Virtual Reality (VR) user interface style allows natural hand and body motions to manipulate  virtual objects in 3D environments using one or more 3D input devices. This style is best suited  to application areas where traditional two-dimensional styles fall short, such as scientific visualization,  architectural visualization, and remote manipulation. Currently, the programming effort required to  produce a VR application is too large, and many pitfalls must be avoided in the creation of successful  VR programs. In this paper we describe the Decoupled Simulation Model for creating successful VR  applications, and a software system that embodies this model. The MR Toolkit simplifies the development  of VR applications by providing standard facilities required by a wide range of VR user interfaces.  These facilities include support for distributed computing, head-mounted displays, room geometry management,  performance monitoring, hand input devices, and sound feedback. The MR Toolk..."
10.1.1.45.9371,Managing Surrogate Objectives to Optimize a Helicopter Rotor Design -- Further Experiments,1998,"It is common engineering practice to use response surface approximations as surrogates for an expensive objective function in engineering design. The rationale is to reduce the number of detailed, costly analyses required during optimization. In earlier work, we developed a rigorous and effective scheme for managing the interplay between the use of surrogates in the optimization and scheduled progress checks with the expensive analysis so that the process converges to a solution of the original design problem. In this paper, we will report our latest numerical tests with a helicopter rotor design problem which has proved to be a fruitful laboratory for experimentation. The results given here support the use of an ANOVA decomposition on a DACE model to identify the most important optimization variables in an optimal design problem. "
10.1.1.45.8831,Mining Knowledge in Geographical Data,1998,"this article, a short overview is provided to summarize recent studies on spatial data mining, including spatial data mining techniques, their strengths and weaknesses, how and when to apply them, and what are the challenges yet to be faced."
10.1.1.45.6507,On the Computation Power of Randomized Branching Programs,1998,"We survey some upper and lower bounds established recently on the sizes of randomized branching programs computing explicit boolean functions. In particular, we display boolean functions on which randomized  read-once ordered branching programs are exponentially more powerful than deterministic or nondeterministic read-k-times branching programs for any k = o(n=log n). We investigate further computational power of  randomized read-once order branching programs (OBDDs) and their basic manipulation properties for verification of boolean functions and for testing graphs of arithmetic functions.  "
10.1.1.45.560,TIA Software User's Manual,,"this report is for accurate reporting and does not constitute an official endorsement, either expressed or implied, of such products or manufacturers by the National Aeronautics and Space Administration. Arvind R. Prabhu, Analytical Services & Materials, Inc., Hampton, Virginia, has also contributed to the development of the Thermal Imaging Application. Work was supported under contracts NAS119236 and NAS1-20043 with Analytical Services & Materials, Inc., Hampton, Virginia. Available electronically at the following URL address: http://techreports.larc.nasa.gov/ltrs/ltrs.html  iii"
10.1.1.45.4417,A Guide to Complexity Theory in Operations Research,1996,": It is a well-known fact that there exists an ever increasing number of problems for which, despite the efforts of many inventive and persistent researchers, it seems virtually impossible to find efficient algorithms. In this situation, the theory of computational complexity may provide helpful insight into how probable the existence of such algorithms is at all. Unluckily, some of its concepts can still be found to be used erroneously, if at all. For instance, it is a common misunderstanding that any problem that generalizes an NP-complete problem is NP-complete or NP-hard itself; indeed any such generalization could as well be exponential in the worst case, i.e. solvable with effort exponentially increasing in the size of the instances attempted. In this work we develop the basic concepts of complexity theory. While doing so, we aim at presenting the material in a way that emphasizes the correspondences between the kind of problems considered in operations research and the formal pr..."
10.1.1.45.4313,On the Computation Power of Randomized Branching Programs,1996,"  We survey some upper and lower bounds established recently on the sizes of randomized branching programs computing explicit boolean functions. In particular, we display boolean functions on which randomized  read-once ordered branching programs are exponentially more powerful than deterministic or nondeterministic read-k-times branching programs for any k = o(n=log n). We investigate further computational power of  randomized read-once order branching programs (OBDDs) and their basic manipulation properties for verification of boolean functions and for testing graphs of arithmetic functions.  "
10.1.1.45.1315,Communication Issues in Heterogeneous Embedded Systems,1996,The recent accelerated development of scalable computing systems has made possible the coordinated use of a suite of High Performance Computing (HPC) components for computationally demanding problems in embedded applications. These emerging Scalable Heterogeneous High Performance Embedded (SHHiPE) systems are designed using commercial-offthe -shelf (COTS) modules. Our current interest is to employ these platforms to solve variety of problems in real-time signal processing. Large performance gains can be realized by exploiting knowledge of the computational structure of an algorithm through data remapping. We present the motivation for a portable programming paradigm that captures key features of a SHHiPE platform. The Message Passing Interface (MPI) standard is proposed as a basis for development of this paradigm. An application in sonar is used to illustrate typical communication requirements in SHHiPE systems. 1 Introduction  Scalable Heterogeneous High Performance Embedded (SHHiPE) ...
10.1.1.44.9058,Algorithms for Index-Assisted Selectivity Estimation,1998,"The standard mechanisms for query selectivity estimation used in relational database systems rely on properties specific to the attribute types. The query optimizer in an extensible database system will, in general, be unable to exploit these mechanisms for user-defined types, forcing the database extender to invent new estimation mechanisms. In this work, we discuss extensions to the generalized search tree, or GiST, that simplify the creation of user-defined selectivity estimation methods. An experimental comparison of such methods with multidimensional estimators from the literature has demonstrated very competitive results. 1. Motivation and General Approach  There has been considerable research in the development of query selectivity estimation techniques, but relatively little in the context of frameworks that can be applied to arbitrary user-defined types. The general frameworks that have been proposed tend to be conceptually simple APIs that impose significant implementation co..."
10.1.1.44.8237,Average-Case Complexity of Shortest-Paths Problems in the Vertex-Potential Model,2000,"We study the average-case complexity of shortest-paths problems in the vertex-potential model. The vertex-potential model is a family of probability distributions on complete directed graphs with arbitrary  real edge lengths but without negative cycles. We show that on a graph with n vertices and with respect to this model, the single-source shortest-paths problem can be solved in O(n²) expected time, and the all-pairs shortest-paths problem can be solved in O(n² log n) expected time. "
10.1.1.44.641,The Future of Spin Networks,1997,"Since Roger Penrose first introduced the notion of a spin network as a simple model of discrete quantum geometry, they have reappeared in quantum gauge theories, quantum gravity, topological quantum field theory and conformal field theory. The roles that spin networks play in these contexts are briefly described, with an emphasis on the question of the relationships among them. It is also argued that spin networks and their generalizations provide a language which may lead to a unification of the different approaches to quantum gravity and quantum geometry. This leads to a set of conjectures about the form of a future theory that may be simultaneously an extension of the non-perturbative quantization of general relativity and a non-perturbative formulation of string theory.    smolin@phys.psu.edu  ""A reformulation is suggested in which quantities normally requiring continuous coordinates for their description are eliminated from primary consideration. In particular, since space and ti..."
10.1.1.44.5598,A Fast Table-Driven Method for Edge Thinning and Linking,1999,"This paper describes fast but effective methods for edge thinning and linking. It has its genesis partly in the idea that it is often preferable to perform simple actions quickly rather than to attempt to do more sophisticated things that take longer. This idea is particularly attractive in the area of real-time computer vision, where the flow of input data may be otherwise overwhelming. We describe computationally cheap table-driven versions of the conventional techniques of edge thinning and edge linking that involve no transcendental functions and only integer arithmetic. Details are given of their computational speed on some representative images. 1. Introduction Edges are important features for object recognition and for matching for stereo and motion analysis. Edge features can be obtained from raw edge detector responses by a process of edge thinning and edge linking. Here we present fast table-driven techniques to perform edge thinning and linking. The basic approach of edge de..."
10.1.1.44.4010,The Shape of a Typical Boxed Plane Partition,1998,". Using a calculus of variations approach, we determine the shape of a typical plane partition in a large box (i.e., a plane partition chosen at random according to the uniform distribution on all plane partitions whose solid Young diagrams fit inside the box). Equivalently, we describe the distribution of the three different orientations of lozenges in a random lozenge tiling of a large hexagon. We prove a generalization of the classical formula of MacMahon for the number of plane partitions in a box; for each of the possible ways in which the tilings of a region can behave when restricted to certain lines, our formula tells the number of tilings that behave in that way. When we take a suitable limit, this formula gives us a functional which we must maximize to determine the asymptotic behavior of a plane partition in a box. Once the variational problem has been set up, we analyze it using a modification of the methods employed by Logan and Shepp and by Vershik and Kerov in their stud..."
10.1.1.44.2857,Propagation Rule Compiler: Technical Documentation,1996,The Propagation Rule Compiler (PROP) is a tool developed within the ESPRIT III project IDEA (Intelligent Database Environment for Advanced Applications) . It aims at supporting developers of Chimera applications during schema design and prototyping. PROP consists of two components: the rule compiler as such and an explanation facility. The task of the explanation facility is to graphically illustrate the logical dependencies established via deductive rules of a given Chimera schema. The purpose of the compilation unit is to generate update propagation triggers from deductive rules. Such triggers are able to automatically compute all implicit changes of a Chimera database once a specific updating transaction has been issued. PROP has been integrated as a subcomponent into Bonn's Chimera Prototyping Tool (CPT). The purpose of this document is to provide the technical documentation of the rule compilation process.   This report has been issued to the ESPRIT III project 6333 (IDEA) as deli...
10.1.1.44.2822,Usability of Parallel I/O Templates,,"This paper presents an alternative high-level approach to parallelizing file I/O. Each parallel file descriptor is annotated with a high-level specification, or template, of the expected parallel behaviour. The annotations are external to and independent of the source code. At run-time, all I/O using a template file descriptor adheres to the semantics of the selected template. By separating the parallel I/O behaviour from the code, a user can quickly change I/O semantics without rewriting code. Templates can be combined hierarchically to allow the simple construction of complex access patterns. Two sample parallel programs using these templates are compared against versions implemented in an existing parallel I/O system (PIOUS). The sample programs show that the use of parallel I/O templates are beneficial from both the performance and software engineering points of view.  1 Introduction  The development of parallel applications has focused on computational parallelism. However, the co..."
10.1.1.43.9196,A Study of User Participation in Standards Setting,1996,"This paper explores the views of members of standards setting organisations in the field of electronic communications. It focuses in particular on their experiences of, and attitudes towards, user participation in standards setting. KEYWORDS: standardisation, e-mail, user requirements INTRODUCTION AND MOTIVATION Conventional approaches to the usability of IT privilege the role of design. Elsewhere we have argued for a broader perspective that addresses the whole of the technology life cycle [3]. For example, many important decisions have already been made before designers begin their task. As a result, IT design work increasingly incorporates, and is structured around, standard components which collectively define the technical framework within which design must be accomplished. In the past, the growing requirement for system interoperability has established the need for standardised communications services. More recently, communications standards have targeted higher level services [4..."
10.1.1.43.8921,Restructuring Partitioned Normal Form Relations Without Information Loss,1997,"Nested relations in partitioned normal form (PNF) are an important subclass of nested relations that are useful in many applications. In this paper we address the question of determining when every PNF relation stored under one nested relation scheme can be transformed into another PNF relation stored under a different nested relation scheme without loss of information, referred to as the two schemes being data equivalent. This issue is important in many database application areas such as view processing, schema integration and schema evolution. The main result of the paper provides two characterisations of data equivalence for nested schemes. The first is that two schemes are data equivalent if and only if the two sets of multivalued dependencies induced by the two corresponding scheme trees are equivalent. The second is that the schemes are equivalent if and only if the corresponding scheme trees can be transformed into the other by a sequence of applications of a local restructuring..."
10.1.1.43.8609,Model based Bayesian Exploration,1999,Reinforcement learning systems are often concerned with balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classical notion of Value of Information --- the expected improvement in future decision quality arising from the information acquired by exploration. Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states. In this paper we investigate ways to represent and reason about this uncertainty in algorithms where the system attempts to learn a model of its environment. We explicitly represent uncertainty about the parameters of the model and build probability distributions over Qvalues based on these. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. 1 Introduction  Reinforcement learnin...
10.1.1.42.5336,Querying as an Enabling Technology in Software Reengineering,1999,"In this paper it is argued that different kinds of reengineering technologies can be based on querying. Several reengineering technologies are presented as being integrated into a technically oriented reengineering taxonomy. The usefulness of querying is pointed out with respect to these reengineering technologies.  To impose querying as a base technology in reengineering examples are given with respect to the EER/GRAL approach to conceptual modeling and implementation. This approach is presented together with GReQL as its query part. The different reengineering technologies are finally reviewed in the context of the GReQL query facility.  1 Introduction  Reengineering may be viewed as any activity that either improves the understanding of a software or else improves the software itself [2].  According to this view software reengineering can be ""partitioned"" into two kinds of activities. The first kind of activities is concerned with understanding such as source code retrieval, browsin..."
10.1.1.42.4604,Domains of Concern in Software Architectures and Architecture Description Languages,1997,"Software architectures shift the focus of developers from lines-of-code to coarser-grained elements and their interconnection structure. Architecture description languages (ADLs) have been proposed as domain-specific languages for the domain of software architecture. There is still little consensus in the research community on what problems are most important to address in a study of software architecture, what aspects of an architecture should be modeled in an ADL, or even what an ADL is. To shed light on these issues, we provide a framework of architectural domains, or areas of concern in the study of software architectures. We evaluate existing ADLs with respect to the framework and study the relationship between architectural and application domains. One conclusion is that, while the architectural domains perspective enables one to approach architectures and ADLs in a new, more structured manner, further understanding of architectural domains, their tie to application domains, and ..."
10.1.1.41.871,Efficient Approximation and Optimization Algorithms for Computational Metrology,1997,"We give efficient algorithms for solving several geometric problems in computational metrology, focusing on the fundamental issues of ""flatness"" and ""roundness."" Specifically, we give approximate and exact algorithms for 2- and 3-dimensional roundness primitives, deriving results that improve previous approaches in several respects, including problem definition, running time, underlying computational model, and dimensionality of the input. We also study methods for determining the width of a d-dimensional point set, which corresponds to the metrology notion of ""flatness,"" giving an approximation method that can serve as a fast exactcomputation filter for this metrology primitive. Finally, we report on experimental results derived from implementation and testing, particularly in 3-space, of our approximation algorithms, including several heuristics designed to significantly speed-up the computations in practice.  1 Introduction  Dimensional Tolerancing and Metrology (DT&M) is concerned ..."
10.1.1.41.5224,Properties of Terms in Continuation-Passing Style in an Ordered Logical Framework,2000,"Machine  We now begin extending our representation to include evaluation of CPS terms. We will begin by showing a representation of a naive evaluator which makes no use of the ordering invariants. The following is a bare abstract machine for CPS evaluation.  `  Exp B  e ,! a  `  Root B k: e ,! a `  Exp B  k t ,! t `  Exp B  e[t=v] ,! a  `  Root B (v: e) t ,! a  `  Exp B  e[t=x][c=k] ,! a  `  Root B (x: k: e) t c ,! a 9  Notice that this machine describes a regular big-step operational semantics for a -calculus---  every redex is reduced by a substitution. We introduce two type constructors to represent bare evaluations:  evalr B : root ! triv ! type: evale B : exp ! triv ! type:  Additionally, we introduce a new object to our signature for CPS terms:  ret : cont ret  1  will be substituted for the continuation identifiers, k, in bare evaluations. In order to make the inverse representation function on objects well-defined, we augment it with a continuation identifier  k when applied t..."
10.1.1.41.3037,"Manageability, Availability and Performance in Porcupine: A Highly Scalable, Cluster-Based Mail Service",1999,"This paper describes the motivation, design, and performance of Porcupine, a scalable mail server. The goal of Porcupine is to provide a highly available and scalable electronic mail service using a large cluster of commodity PCs. We designed Porcupine to be easy to manage by emphasizing dynamic load balancing, automatic configuration, and graceful degradation in the presence of failures. Key to the system's manageability, availability, and performance is that sessions, data, and underlying services are distributed homogeneously and dynamically across nodes in a cluster. 1 Introduction The growth of the Internet has led to the need for highly scalable and highly available services. This paper describes the Porcupine scalable electronic mail service. Porcupine achieves scalability by clustering many small machines (PCs), enabling them to work together in an efficient manner. In this section, we describe system requirements for Porcupine, relate the rationale for choosing a mail applic..."
10.1.1.41.2580,DyC: An Expressive Annotation-Directed Dynamic Compiler for C,1998,"We present the design of DyC, a dynamic-compilation system for C based on run-time specialization. Directed by a few declarative user annotations that specify the variables and code on which dynamic compilation should take place, a binding-time analysis computes the set of run-time constants at each program point in the annotated procedure's control-flow graph; the analysis supports programpoint -specific polyvariant division and specialization. The analysis results guide the construction of a specialized run-time specializer for each dynamically compiled region; the specializer supports various caching strategies for managing dynamically generated code and mixes of speculative and demand-driven specialization of dynamic branch successors. Most of the key cost/benefit trade-offs in the binding-time analysis and the run-time specializer are open to user control through declarative policy annotations.  Our design has been implemented in the context of an existing optimizing compiler, and..."
10.1.1.40.9299,The Click modular router,1999,"Click is a new software architecture for building flexible and configurable routers. A Click router is assembled from packet processing modules called elements. Individual elements implement simple router functions like packet classification, queueing, scheduling, and interfacing with network devices. Complete configurations are built by connecting elements into a graph; packets flow along the graph's edges. Several features make individual elements more powerful and complex configurations easier to write, including pull processing, which models packet flow driven by transmitting interfaces, and flow-based router context, which helps an element locate other interesting elements.  We demonstrate several working configurations, including an IP router and an Ethernet bridge. These configurations are modular---the IP router has 16 elements on the forwarding path---and easy to extend by adding additional elements, which we demonstrate with augmented configurations. On commodity PC hardware ..."
10.1.1.40.9196,A Formal Specification of dMARS,1997," The Procedural Reasoning System (PRS) is the best established agent architecture currently available. It has been deployed in many major industrial applications, ranging from fault diagnosis on the space shuttle to air traffic management and business process control. The theory of PRS-like systems has also been widely studied: within the intelligent agents research community, the beliefdesire -intention (BDI) model of practical reasoning that underpins PRS is arguably the dominant force in the theoretical foundations of rational agency. Despite the interest in PRS and BDI agents, no complete attempt has yet been made to precisely specify the behaviour of real PRS systems. This has led to the development of a range of systems that claim to conform to the PRS model, but which differ from it in many important respects. Our aim in this paper is to rectify this omission. We provide an abstract formal model of an idealised dMARS system (the most recent implementation of the PRS architectur..."
10.1.1.40.7389,Communication Issues in Heterogeneous Embedded Systems,1996,The recent accelerated development of scalable computing systems has made possible the coordinated use of a suite of High Performance Computing (HPC) components for computationally demanding problems in embedded applications. These emerging Scalable Heterogeneous High Performance Embedded (SHHiPE) systems are designed using commercial-offthe -shelf (COTS) modules. Our current interest is to employ these platforms to solve variety of problems in real-time signal processing. Large performance gains can be realized by exploiting knowledge of the computational structure of an algorithm through data remapping. We present the motivation for a portable programming paradigm that captures key features of a SHHiPE platform. The Message Passing Interface (MPI) standard is proposed as a basis for development of this paradigm. An application in sonar is used to illustrate typical communication requirements in SHHiPE systems. 1 Introduction  Scalable Heterogeneous High Performance Embedded (SHHiPE) ...
10.1.1.40.4852,Towards a Computational Theory of Rat Navigation,1994,"ut, and place fields can form when the animal explores novel environments in the dark. Place cells also continue to fire when distal landmarks are removed, but permutation of landmarks causes the animal to behave as if it were in an unfamiliar environment. Finally, place cell firing may be dependent on head direction, at least under certain conditions. An acceptable model of place memory must allow the ""current place"" to be updated by non-visual means such as motor feedback, and must be both sensitive to visual cues and robust in their absence. We propose a computational theory of the core of rat navigation abilities, based on coupled mechanisms for path integration, place recognition, and maintenance of head direction. We assume the rat has a path integration system (see [Etienne 1987, Mittelstaedt & Mittelstaedt 1980]) that is able to keep track of its current position relative to selected reference points. We postulate that hippocampal pyramidal cells form place descriptions by lear"
10.1.1.40.4535,Credit-Flow-Controlled ATM for MP Interconnection: the ATLAS I Single-Chip ATM Switch,1998,"Multiprocessing (MP) on networks of workstations (NOW) is a high-performance computing architecture of growing importance. In traditional MP's, wormhole routing interconnection networks use fixed-size flits and backpressure. In NOW's, ATM-one of the major contending interconnection technologies- uses fixed-size cells, while backpressure can be added to it. We argue that ATM with backpressure has interesting similarities with wormhole routing. We are implementing ATLAS I, a single-chip gigabit ATM switch, which includes credit flow control (backpressure), according to a protocol resembling Quantum Flow Control (QFC). We show by simulation that this protocol performs better than the traditional multi-lane wormhole protocol: high throughput and low latency are provided with less buffer space. Also, ATLAS I demonstrates little sensitivity to bursty traffic, and, unlike wormhole, it is fair in terms of latency in hot-spot configurations. We use detailed switch models, operating at clock-cyc..."
10.1.1.40.4007,What You Always Wanted to Know About Genetic Algorithms But Were Afraid to Hear,,"In spite of their seemingly \obvious"" virtues  as a search strategy, genetic algorithms have  ended up playing only a modest role as design  tools in science and engineering. We review  the reasons for this apparent failure, and we  suggest a more relaxed view of their utility.  1 INTRODUCTION  \Adaptive evolution is the motor of biology. But its mechanisms are so general that they should be eective in shaping articial systems as well!"" This was the manifesto John Holland read out at the beginning of the 70's. What many heard, or pretended to hear, was a message with more haste and more hubris: \Biological evolution works wonders. Let us apply its methods to engineering and we'll work wonders ourselves!""  Thus, the genetic algorithm (GA) soon found its way in the engineer's toolbox, to be employed as a readyto -use feedback loop for design optimization (Shaer 1999); the problem-specic part is supplied through a \tness function"". This combination works much like the generic operati..."
10.1.1.40.2308,Evolving Algebras and Partial Evaluation,1994,We describe an automated partial evaluator for evolving algebras implemented at the University of Michigan.
10.1.1.40.1501,Domains of Concern in Software Architectures and Architecture Description Languages,1997,"Software architectures shift the focus of developers from lines-of-code to coarser-grained elements and their interconnection structure. Architecture description languages (ADLs) have been proposed as domain-specific languages for the domain of software architecture. There is still little consensus in the research community on what problems are most important to address in a study of software architecture, what aspects of an architecture should be modeled in an ADL, or even what an ADL is. To shed light on these issues, we provide a framework of architectural domains, or areas of concern in the study of software architectures. We evaluate existing ADLs with respect to the framework and study the relationship between architectural and application domains. One conclusion is that, while the architectural domains perspective enables one to approach architectures and ADLs in a new, more structured manner, further understanding of architectural domains, their tie to application domains, and ..."
10.1.1.40.1490,"The Architectural Costs of Streaming I/O: A Comparison of Workstations, Clusters, and SMPs",1998,"We investigate resource usage while performing streaming I/O by contrasting three architectures, a single workstation, a cluster, and an SMP, under various I/O benchmarks. We derive analytical and empiricallybased models of resource usage during data transfer, examining the I/O bus, memory bus, network, and processor of each system. By investigating each resource in detail, we assess what comprises a wellbalanced system for these workloads.  We find that the architectures we study are not well balanced for streaming I/O applications. Across the platforms, the main limitation to attaining peak performance is the CPU, due to lack of data locality. Increasing processorperformance (especially with improved block operation performance) will be of great aid for these workloads in the future. For a cluster workstation, the I/O bus is a major system bottleneck, because of the increased load placed on it from network communication. Awell-balanced cluster workstation should have copious I/O bus b..."
10.1.1.4.7825,Processor Acceleration Through Automated Instruction Set Customization,2003,"Application-specific extensions to the computational capabilities of a processor provide an efficient mechanism to meet the growing performance and power demands of embedded applications. Hardware, in the form of new function units (or co-processors), and the corresponding instructions, are added to a baseline processor to meet the critical computational demands of a target application. The central challenge with this approach is the large degree of human effort required to identify and create the custom hardware units, as well as porting the application to the extended processor. In this paper, we present the design of a system to automate the instruction set customization process. A dataflow graph design space exploration engine efficiently identifies profitable computation subgraphs from which to create custom hardware, without artificially constraining their size or shape. The system also contains a compiler subgraph matching framework that identifies opportunities to exploit and generalize the hardware to support more computation graphs. We demonstrate the effectiveness of this system across a range of application domains and study the applicability of the custom hardware across the domain."
10.1.1.4.1954, 	 Abductive plan recognition and diagnosis: A comprehensive empirical evaluation ,1992,"While it has been realized for quite some time  within AI that abduction is a general model  of explanation for a variety of tasks, there  have been no empirical investigations into the  practical feasibility of a general, logic-based  abductive approach to explanation. In this  paper we present extensive empirical results  on applying a general abductive system, Accel,  to moderately complex problems in plan  recognition and diagnosis. In plan recognition,   Accel has been tested on 50 short narrative  texts, inferring characters' plans from  actions described in a text. In medical diagnosis,   Accel has diagnosed 50 real-world  patient cases involving brain damage due to  stroke (previously addressed by set-covering  methods). Accel also uses abduction to accomplish  model-based diagnosis of logic circuits  (a full adder) and continuous dynamic  systems (a temperature controller and the  water balance system of the human kidney). The"
10.1.1.4.1254,"Misperception, Communication and Diversity",2002,It is commonly agreed upon that misperception is detrimental.
10.1.1.39.6147,Insertion of an Articulated Human into a Networked Virtual Environment,1994,This paper
10.1.1.39.1298,Lazy Rewriting on Eager Machinery,1995,"We define Lazy Term Rewriting Systems and show that they can be realized by local adaptations of an eager implementation of conventional term rewriting systems. The overhead of lazy evaluation is only incurred when lazy evaluation is actually performed. Our method is modelled by a transformation of term rewriting systems, which concisely expresses the intricate interaction between pattern matching and lazy evaluation. The method easily extends to term graph rewriting.  CR Subject Classification (1991): D.3.4 [Programming languages]: Processors -- Compilers, Optimization; D.1.1 [Programming Techniques]: Applicative (Functional) Programming; D.1.6: Logic Programming.  AMS Subject Classification (1991): 68N20: Compilers and generators; 68Q05: Models of Computation; 68Q42: Rewriting Systems  Keywords & Phrases: lazy term rewriting, program transformation.  Note: Partial support received from the European Communities under the ESPRIT project 5399 (Compiler Generation for Parallel Machines -..."
10.1.1.38.6987,Reducing Disjunctive to Non-Disjunctive Semantics by Shift-Operations,1996,"It is wellknown that Minker's semantics GCWA for positive disjunctive programs P is  P  P  2 -complete , i.e. to decide if a literal is true in all minimal models of P . This is in  contrast to the same entailment problem for semantics of non-disjunctive programs such  as STABLE and SUPPORTED (both are co-NP-complete) as well as M  supp  P and WFS  (that are even polynomial).  Recently, the idea of reducing disjunctive to non-disjunctive programs by using so  called shift-operations was introduced independently by the authors and Marco Schaerf.  In fact, Schaerf associated to each semantics SEM for normal programs a corresponding  semantics Weak-SEM for disjunctive programs and asked for the properties of these  weak semantics, in particular for the complexity of their entailment relations. While  Schaerf concentrated on Weak-STABLE and Weak-SUPPORTED, we investigate the  weak versions of Apt, Blair, and Walker's stratified semantics M  supp  P  and of Van Gelder,  Ross, and Schlipf's ..."
10.1.1.38.6121,Confidence Measure Based Language Identification,2000,"In this paper we present a new application for confidence measures in spoken language processing. In today's computerized dialogue systems, language identification (LID) is typically achieved via dedicated modules. In our approach, LID is integrated into the speech recognizer, therefore profiting from high-level linguistic knowledge at very little extra cost. Our new approach is based on a word lattice based confidence measure [3], which was originally devised for unsupervised training. In this work, we show that the confidence based language identification algorithm outperforms conventional score based methods. Also, this method is less dependent on the acoustic characteristics of the transmission channel than score based methods. By introducing additional parameters, unknown languages can be rejected. The proposed method is compared to a score based approach on the Verbmobil database, a three language task."
10.1.1.38.4946,Amoeba --- A Distributed Operating System for the 1990s,1990,"Amoeba is the distributed system developed at the Free University (VU)  and Centre for Mathematics and Computer Science (CWI), both in Amsterdam.  Throughout the project's ten-year history, a major concern of the designers has  been to combine the research themes of distributed systems, such as high availability,  use of parallelism and scalability, with simplicity and high performance.  Distributed systems are necessarily more complicated than centralized systems,  so they have a tendency to be much slower. Amoeba was always designed to be  used, so it was deemed essential to achieve extremely high performance. We are  working hard to achieve this goal --- Amoeba is already one of the fastest distributed  systems (on its class of hardware) reported so far in the scientific literature  and future versions will be even faster.  The Amoeba software is based on objects. An object is a piece of data on  which well-defined operations may be performed by authorized users, independent  of whe..."
10.1.1.38.3573,"Is It ""Economics and Psychology""?: The Case of Hyperbolic Discounting",,"The paper questions the methodology of ""economics and psychology"". It focuses on the case of hyperbolic discounting. Using some experimental results, I argue that the same sort of evidence which rejects the standard constant discount utility functions can just as easily reject hyperbolic discounting as well. Furthermore, a decision-making procedure based on similarity relations better explains the observations and is more intuitive. The paper concludes that combining ""economics and psychology"" requires opening the black box of decision-makers rather than modifying functional forms. Key words: Hyperbolic discounting, choice over time, similarity, procedure, decision making.  Page 3 1. Introduction  My interest in this paper is how we, theoretical economists, interpret experimental evidence to justify our assumptions. Rather than presenting an abstract discussion, I will focus on one currently fashionable topic in the application of economic theory: ""hyperbolic discounting"". A recent sp..."
10.1.1.38.2752,Speculative Execution And Instruction-Level Parallelism,1994,"Full exploitation of instruction-level parallelism by superscalar and similar  architectures requires speculative execution, in which we are willing to issue a  potential future instruction early even though an intervening branch may  send us in another direction entirely. Speculative execution can be based either  on branch prediction, where we explore the most likely path away from  the branch, or on branch fan-out, in which we explore both paths and  sacrifice some hardware parallelism for the sake of not being entirely wrong.  Recent techniques for branch prediction have greatly improved its potential  success rate; we measure the effect this improvement has on parallelism. We  also measure the effect of fan-out, alone and also in combination with a  predictor. Finally, we consider the effect of fallible instructions, those that  might lead to spurious program failure if we execute them speculatively;  simply refusing to do so can drastically reduce the parallelism. "
10.1.1.38.1982,A Mechanical Verification of the Alternating Bit Protocol,1981,"The Alternating Bit Protocol has been modeled via a straighforward application of the Gypsy methodology. A safety property was stated for its service specification and a procedural protocol specification was written using Gypsy procedure definitions. Mechanical verification was carried out, including proofs of the supporting lemmas. A unique aspect of this verification effort is the cooperative proof strategy that was employed, making use of two separate verification systems. The combined capabilities of both the Gypsy system and the Affirm system were utilized to achieve this result.  2 1. Introduction  The world has yet another verification of the Alternating Bit Protocol. A brief description of this latest addition is presented. The protocol was modeled as an abstract program using the Gypsy verification methodology. A fully mechanical proof of a safety property was obtained. What is perhaps more interesting is that the proof was performed with the combined help of two separate ver..."
10.1.1.38.1107,Sensitivity Profiles from an Array of Coils for Encoding and Reconstruction in Parallel (SPACE RIP),2000,"1506).  *Correspondence to: Walid E. Kyriakos, Department of Radiology, Brigham and Women's Hospital, 221 Longwood Avenue, Boston, MA 02115.  Received 12 July 1999; revised 27 March 2000; accepted 30 March 2000.  Magnetic Resonance in Medicine 44:301--308 (2000)  2000 Wiley-Liss, Inc. 301  real-time rendering possibilities with the use of multiple processors.  METHODS  Encoding Scheme  The concept of parallel imaging is based on using multiple receiver coils, with each providing independent information about the image.  The MR signal received in a coil having W k ( x, y) as its complex 2D sensitivity profile, when neglecting all relaxation phenomena, can be written as:  s k #G y  g  , t# #  ##  r#x, y#W k #x, y#e  j ##Gxxt#G y  g  y ##  dxdy,  [1] where r( x, y) denotes the proton density function, G x represents the readout gradient amplitude applied in the x direction, G y  g  represents the phase encoding gradient applied during the g  th  acquisition, x and y represent the x and y ..."
10.1.1.37.2260,From a Formal Dynamic Semantics of Sisal to a Sisal Environment,1995,"We present a formal definition of the dynamic semantics of a significant part of the language Sisal 2.0  in the structural operational style of Natural Semantics, using Typol inference rules within the Centaur  system, a generic specification environment.  Sisal is a strongly typed, applicative, single assignment language in use on a variety of parallel  processors, including conventional multiprocessors, vector machines and data-flow machines.  The motivations of our work are, with a formal semantic description of Sisal, to provide a firm  foundation for understanding and evaluating language design issues, aid the elimination of ambiguities  in the language, provide a valuable reference for both implementors and programmers, and facilitate  comparison of Sisal with other parallel functional languages. At the same time, Centaur specifications  automatically yield a structure editor and an interpreter for Sisal, which can be developed into an  interactive environment for Sisal programmi..."
10.1.1.36.6331,Boosting for Document Routing,2000,"RankBoost is a recently proposed algorithm for learning ranking functions. It is simple to implement and has strong justifications from computational learning theory. We describe the algorithm and present experimental results on applying it to the document routing problem. The first set of results applies RankBoost to a text representation produced using modern term weighting methods. Performance of RankBoost is somewhat inferior to that of a state-of-the-art routing algorithm which is, however, more complex and less theoretically justified than RankBoost. RankBoost achieves comparable performance to the state-of-the-art algorithm when combined with feature or example selection heuristics. Our second set of results examines the behavior of RankBoost when it has to learn not only a ranking function but also all aspects of term weighting from raw data. Performance is usually, though not always, less good here, but the term weighting functions implicit in the resulting ranking functions a..."
10.1.1.36.5906,A Mobile Manipulator,1996,"This paper describes a mobile manipulator that uses its wheels for manipulation as well as locomotion. This robot, named the mobipulator, looks like a small car with four independently powered wheels, none of them steered. It is designed to manipulate paper and other objects on the surface of a desk. The wheels are used for locomotion or for manipulation, switching functions dynamically as the task demands. So far we have preliminary demonstrations of a variety of motions, and performance data for the task of moving a sheet of paper in a square while maintaining constant orientation.  1 Introduction  This paper describes a mobile manipulator, which we call the mobipulator. It combines the two functions of locomotion and manipulation in a uniform way: it uses wheels for both. So far the robot has demonstrated the ability to manipulate a piece of paper on a desktop, and also shows potential for manipulating other common desktop objects. This paper introduces the robot, describes several ..."
10.1.1.36.5454,"A Mixed Linear and Non-Linear Logic: Proofs, Terms and Models",1994,"Intuitionistic linear logic regains the expressive power of intuitionistic logic through the ! (`of course') modality. Benton, Bierman, Hyland and de Paiva have given a term assignment system for ILL and an associated notion of categorical model in which the ! modality is modelled by a comonad satisfying certain extra conditions. Ordinary intuitionistic logic is then modelled in a cartesian closed category which arises as a full subcategory of the category of coalgebras for the comonad. This paper attempts to explain the connection between ILL and IL more directly and symmetrically by giving a logic, term calculus and categorical model for a system in which the linear and non-linear worlds exist on an equal footing, with operations allowing one to pass in both directions. We start from the categorical model of ILL given by Benton, Bierman, Hyland and de Paiva and show that this is equivalent to having a symmetric monoidal adjunction between a symmetric monoidal closed category and a ca..."
10.1.1.36.4161,A Mobile Manipulator,1999,"This paper describes a mobile manipulator that uses its wheels for manipulation as well as locomotion. This robot, named the mobipulator, looks like a small car with four independently powered wheels, none of them steered. It is designed to manipulate paper and other objects on the surface of a desk. The wheels are used for locomotion or for manipulation, switching functions dynamically as the task demands. So far we have preliminary demonstrations of a variety of motions, and performance data for the task of moving a sheet of paper in a square while maintaining constant orientation.  1 Introduction  This paper describes a mobile manipulator, which we call the mobipulator. It combines the two functions of locomotion and manipulation in a uniform way: it uses wheels for both. So far the robot has demonstrated the ability to manipulate a piece of paper on a desktop, and also shows potential for manipulating other common desktop objects. This paper introduces the robot, describes several ..."
10.1.1.35.8103,Scalable Migration for Mobile Agents,,This paper was accepted and presented at the
10.1.1.35.6788,Scalable Migration for Mobile Agents,1999,"Introduction  One of the most promising approaches to the design of distributed software is the technique of mobile agents [White96]. A mobile agent is an entity, which is capable to migrate around in a distributed system to fulfill its task instructed by the user. The typical migration entity of a mobile agent is an object of the object-based programming paradigm [Booch94] or a whole group of objects migrating together. The usage of mobile agents has not only the advantage of work delegation but has technical reasons as well: one advantage is lower usage of bandwith by moving the computation with the mobile agent to the data rather than the data to the computation. Especially if the agent has to deal with vast volumes of data, as it is typical in the area of information -retrieval, the paradigm of mobile objects seems to be feasible. But the use of mobile agents is still an area of academic research, i.e. typical implementations of mobile agents are small. With "
10.1.1.35.6545,Hoard: A Scalable Memory Allocator for Multithreaded Applications,2000,"Parallel, multithreaded C and C++ programs such as web servers, database managers, news servers, and scientific applications are becoming increasingly prevalent. For these applications, the memory allocator is often a bottleneck that severely limits program performance and scalability on multiprocessor systems. Previous allocators suffer from problems that include poor performance and scalability, and heap organizations that introduce false sharing. Worse, many allocators exhibit a dramatic increase in memory consumption when confronted with a producer-consumer pattern of object allocation and freeing. This increase in memory consumption can range from a factor of P (the number of processors) to unbounded memory consumption. This paper introduces Hoard, a fast, highly scalable allocator that largely avoids false sharing and is memory efficient. Hoard is the first allocator to simultaneously solve the above problems. Hoard combines one global heap and per-processor heaps with a novel d..."
10.1.1.35.5705,The Role of Trust Management in Distributed Systems Security,0,". Existing authorization mechanisms fail to provide powerful  and robust tools for handling security at the scale necessary for today's  Internet. These mechanisms are coming under increasing strain from the  development and deployment of systems that increase the programmability  of the Internet. Moreover, this ""increased flexibility through programmability  "" trend seems to be accelerating with the advent of proposals  such as Active Networking and Mobile Agents.  The trust-management approach to distributed-system security was developed  as an answer to the inadequacy of traditional authorization  mechanisms. Trust-management engines avoid the need to resolve ""identities  "" in an authorization decision. Instead, they express privileges and  restrictions in a programming language. This allows for increased flexibility  and expressibility, as well as standardization of modern, scalable  security mechanisms. Further advantages of the trust-management approach  include proofs that reque..."
10.1.1.35.4594,"Plurisubharmonic Extremal Functions, Lelong Numbers And Coherent Ideal Sheaves",1998,"We introduce a new type of pluricomplex Green function which has a logarithmic pole along a complex subspace A of a complex manifold X. It is the largest negative plurisubharmonic function on X whose Lelong number is at least the Lelong number of log maxfjf 1 j; : : : ; jf m jg, where f 1 ; : : : ; fm are local generators for the ideal sheaf of A. The pluricomplex Green function with a single logarithmic pole or a finite number of weighted poles is a very special case of our construction. We give several equivalent definitions of this function and study its properties, including boundary behaviour, continuity, and uniqueness. This is based on and extends our previous work on disc functionals and their envelopes. "
10.1.1.34.9610,A Fast Table-Driven Method for Edge Thinning and Linking,,"This paper describes fast but effective methods for edge thinning and linking. It has its genesis partly in the idea that it is often preferable to perform simple actions quickly rather than to attempt to do more sophisticated things that take longer. This idea is particularly attractive in the area of real-time computer vision, where the flow of input data may be otherwise overwhelming. We describe computationally cheap table-driven versions of the conventional techniques of edge thinning and edge linking that involve no transcendental functions and only integer arithmetic. Details are given of their computational speed on some representative images.  1. Introduction  Edges are important features for object recognition and for matching for stereo and motion analysis. Edge features can be obtained from raw edge detector responses by a process of edge thinning and edge linking.  Here we present fast table-driven techniques to perform edge thinning and linking.  The basic approach of edg..."
10.1.1.34.665,Accurate SVDs of Structured Matrices,1997,"We present new O(n³) algorithms to compute very accurate SVDs of Cauchy matrices, Vandermonde  matrices, and related ""unit-displacement-rank"" matrices. These algorithms compute  all the singular values with guaranteed relative accuracy, independent of their dynamic range.  In contrast, previous O(n³) algorithms can potentially lose all relative accuracy in the tiniest  singular values. "
10.1.1.34.2039,Towards Usable VR: An Empirical Study of User Interfaces for Immersive Virtual Environments,0,"This paper reports empirical results from a study into the use of 2D widgets in 3D immersive virtual environments. Several researchers have proposed the use of 2D interaction techniques in 3D environments, however little empirical work has been done to test the usability of such approaches. We present the results of two experiments conducted on low-level 2D manipulation tasks within an immersive virtual environment. We empirically show that the addition of passive-haptic feedback for use in precise UI manipulation tasks can significantly increase user performance. Furthermore, users prefer interfaces that provide a physical surface, and that allow them to work with interface widgets in the same visual field of view as the objects they are modifying.  "
10.1.1.33.4973,Model based Bayesian Exploration,1999,Reinforcement learning systems are often concerned  with balancing exploration of untested actions against  exploitation of actions that are known to be good. The  benefit of exploration can be estimated using the classical  notion of Value of Information --- the expected improvement  in future decision quality arising from the  information acquired by exploration. Estimating this  quantity requires an assessment of the agent's uncertainty  about its current value estimates for states.  In this paper we investigate ways to represent and reason  about this uncertainty in algorithms where the system  attempts to learn a model of its environment. We  explicitly represent uncertainty about the parameters of  the model and build probability distributions over Qvalues  based on these. These distributions are used to  compute a myopic approximation to the value of information  for each action and hence to select the action  that best balances exploration and exploitation.  1 Introduction  Rei...
10.1.1.33.164,World-Wide Web Proxies,1994,"A WWW proxy server, proxy for short, provides access to the Web for people on closed subnets who can only access the Internet through a firewall machine. The hypertext server developed at CERN, cern_httpd, is capable of running as a proxy, providing seamless external access to HTTP, Gopher, WAIS and FTP. cern_httpd has had gateway features for a long time, but only this spring they were extended to support all the methods in the HTTP protocol used by WWW clients. Clients don't lose any functionality by going through a proxy, except special processing they may have done for nonnative Web protocols such as Gopher and FTP. A brand new feature is caching performed by the proxy, resulting in shorter response times after the first document fetch. This makes proxies useful even to the people who do have full Internet access and don't really need the proxy just to get out of their local subnet. This paper gives an overview of proxies and reports their current status.  1.0 Introduction  The pri..."
10.1.1.32.8484,Fully Dynamic Transitive Closure: Breaking Through the O(n^2) Barrier,2000,"In this paper we introduce a general framework for casting fully dynamic transitive closure into the problem of reevaluating polynomials over matrices. With this technique, we improve the best known bounds for fully dynamic transitive closure. In particular, we devise a deterministic algorithm for general directed graphs that achieves O(n  2  ) amortized time for updates, while preserving unit worstcase cost for queries. In case of deletions only, our algorithm performs updates faster in O(n) amortized time.  Our matrix-based approach yields an algorithm for directed acyclic graphs which breaks through the O(n  2  ) barrier on the single-operation complexity of fully dynamic transitive closure. We can answer queries in O(n  ffl  ) time and perform updates in O(n  !(1;ffl;1)\Gammaffl  +n  1+ffl  ) time, for any ffl 2 [0; 1], where !(1; ffl; 1) is the exponent of the multiplication of an n\Thetan  ffl  matrix by an n  ffl  \Thetan matrix. The current best bounds on !(1; ffl; 1) imply an ..."
10.1.1.32.7861,Knowledge Management: Are We Missing Something?,1999,"As commercial organisations face up to modern pressures to downsize and outsource they have lost knowledge as people leave and take with them what they know. This knowledge is increasingly being recognised as an important resource and organisations are now taking steps to manage it. In addition, as the pressures for globalisation increase, collaboration and co-operation are becoming more distributed and international. Knowledge sharing in a distributed international environment is becoming an essential part of Knowledge Management (KM).  In this paper we make a distinction between hard and soft knowledge within an organisation and argue that much of what is called KM deals with hard knowledge and emphasises capture-codify-store. This is a major weakness of the current approach to KM. This paper addresses this weakness by exploring the sharing of `soft' knowledge using the concept of communities of practice.  1. INTRODUCTION: MANAGING KNOWLEDGE -- THE CHALLENGE  Three major issues facin..."
10.1.1.32.7028,Status of this Memo,,"This memorandum describes the real-time transport protocol, RTP. RTP provides end-toend  network transport functions suitable for applications transmitting real-time data, such as  audio, video or simulation data over multicast or unicast network services. RTP does not address  resource reservation and does not guarantee quality-of-service for real-time services. The data  transport is augmented by a control protocol (RTCP) designed to provide minimal control and  identification functionality, particularly in multicast networks. RTP and RTCP are designed to  be independent of the underlying transport and network layers. The protocol supports the use  of RTP-level translators and bridges.  This specification is a product of the Audio/Video Transport working group within the Internet Engineering Task Force. Comments are solicited and should be addressed to the working group's mailing list at rem-conf@es.net and/or the authors.  The protocol is under development and changes to aspects of ..."
10.1.1.32.5901,Application Of ARC In System Design,2000,"Current and future mobile communication systems require the combination of high  performance technology from very diverse engineering disciplines. Consequently, these  systems are complex by nature, which complicates system-wide Quality of Service  optimisation. In this paper we consider system-wide optimisation of resource distribution  within the framework of a given (static) architecture but with components that  do support multiple modes of operation. We take into account variable type of applications,  quality requirements, resource availability, and external context. In this paper  we use the concepts of Adaptive resource contracts (ARC) to capture and exploit the  capabilities of individual components in the system. A relevant case study demonstrates  the added value of the ARC concepts for system design. Eventually the method yields a  specification of individual system components and an indication of the optimised overall  QoS.  1 INTRODUCTION  Recently, a number of demanding ..."
10.1.1.32.4894,Reducing Disjunctive to Non-Disjunctive Semantics by Shift-Operations,1996," It is wellknown that Minker's semantics GCWA for positive disjunctive  programs P is \Pi  P  2 -complete , i.e. to decide if a literal is true in all minimal models of  P . This is in contrast to the same entailment problem for semantics of non-disjunctive  programs such as STABLE and SUPPORTED (both are co-NP-complete) as well as  M  supp  P and WFS (that are even polynomial).  Recently, the idea of reducing disjunctive to non-disjunctive programs by using so  called shift-operations was introduced independently by Bonatti, Dix/Gottlob/Marek ,  and Schaerf . In fact, Schaerf associated to each semantics SEM for normal programs a  corresponding semantics Weak-SEM for disjunctive programs and asked for the properties  of these weak semantics, in particular for the complexity of their entailment  relations. While Schaerf concentrated on Weak-STABLE and Weak-SUPPORTED, we  investigate the weak versions of Apt/Blair/Walker's stratified semantics M  supp  P and of  vanGelder/Ross/Schlipf'..."
10.1.1.31.7685,Lazy Rewriting on Eager Machinery,1995,"We define Lazy Term Rewriting Systems and show that they can be realized by local  adaptations of an eager implementation of conventional term rewriting systems. The overhead  of lazy evaluation is only incurred when lazy evaluation is actually performed.  Our method is modelled by a transformation of term rewriting systems, which concisely expresses  the intricate interaction between pattern matching and lazy evaluation. The method easily  extends to term graph rewriting.  CR Subject Classification (1991): D.3.4 [Programming languages]: Processors  -- Compilers, Optimization; D.1.1 [Programming Techniques]: Applicative (Functional)  Programming; D.1.6: Logic Programming.  AMS Subject Classification (1991): 68N20: Compilers and generators; 68Q05: Models  of Computation; 68Q42: Rewriting Systems  Keywords & Phrases: lazy term rewriting, program transformation.  Note: Partial support received from the European Communities under the ESPRIT project 5399  (Compiler Generation for Parallel M..."
10.1.1.31.735,Scheduling Dynamic Graphs,1999,"In parallel and distributed computing scheduling low level tasks on the available hardware is a fundamental  problem. Traditionally, one has assumed that the set of tasks to be executed is known beforehand. Then the  scheduling constraints are given by a precedence graph. Nodes represent the elementary tasks and edges the  dependencies among tasks. This static approach is not appropriate in situations where the set of tasks is not  known exactly in advance, for example, when different options how to continue a program may be granted.  In this paper a new model for parallel and distributed programs, the dynamic process graph, will be introduced,  which represents all possible executions of a program in a compact way. The size of this representation is small --  in many cases only logarithmically with respect to the size of any execution. An important feature of our model  is that the encoded executions are directed acyclic graphs having a ""regular"" structure that is typical of parallel ..."
10.1.1.31.6660,Program Slicing,1992,"The concept of static program slicing was first introduced by Weiser. Ottenstein et al. indicated that an intraprocedural slice can be found in linear time by traversing a suitable graph representation of the program referred to as the program dependence graph (PDG). Horwitz et al. introduced algorithms to construct interprocedural slices by extending the program dependence graph to a supergraph of the PDG referred to as the system dependence graph (SDG). This extension captures the calling context of procedures.  In a previous paper, we demonstrated that a parse-tree-based SDG provides us with ""smaller"" and therefore more precise slices than a statement-based SDG. Furthermore, we described extensions to the SDG in order to handle particular constructs in a language that is a subset of ANSI C. In this paper, we will describe a new method for the calculation of transitive dependences and therefore build a SDG that does require neither the calculation of the GMOD and GREF sets nor the co..."
10.1.1.31.5488,Intelligent Built-in Torque Sensor for Harmonic Drive Systems,1997,"A harmonic drive is a compact, light--weight and high--ratio torque transmission device which is used in many electrically actuated robot manipulators. In this paper a built--in torque sensor for harmonic drive systems is examined in detail. The method proposed by Hashimoto, in which strain--gauges are directly mounted on the flexspline, is employed and improved in this paper. To minimize sensing inaccuracy, four Rosette strain gauges are used employing an accurate positioning method. To cancel the torque ripples, the oscillation observed on the measured torque and caused mainly by gear teeth meshing, Kalman filter estimation is used. A simple forth order harmonic oscillator proved to accurately model the torque ripples. Moreover, the error model is extended to incorporate any misalignment torque. By on line implementation of the Kalman filter, it has been shown that this method is a fast and accurate way to filter torque ripples and misalignment torque. Hence, the intelligent built--i..."
10.1.1.31.4024,Multi-Value-Functions: Efficient Automatic Action Hierarchies for Multiple Goal MDPs,1999,"If you have planned to achieve one particular goal in a stochastic delayed rewards problem and then someone asks about a different goal what should you do? What if you need to be ready to quickly supply an answer for any possible goal? This paper shows that by using a new kind of automatically generated abstract action hierarchy that with  N states, preparing for all of N possible goals can be much much cheaper than N times the work of preparing for one goal. In goal-based Markov Decision Problems, it is usual to generate a policy ß(x),  mapping states to actions, and a value function  J(x), mapping states to an estimate of minimum expected cost-to-goal, starting at x. In this paper we will use the terminology that a multi-policy  ß  ?  (x; y) (for all state-pairs (x; y)) maps a state x  to the first action it should take in order to reach y  with expected minimum cost and a multi-valuefunction  J  ?  (x; y) is a definition of this minimum cost. Building these objects quickly and with ..."
10.1.1.30.9381,Predictive Models for the Breeder Genetic Algorithm -- I. Continuous Parameter Optimization,1993,In this paper a new genetic algorithm called the Breeder Genetic Algorithm  (BGA) is introduced. The BGA is based on artificial selection  similar to that used by human breeders. A predictive model for the BGA is  presented which is derived from quantitative genetics. The model is used to  predict the behavior of the BGA for simple test functions. Different mutation  schemes are compared by computing the expected progress to the solution.  The numerical performance of the BGA is demonstrated on a test suite of  multimodal functions. The number of function evaluations needed to locate  the optimum scales only as n ln(n) where n is the number of parameters.  Results up to n = 1000 are reported. 
10.1.1.30.5913,A Temporal Data Model for Multimedia Database Systems,1997,": The presented data model is a novel approach for integrating temporal concepts into a multimedia database system. Multimedia objects are extended with the traditional time dimensions valid time and transaction time. In addition a new time dimension specifically tailored for multimedia data types is presented with semantics that are completely orthogonal to the already established time dimensions, valid time and transaction time, i.e., the model supports a 3D time for multimedia data. This new time dimension, the play time dimension, places the building blocks of multimedia data in a temporal structure for multimedia presentation. This model is currently being implemented in a MMDBS for distance education at UNIK, University of Oslo.  1. Introduction  The most common time dimensions used in temporal database systems are valid time and transaction time. The valid time of a fact is the time when the fact was, is or will be true in the modeled reality and the transaction time of a fact i..."
10.1.1.30.2363,Querying as an Enabling Technology in Software Reengineering,1999,"In this paper it is argued that different kinds of reengineering technologies can be based on querying. Several reengineering technologies are presented as being integrated into a technically oriented reengineering taxonomy. The usefulness of querying is pointed out with respect to these reengineering technologies.  To impose querying as a base technology in reengineering examples are given with respect to the EER/GRAL approach to conceptual modeling and implementation. This approach is presented together with GReQL as its query part. The different reengineering technologies are finally reviewed in the context of the GReQL query facility.  1 Introduction  Reengineering may be viewed as any activity that either improves the understanding of a software or else improves the software itself [2].  According to this view software reengineering can be ""partitioned"" into two kinds of activities. The first kind of activities is concerned with understanding such as source code retrieval, browsin..."
10.1.1.3.832,Generic Sentence Fusion is an Ill-Defined Summarization Task,,"We report on a series of human evaluations of the task of sentence fusion. In this task, a human is given two sentences and asked to produce a single coherent sentence that contains only the important information from the original two. Thus, this is a highly constrained summarization task. Our investigations show that even at this restricted level, there is no measurable agreement between humans regarding what information should be considered important. We further investigate the ability of separate evaluators to assess summaries, and find similarly disturbing lack of agreement."
10.1.1.3.5198,R-D Analysis of Adaptive Edge Representations,2002,"This paper presents a Rate-Distortion analysis for a simple horizon edge image model. A quadtree with anisotropy and rotation is performed on this kind of image, giving a toy model for a non-linear adaptive coding technique, and its Rate-Distortion behavior is studied. The effect of refining the quadtree decomposition is also analyzed."
10.1.1.3.4808,A generalized Rate-Distortion limit for edge representation,2002,"This paper presents a rate-distortion analysis for a simple horizon edge image model. A quadtree with anisotropy and rotation is performed in this kind of image, giving a toy model for a non-linear adaptive coding technique, and its rate-distortion behavior is studied. The e#ect of refining the quadtree decomposition in the Rate-Distortion decay is also studied."
10.1.1.3.2533,Lead Field Basis for FEM Source Localization ,1999,"In recent years, significant progress has been made in the area of EEG/MEG source imaging. Source imaging on simple spherical models has become increasingly efficient, with consistently reported accuracy of within 5mm. In contrast, source localization on realistic head models remains slow, with sub-centimeter accuracy being the exception rather than the norm. A primary reason for this discrepancy is that most source imaging techniques are based on lead-fields. While the lead-field for simplified geometries can be easily computed analytically, an efficient method for computing realistic domain lead-fields has, until now, remained elusive. In this paper, we propose two efficient methods for computing realistic EEG lead-field bases: the first is element-oriented, and the second is node-oriented. We compare these two bases, discuss how they can be used to apply recent source imaging methods to realistic models, and report timings for constructing the bases."
10.1.1.29.99,Propagation Rule Compiler: Technical Documentation,1995,The Propagation Rule Compiler (PROP) is a tool developed within the ESPRIT III project IDEA (Intelligent Database Environment for Advanced Applications) . It aims at supporting developers of Chimera applications during schema design and prototyping. PROP consists of two components: the rule compiler as such and an explanation facility. The task of the explanation facility is to graphically illustrate the logical dependencies established via deductive rules of a given Chimera schema. The purpose of the compilation unit is to generate update propagation triggers from deductive rules. Such triggers are able to automatically compute all implicit changes of a Chimera database once a specific updating transaction has been issued. PROP has been integrated as a subcomponent into Bonn's Chimera Prototyping Tool (CPT). The purpose of this document is to provide the technical documentation of the rule compilation process.   This report has been issued to the ESPRIT III project 6333 (IDEA) as deli...
10.1.1.29.9108,Recurrences and Legendre Transform,1992,"A binomial identity ((1) below), which relates the famous Ap'ery numbers and the sums of cubes of binomial coefficients (for which Franel has established a recurrence relation almost 100 years ago), can be seen as a particular instance of a Legendre transform between sequences. A proof of this identity can be based on the more general fact that the Ap'ery and Franel recurrence relations themselves are conjugate via Legendre transform. This motivates a closer look at conjugacy of sequences satisfying linear recurrence relations with polynomial coefficients. The role of computer-aided proof and verification in the study of binomial identities and recurrence relations is illustrated, and potential applications of conjugacy in diophantine approximation are mentioned. This article is an expanded version of a talk given at the 29. meeting of the  S'eminaire Lothringien de Combinatoire, Thurnau, september 1992. "
10.1.1.28.999,Learning Search Engine Specific Query Transformations for Question Answering,2001,"We introduce a method for learning query transformations that improves the ability to retrieve answers to questions from an information retrieval system. During the training stage the method involves automatically learning phrase features for classifying questions into different types, automatically generating candidate query transformations from a training set of question/answer pairs, and automatically evaluating the candidate transforms on target information retrieval systems such as real-world general purpose search engines. At run time, questions are transformed into a set of queries, and re-ranking is performed on the documents retrieved. We present a prototype search engine, Tritus, that applies the method to web search engines. Blind evaluation on a set of real queries from a web search engine log shows that the method significantly outperforms the underlying web search engines as well as a commercial search engine specializing in question answering. Keywords Web search, quer..."
10.1.1.28.9511,"A Secure, Publisher-Centric Web Caching Infrastructure",2001,"The current web caching infrastructure, though it has a number of performance benefits for clients and network providers, does not meet publishers' requirements. We argue that to satisfy these requirements, caches should be enhanced in both the data and control planes. In the data plane, caches will dynamically generate content for clients by running code provided by publishers. In the control plane, caches will return logs of client accesses to publishers. In this paper, we introduce Gemini, a system which has both of these capabilities, and discuss two of its key components: security and incremental deployment. Since Gemini caches are deeply involved in content preparation and logging, ensuring that they perform correctly is vital. Traditional end-to-end security mechanisms are not sufficient to protect clients and publishers, so we introduce a new security model which consists of two pieces: an authorization mechanism and a verification mechanism. The former allows a publisher to authorize a set of caches to run its code and serve its content, while the latter allows clients and publishers to probabilistically verify that authorized caches are operating correctly. Because it is unrealistic to assume that Gemini caches will be deployed everywhere simultaneously, we have designed the system to be incrementally deployable and to coexist with legacy clients, caches, and servers. Finally, we describe our implementation of Gemini and present preliminary performance results.   "
10.1.1.28.8872,Multilevel Algorithms for Generating Coarse Grids for Multigrid Methods,2001,"Geometric Multigrid methods have gained  widespread acceptance for solving large systems  of linear equations, especially for structured grids.  One of the challenges in successfully extending  these methods to unstructured grids is the problem  of generating an appropriate set of coarse grids. The  focus of this paper is the development of robust algorithms,  both serial and parallel, for generating a  sequence of coarse grids from the original unstructured  grid. Our algorithms treat the problem of  coarse grid construction as an optimization problem  that tries to optimize the overall quality of the resulting  fused elements. We solve this problem using the  multilevel paradigm that has been very successful in  solving the related grid/graph partitioning problem.  The parallel formulation of our algorithm incurs a  very small communication overhead, achieves high  degree of concurrency, and maintains the high quality  of the coarse grids obtained by the serial algorithm.  # This work was supported by NSF CCR-9972519, EIA-9986042, ACI-9982274, by Army Research Office contract DA/DAAG55-98-10441, by the DOE ASCI program, and by Army High Performance Computing Research Center contract number DAAH04-95-C-0008.  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies  are not made or distributed for profit or commercial advantage, and that  copies bear this notice and the full citation on the first page. To copy  otherwise, to republish, to post on servers or to redistribute to lists, requires  prior specific permission and/or a fee. SC2001 November 2001,  Denver (c) 2001 ACM 1-58113-293-X/01/0011 $5.00  1 "
10.1.1.28.7960,Learning Search Engine Specific Query Transformations for Question Answering,2001,"We introduce a method for learning query transformations that improves the ability to retrieve answers to questions from an information retrieval system. During the training stage the method involves automatically learning phrase features for classifying questions into different types, automatically generating candidate query transformations from a training set of question/answer pairs, and automatically evaluating the candidate transforms on target information retrieval systems such as real-world general purpose search engines. At run time, questions are transformed into a set of queries, and re-ranking is performed on the documents retrieved. We present a prototype search engine, Tritus, that applies the method to web search engines. Blind evaluation on a set of real queries from a web search engine log shows that the method significantly outperforms the underlying web search engines as well as a commercial search engine specializing in question answering."
10.1.1.28.5387,"Topologies, Migration Rates, and Multi-Population Parallel Genetic Algorithms",1999,"This paper presents a study of parallel genetic algorithms (GAs) with multiple populations (also called demes or islands). The study makes explicit the relation between the probability of reaching a desired solution with the deme size, the migration rate, and the degree of the connectivity graph. The paper considers arbitrary topologies with a fixed number of neighbors per deme. The demes evolve in isolation until each converges to a unique solution. Then, the demes exchange an arbitrary number of individuals and restart their execution. An accurate deme-sizing equation is derived, and it is used to determine the optimal configuration of an arbitrary number of demes that minimizes the execution time of the parallel GA. 1 INTRODUCTION Parallel genetic algorithms (GAs) with multiple populations are difficult to configure because they are controlled by many parameters that affect their efficiency and accuracy. Among other things, one must decide the number and the size of the populations..."
10.1.1.28.5031,Grid Information Services for Distributed Resource Sharing,2001,"Grid technologies enable large-scale sharing of resources within formal or informal consortia of individuals and/or institutions: what are sometimes called virtual organizations. In these settings, the discovery, characterization, and monitoring of resources, services, and computations can be challenging due to the considerable diversity, large numbers, dynamic behavior, and geographical distribution of the entities in which a user might be interested. Hence, information services are a vital part of any Grid software infrastructure, providing fundamental mechanisms for discovery and monitoring, and thus for planning and adapting application behavior. We present here an information services architecture that addresses performance, security, scalability, and robustness requirements. Our architecture defines low-level enquiry and registration protocols that make it easy to incorporate individual entities into information structures, including aggregate directories  supporting different query languages and discovery strategies. These protocols can also be combined with other Grid protocols to construct additional higher-level services and capabilities such as brokering, monitoring, fault detection, and troubleshooting. Our architecture has been implemented as MDS-2, which forms part of the Globus Grid toolkit and has been widely deployed and applied."
10.1.1.26.5588,The Internet Worm Incident  ,1991, 
10.1.1.26.4934,A Report on Some Recent Developments in TCP Congestion Control,2000,"This paper discusses several changes to TCP's congestion control, either proposed or in progress. The changes to TCP include a Limited Transmit mechanism for transmitting new packets on the receipt of one or two duplicate acknowledgements, and a SACK-based mechanism for detecting and responding to unnecessary Fast Retransmits or Retransmit Timeouts.  These changes to TCP are designed to avoid unnecessary Retransmit Timeouts, to correct unnecessary Fast Retransmits or Retransmit Timeouts resulting from reordered or delayed packets, and to assist the development of viable mechanisms for Corruption Notification. The changes in the network include Explicit Congestion Notification (ECN), which builds upon the addition of Active Queue Management. 1 Introduction  The basis of TCP congestion control lies in Additive Increase Multiplicative Decrease (AIMD), halving the congestion window for every window containing a packet loss, and increasing the congestion window by roughly one segment per RT..."
10.1.1.26.4045,A Component- and Message-Based Architectural Style for GUI Software,1996,"While a large fraction of application code is devoted to graphical user interface (GUI) functions, support for reuse in this domain has largely been confined to the creation of GUI toolkits (""widgets""). We present a novel architectural style directed at supporting larger grain reuse and flexible system composition. Moreover, the style supports design of distributed, concurrent applications. Asynchronous notification messages and asynchronous request messages are the sole basis for inter-component communication. A key aspect of the style is that components are not built with any dependencies on what typically would be considered lower-level components, such as user interface toolkits. Indeed, all components are oblivious to the existence of any components to which notification messages are sent. While our focus has been on applications involving graphical user interfaces, the style has the potential for broader applicability. Several trial applications using the style are described.  "
10.1.1.26.3390,Automatic Synthesis of Control Programs in Polynomial Time for an Assembly Line,,"The industry wants provably correct and fast formal methods for handling combinatorial dynamical systems. One example of such problems is error recovery in industrial processes. We have used a provably correct, polynomial-time planning algorithm to plan for a miniature assembly line, which assembles toy cars. Although somewhat limited, this process has many similarities with real industrial processes. By exploring the structure of this assembly line we have extended a previously presented algorithm, thus extending the class of problems that can be handled in polynomial time. The planning tool presented here contains general-purpose algorithms that generate plans in the form of GRAFCET charts that are automatically translated into PLC code using a commercial PLC compiler.  "
10.1.1.25.6221,Confidence Measure Based Language Identification,2000,"In this paper we present a new application for confidence measures in spoken language processing. In today's computerized dialogue systems, language identification (LID) is typically achieved via dedicated modules. In our approach, LID is integrated into the speech recognizer, therefore profiting from high-level linguistic knowledge at very little extra cost. Our new approach is based on a word lattice based confidence measure [3], which was originally devised for unsupervised training. In this work, we show that the confidence based language identification algorithm outperforms conventional score based methods. Also, this method is less dependent on the acoustic characteristics of the transmission channel than score based methods. By introducing additional parameters, unknown languages can be rejected. The proposed method is compared to a score based approach on the Verbmobil database, a three language task. "
10.1.1.25.6054,DyC: An Expressive Annotation-Directed Dynamic Compiler for C,1997,"We present the design of DyC, a dynamic-compilation system for C based on run-time specialization. Directed by a few declarative user annotations that specify the variables and code on which dynamic compilation should take place, a binding-time analysis computes the set of run-time constants at each program point in the annotated procedure's control-flow graph; the analysis supports programpoint -specific polyvariant division and specialization. The results of the analysis guide the construction of a run-time specializer for each dynamically compiled region; the specializer supports various caching strategies for managing dynamically generated code and mixes of speculative and demand-driven specialization of dynamic branch successors. Most of the key cost/benefit trade-offs in the binding-time analysis and the run-time specializer are open to user control through declarative policy annotations. DyC has been implemented in the context of an optimizing compiler, and initial results have ..."
10.1.1.25.5401,Properties of Terms in Continuation-Passing Style in an Ordered Logical Framework,2000,Machine  We now begin extending our representation to include evaluation of CPS terms. We will begin by showing a representation of a naive evaluator which makes no use of the ordering invariants. The following is a bare abstract machine for CPS evaluation.
10.1.1.24.6252,Smart Videoconferencing,2000,"The combination of acoustical and video processing to achieve a smart audio and video feed from a set of N microphones and M cameras is a task that might conventionally be accomplished by camerapersons and control room staff. However, in the context of videoconferencing this process needs to be performed by control software. We discuss the use of a multi-camera multi-microphone set up for unattended videoconferencing, and present details of a prototype implementation being developed."
10.1.1.24.5112,A Report on Some Recent Developments in TCP Congestion Control,2000,"This paper discusses several changes to TCP's congestion control, either proposed or in progress. The changes to TCP include a Limited Transmit mechanism for transmitting new packets on the receipt of one or two duplicate acknowledgements, and a SACK-based mechanism for detecting and responding to unnecessary Fast Retransmits or Retransmit Timeouts."
10.1.1.233.4878,Not for Quotation TEACHER PERFORMANCE INCENTIVES AND STUDENT OUTCOMES,2000,The paper was prepared for a National Academy of Sciences Conference entitled “Devising Incentives
10.1.1.232.463,Summarization evaluation using transformed basic elements,2008,"This paper describes BEwTE (Basic Elements with Transformations for Evaluation), an automatic system for summarization evaluation. BEwTE is a new, more sophisticated implementation of the BE framework that uses transformations to match BEs (minimallength syntactically wellformed units) that are lexically different yet semantically similar. We demonstrate the effectiveness of BEwTE using DUC and TAC datasets. 1"
10.1.1.231.8651,The bag-of-opinions method for review rating prediction from sparse text patterns,2010,"The problem addressed in this paper is to predict a user’s numeric rating in a product review from the text of the review. Unigram and n-gram representations of text are common choices in opinion mining. However, unigrams cannot capture important expressions like “could have been better”, which are essential for prediction models of ratings. N-grams of words, on the other hand, capture such phrases, but typically occur too sparsely in the training set and thus fail to yield robust predictors. This paper overcomes the limitations of these two models, by introducing a novel kind of bag-of-opinions representation, where an opinion, within a review, consists of three components: a root word, a set of modifier words from the same sentence, and one or more negation words. Each opinion is assigned a numeric score which is learned, by ridge regression, from a large, domain-independent corpus of reviews. For the actual test case of a domain-dependent review, the review’s rating is predicted by aggregating the scores of all opinions in the review and combining it with a domaindependent unigram model. The paper presents a constrained ridge regression algorithm for learning opinion scores. Experiments show that the bag-of-opinions method outperforms prior state-of-the-art techniques for review rating prediction."
10.1.1.231.2352,"Section: Data Cube & OLAP 1 Models, Issues, and Techniques in OLAP",,"The problem of efficiently visualizing multidimensional data sets produced by scientific and statistical tasks/ processes is becoming increasingly challenging, and is attracting the attention of a wide multidisciplinary"
10.1.1.23.9777, Managing Surrogate Objectives to Optimize a Helicopter Rotor Design -- Further Experiments ,1998,"It is common engineering practice to use response surface approximations as surrogates for an expensive objective function in engineering design. The rationale is to reduce the number of detailed, costly analyses required during optimization. In earlier work, we developed a rigorous and effective scheme for managing the interplay between the use of surrogates in the optimization and scheduled progress checks with the expensive analysis so that the process converges to a solution of the original design problem. In this paper, we will report our latest numerical tests with a helicopter rotor design problem which has proved to be a fruitful laboratory for experimentation. The results given here support the use of an ANOVA decomposition on a DACE model to identify the most important optimization variables in an optimal design problem."
10.1.1.23.8906,A Unified Approach to Spatial Outliers Detection,2003,"Spatial outliers represent locations which are significantly different from their neighborhoods even though they may not be significantly different from the entire population. Identification of spatial outliers can lead to the discovery of unexpected, interesting, and implicit knowledge, such as local instability. In this paper, we first provide a general definition of S-outliers for spatial outliers. This definition subsumes the traditional definitions of spatial outliers. Second, we characterize the computation structure of spatial outlier detection methods and present scalable algorithms. Third, we provide a cost model of the proposed algorithms."
10.1.1.23.8510,Computing Partial Data Cubes for Parallel Data Warehousing Applications,2001,"In this paper, we focus on an approach to On-Line Analytical  Processing (OLAP) that is based on a database operator and data  structure called the datacube. The datacube is a relational operator that  is used to construct all possible views of a given data set. E#cient algorithms  for computing the entire datacube --- both sequentially and in  parallel --- have recently been proposed. However, due to space and time  constraints, the assumption that all 2  d  (where d = dimensions) views  should be computed is often not valid in practice. As a result, algorithms  for computing partial datacubes are required. In this paper, we  describe a parallel algorithm for computing partial datacubes and provide  preliminary experimental results based on an implementation in C  and MPI."
10.1.1.23.7893,Eraser: A Dynamic Data Race Detector for Multi-Threaded Programs,1997,"Multi-threaded programming is difficult and error prone. It is easy to make a mistake in synchronization that produces a data race, yet it can be extremely hard to locate this mistake during debugging. This paper describes a new tool, called Eraser, for dynamically detecting data races in lock-based multi-threaded programs. Eraser uses binary rewriting techniques to monitor every shared memory reference and verify that consistent locking behavior is observed. We present several case studies, including undergraduate coursework and a multi-threaded Web search engine, that demonstrate the effectiveness of this approach."
10.1.1.23.7215,PostgreSQL 7.1 Programmer's Guide,1998,Accessible reference for SQL features.
10.1.1.23.697,World-Wide Web Proxies,1994,"A WWW proxy server, proxy for short, provides access to the Web for people on closed subnets who can only access the Internet through a firewall machine. The hypertext server developed at CERN, cern_httpd, is capable of running as a proxy, providing seamless external access to HTTP, Gopher, WAIS and FTP. cern_httpd has had gateway features for a long time, but only this spring they were extended to support all the methods in the HTTP protocol used by WWW clients. Clients don't lose any functionality by going through a proxy, except special processing they may have done for nonnative Web protocols such as Gopher and FTP. A brand new feature is caching performed by the proxy, resulting in shorter response times after the first document fetch. This makes proxies useful even to the people who do have full Internet access and don't really need the proxy just to get out of their local subnet. This paper gives an overview of proxies and reports their current status.  1.0 "
10.1.1.229.2982,Distribution Fields for Tracking,,"Visual tracking of general objects often relies on the assumption that gradient descent of the alignment function will reach the global optimum. A common technique to smooth the objective function is to blur the image. However, blurring the image destroys image information, which can cause the target to be lost. To address this problem we introduce a method for building an image descriptor using distribution fields (DFs), a representation that allows smoothing the objective function without destroying information about pixel values. We present experimental evidence on the superiority of the width of the basin of attraction around the global optimum of DFs over other descriptors. DFs also allow the representation of uncertainty about the tracked object. This helps in disregarding outliers during tracking (like occlusions or small misalignments) without modeling them explicitly. Finally, this provides a convenient way to aggregate the observations of the object through time and maintain an updated model. We present a simple tracking algorithm that uses DFs and obtains state-of-the-art results on standard benchmarks. 1."
10.1.1.229.1053,The bag-of-opinions method for review rating prediction from sparse text patterns,2010,"The problem addressed in this paper is to predict a user’s numeric rating in a product review from the text of the review. Unigram and n-gram representations of text are common choices in opinion mining. However, unigrams cannot capture important expressions like “could have been better”, which are essential for prediction models of ratings. N-grams of words, on the other hand, capture such phrases, but typically occur too sparsely in the training set and thus fail to yield robust predictors. This paper overcomes the limitations of these two models, by introducing a novel kind of bag-of-opinions representation, where an opinion, within a review, consists of three components: a root word, a set of modifier words from the same sentence, and one or more negation words. Each opinion is assigned a numeric score which is learned, by ridge regression, from a large, domain-independent corpus of reviews. For the actual test case of a domain-dependent review, the review’s rating is predicted by aggregating the scores of all opinions in the review and combining it with a domaindependent unigram model. The paper presents a constrained ridge regression algorithm for learning opinion scores. Experiments show that the bag-of-opinions method outperforms prior state-of-the-art techniques for review rating prediction."
10.1.1.228.8436,H.: Geodesic distance-weighted shape vector image diffusion,2008,"Abstract—This paper presents a novel and efficient surface matching and visualization framework through the geodesic distanceweighted shape vector image diffusion. Based on conformal geometry, our approach can uniquely map a 3D surface to a canonical rectangular domain and encode the shape characteristics (e.g., mean curvatures and conformal factors) of the surface in the 2D domain to construct a geodesic distance-weighted shape vector image, where the distances between sampling pixels are not uniform but the actual geodesic distances on the manifold. Through the novel geodesic distance-weighted shape vector image diffusion presented in this paper, we can create a multiscale diffusion space, in which the cross-scale extrema can be detected as the robust geometric features for the matching and registration of surfaces. Therefore, statistical analysis and visualization of surface properties across subjects become readily available. The experiments on scanned surface models show that our method is very robust for feature extraction and surface matching even under noise and resolution change. We have also applied the framework on the real 3D human neocortical surfaces, and demonstrated the excellent performance of our approach in statistical analysis and integrated visualization of the multimodality volumetric data over the shape vector image. Index Terms—Surface Matching, Shape Vector Image, Multiscale Diffusion, Visualization. 1"
10.1.1.228.2186,Distribution Fields for Tracking ,,"Visual tracking of general objects often relies on the assumption that gradient descent of the alignment function will reach the global optimum. A common technique to smooth the objective function is to blur the image. However, blurring the image destroys image information, which can cause the target to be lost. To address this problem we introduce a method for building an image descriptor using distribution fields (DFs), a representation that allows smoothing the objective function without destroying information about pixel values. We present experimental evidence on the superiority of the width of the basin of attraction around the global optimum of DFs over other descriptors. DFs also allow the representation of uncertainty about the tracked object. This helps in disregarding outliers during tracking (like occlusions or small misalignments) without modeling them explicitly. Finally, this provides a convenient way to aggregate the observations of the object through time and maintain an updated model. We present a simple tracking algorithm that uses DFs and obtains state-of-the-art results on standard benchmarks.  "
10.1.1.227.8515,Methods,2012,AutoLockMass- This function decides where the lock mass scans are in the xcmsRaw object. This is done by using the scan time differences. Arguments object An xcmsRaw-class object
10.1.1.227.7452,PESA: Phrase pair extraction as sentence splitting,2005,"Most statistical machine translation systems use phrase-to-phrase translations to capture local context information, leading to better lexical choice and more reliable local reordering. The quality of the phrase alignment is crucial to the quality of the resulting translations. Here, we propose a new phrase alignment method, not based on the Viterbi path of word alignment models. Phrase alignment is viewed as a sentence splitting task. For a given spitting of the source sentence (source phrase, left segment, right segment) find a splitting for the target sentence, which optimizes the overall sentence alignment probability. Experiments on different translation tasks show that this phrase alignment method leads to highly competitive translation results. 1"
10.1.1.227.6950,FaceTracer: A search engine for large collections of images with faces. ECCV,2008,"Abstract. We have created the first image search engine based entirely on faces. Using simple text queries such as “smiling men with blond hair and mustaches, ” users can search through over 3.1 million faces which have been automatically labeled on the basis of several facial attributes. Faces in our database have been extracted and aligned from images downloaded from the internet using a commercial face detector, and the number of images and attributes continues to grow daily. Our classification approach uses a novel combination of Support Vector Machines and Adaboost which exploits the strong structure of faces to select and train on the optimal set of features for each attribute. We show state-of-the-art classification results compared to previous works, and demonstrate the power of our architecture through a functional, large-scale face search engine. Our framework is fully automatic, easy to scale, and computes all labels off-line, leading to fast on-line search performance. In addition, we describe how our system can be used for a number of applications, including law enforcement, social networks, and personal photo management. Our search engine will soon be made publicly available. 1"
10.1.1.227.1851,“access/refractory ” and “degraded-store”,,semantic impairments
10.1.1.226.8283,Cross-Cutting Models of Lexical Semantics,,"Context-dependent word similarity can be measured over multiple cross-cutting dimensions. For example, lung and breath are similar thematically, while authoritative and superficial occur in similar syntactic contexts, but share little semantic similarity. Both of these notions of similarity play a role in determining word meaning, and hence lexical semantic models must take them both into account. Towards this end, we develop a novel model, Multi-View Mixture (MVM), that represents words as multiple overlapping clusterings. MVM finds multiple data partitions based on different subsets of features, subject to the marginal constraint that feature subsets are distributed according to Latent Dirichlet Allocation. Intuitively, this constraint favors feature partitions that have coherent topical semantics. Furthermore, MVM uses soft feature assignment, hence the contribution of each data point to each clustering view is variable, isolating the impact of data only to views where they assign the most features. Through a series of experiments, we demonstrate the utility of MVM as an inductive bias for capturing relations between words that are intuitive to humans, outperforming related models such as Latent Dirichlet Allocation. 1"
10.1.1.226.404,Methods,2012,AutoLockMass- This function decides where the lock mass scans are in the xcmsRaw object. This is done by using the scan time differences. Arguments object An xcmsRaw-class object
10.1.1.225.9097,Hard-To-Use Interfaces Considered Beneficial (Some of the Time),,Copyright is held by the author/owner(s).
10.1.1.224.6866,Jointly Learning to Extract and Compress,,"We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a marginbased objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set. 1"
10.1.1.224.5470,Cross-Cutting Models of Lexical Semantics,,"Context-dependent word similarity can be measured over multiple cross-cutting dimensions. For example, lung and breath are similar thematically, while authoritative and superficial occur in similar syntactic contexts, but share little semantic similarity. Both of these notions of similarity play a role in determining word meaning, and hence lexical semantic models must take them both into account. Towards this end, we develop a novel model, Multi-View Mixture (MVM), that represents words as multiple overlapping clusterings. MVM finds multiple data partitions based on different subsets of features, subject to the marginal constraint that feature subsets are distributed according to Latent Dirichlet Allocation. Intuitively, this constraint favors feature partitions that have coherent topical semantics. Furthermore, MVM uses soft feature assignment, hence the contribution of each data point to each clustering view is variable, isolating the impact of data only to views where they assign the most features. Through a series of experiments, we demonstrate the utility of MVM as an inductive bias for capturing relations between words that are intuitive to humans, outperforming related models such as Latent Dirichlet Allocation. 1"
10.1.1.224.3770,19 Influenza Influenza,2011,"The disease Influenza is an acute viral infection of the respiratory tract. There are three types of influenza virus: A, B and C. Influenza A and influenza B are responsible for most clinical illness. Influenza is highly infectious with a usual incubation period of one to three days. The disease is characterised by the sudden onset of fever, chills, headache, myalgia and extreme fatigue. Other common symptoms include a dry cough, sore throat and stuffy nose. For otherwise healthy individuals, influenza is an unpleasant but usually self-limiting disease with recovery usually within two to seven days. The illness may be complicated by (and may present as) bronchitis, secondary bacterial pneumonia or, in children, otitis media. Influenza can be complicated by meningitis, encephalitis or meningoencephalitis. The risk of serious illness from influenza is higher amongst children under six months of"
10.1.1.224.1983,19 Influenza Influenza,2011,"The disease Influenza is an acute viral infection of the respiratory tract. There are three types of influenza virus: A, B and C. Influenza A and influenza B are responsible for most clinical illness. Influenza is highly infectious with a usual incubation period of one to three days. The disease is characterised by the sudden onset of fever, chills, headache, myalgia and extreme fatigue. Other common symptoms include a dry cough, sore throat and stuffy nose. For otherwise healthy individuals, influenza is an unpleasant but usually self-limiting disease with recovery usually within two to seven days. The illness may be complicated by (and may present as) bronchitis, secondary bacterial pneumonia or, in children, otitis media. Influenza can be complicated by meningitis, encephalitis or meningoencephalitis. The risk of serious illness from influenza is higher amongst children under six months of"
10.1.1.222.7587,of LaborComparing the Early Research Performance of PhD Graduates in Labor Economics in Europe and the USA,2008,"Any opinions expressed here are those of the author(s) and not those of IZA. Research published in this series may include views on policy, but the institute itself takes no institutional policy positions. The Institute for the Study of Labor (IZA) in Bonn is a local and virtual international research center and a place of communication between science, politics and business. IZA is an independent nonprofit organization supported by Deutsche Post World Net. The center is associated with the University of Bonn and offers a stimulating research environment through its international network, workshops and conferences, data service, project support, research visits and doctoral program. IZA engages in (i) original and internationally competitive research in all fields of labor economics, (ii) development of policy concepts, and (iii) dissemination of research results and concepts to the interested public. IZA Discussion Papers often represent preliminary work and are circulated to encourage discussion. Citation of such a paper should account for its provisional character. A revised version may be"
10.1.1.221.7572,On Leighton’s graph covering theorem,,"Abstract. We give short expositions of both Leighton’s proof and the Bass-Kulkarni proof of Leighton’s graph covering theorem, in the context of colored graphs. We discuss a further generalization, needed elsewhere, to “symmetryrestricted graphs. ” We can prove it in some cases, for example, if the “graph of colors ” is a tree, but we do not know if it is true in general. We show that Bass’s Conjugation Theorem, which is a tool in the Bass-Kulkarni approach, does hold in the symmetry-restricted context. Leighton’s graph covering theorem says: Theorem (Leighton [5]). Two finite graphs which have a common covering have a common finite covering. It answered a conjecture of Angluin and Gardiner who had proved the case that both graphs are k–regular [1]. Leighton’s proof is short (two pages), but has been considered by some to lack transparency. It was reframed in terms of Bass-Serre theory by Bass and Kulkarni [2, 3], expanding its length considerably but providing group-theoretic tools which have other uses."
10.1.1.221.7060,Documentation Check List,2005,The revision starts after page 4 of this document
10.1.1.220.7843,Towards Interactivity for TEX,,"Much work has been done to improve the level of interactivity avdable to TEX users. This work is categorized, and probable reasons are discussed why it is not really widespread. A more general view of ""interactivity "" may also lead to other tools. A common prerequisite for all these tools is the need to know about TEX'S functionality. The description of TEX should be formal, since the avdable lnformal descriptions have not given satisfactory results. After an abstract decomposition of TEX, an approach for the formal specification of one subsystem (the macro language) is presented. This specification may be interpreted by a Common Lisp system. The resulting Executable TEX Language Specification (ETLS) can be used as the kernel of a TEX macro debugger. Variations on A Theme ""Interactive TEX is the oldest theme on TUG meetings:"
10.1.1.220.504,Tutorial to Locales and Locale Interpretation,,"Locales are Isabelle’s approach for dealing with parametric theories. They have been designed as a module system for a theorem prover that can adequately represent the complex inter-dependencies between structures found in abstract algebra, but have proven fruitful also in other applications — for example, software verification. Both design and implementation of locales have evolved considerably since Kammüller did his initial experiments. Today, locales are a simple yet powerful extension of the Isar proof language. The present tutorial covers all major facilities of locales. It is intended for locale novices; familiarity with Isabelle and Isar is presumed. 1"
10.1.1.220.234,Convergence Distance,,"⇒ All messages scanned by security gateways 2. Wireless (WiFi) is spreading (Intel Centrino) 3. More Cell phones than POTS. Smart Cell phones w PDA, email, video, images ⇒ Mobility 4. Broadband Access is growing faster than cell phones Fiber is creeping towards home 5. Ethernet extending from Enterprise to Access to Metro … 6. Wiring more expensive than equipment ⇒ Wireless Access 7. Multi-Protocol Label Switching for traffic engineering 8. Voice over Internet Protocol (VOIP) is in the Mainstream"
10.1.1.22.741,Grid Information Services for Distributed Resource Sharing,2001,"Grid technologies enable large-scale sharing of resources within formal or informal consortia of individuals and/or institutions: what are sometimes called virtual organizations. In these settings, the discovery, characterization, and monitoring of resources, services, and computations are challenging problems due to the considerable diversity, large numbers, dynamic behavior, and geographical distribution of the entities in which a user might be interested. Consequently, information services are a vital part of any Grid software infrastructure, providing fundamental mechanisms for discovery and monitoring, and hence for planning and adapting application behavior.  We present here an information services architecture that addresses performance, security, scalability, and robustness requirements. Our architecture defines simple low-level enquiry  and registration protocols that make it easy to incorporate individual entities into various information structures, such as aggregate directories that support a variety of different query languages and discovery strategies. These protocols can also be combined with other Grid protocols to construct additional higher-level services and capabilities such as brokering, monitoring, fault detection, and troubleshooting. Our architecture has been implemented as MDS-2, which forms part of the Globus Grid toolkit and has been widely deployed and applied.  "
10.1.1.22.6188,Modeling and Rendering of Weathered Stone,0,"Stone is widespread in its use as a building material and artistic medium. One of its most remarkable qualities is that it changes appearance as it interacts with the environment. These changes are mainly confined to the surface but involve complex volumetric effects such as erosion and mineral dissolution. This paper presents an approach for the modeling and rendering of changes in the shape and appearance of stone.  To represent stone, we introduce a slab data structure, which is a surface-aligned volume confined to a narrow region around the boundary of the stone. Our weathering model employs a simulation of the flow of moisture and the transport, dissolution, and recrystallization of minerals within the porous stone volume. In addition, this model governs the erosion of material from the surface. To render the optical effects of translucency and coloration due to the composition of minerals near the surface, we simulate the scattering of light inside the stone using a general subsurface Monte Carlo ray tracer. These techniques can capture many aspects of the time-dependent appearance of stone. We demonstrate the approach with models of granite and marble statues, as well as a sandstone column.  CR Categories: I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism; I.6.3 [Simulation and Modeling]: Applications. Additional Keywords: erosion, material models, natural phenomena, physical simulation, ray tracing, subsurface scattering, texturing, volume modeling, weathering.  1 "
10.1.1.22.3739,A Unified Approach to Spatial Outliers Detection,2003,"Spatial outliers represent locations which are significantly different from their neighborhoods even though they may not be significantly different from the entire population. Identification of spatial outliers can lead to the discovery of unexpected, interesting, and implicit knowledge, such as local instability. In this paper, we first provide a general definition of S-outliers for spatial outliers. This definition subsumes the traditional definitions of spatial outliers. Second, we characterize the computation structure of spatial outlier detection methods and present scalable algorithms. Third, we provide a cost model of the proposed algorithms. Finally, we provide experimental evaluations of our algorithms using a MinneapolisSt. Paul(Twin Cities) traffic data set."
10.1.1.22.3256,Eraser: A Dynamic Data Race Detector for Multi-Threaded Programs,1997,"Multi-threaded programming is difficult and error prone. It is easy to make a mistake in synchronization that produces a data race, yet it can be extremely hard to locate this mistake during debugging. This paper describes a new tool, called Eraser, for dynamically detecting data races in lock-based multi-threaded programs. Eraser uses binary rewriting techniques to monitor every shared memory reference and verify that consistent locking behavior is observed. We present several case studies, including undergraduate coursework and a multi-threaded Web search engine, that demonstrate the effectiveness of this approach. 1 Introduction  Multi-threading has become a common programming technique. Most commercial operating systems support threads, and popular applications like Microsoft Word and Netscape Navigator are multi-threaded. Unfortunately, debugging a multi-threaded program can be difficult. Simple errors in synchronization can produce timing-dependent data races that can take weeks ..."
10.1.1.22.2354,Efficient Approximation and Optimization Algorithms for Computational Metrology,1997,"We give efficient algorithms for solving several geometric problems in computational metrology, focusing on the fundamental issues of ""flatness"" and ""roundness."" Specifically, we give approximate and exact algorithms for 2- and 3-dimensional roundness primitives, deriving results that improve previous approaches in several respects, including problem definition, running time, underlying computational model, and dimensionality of the input. We also study methods for determining the width of a d-dimensional point set, which corresponds to the metrology notion of ""flatness,"" giving an approximation method that can serve as a fast exactcomputation filter for this metrology primitive. Finally, we report on experimental results derived from implementation and testing, particularly in 3-space, of our approximation algorithms, including several heuristics designed to significantly speed-up the computations in practice.  "
10.1.1.22.2062,Attempto Controlled English (ACE),,Contents 1 What is Attempto Controlled English (ACE)? .................................................................1 2 ACE in a Nutshell.............................................................................................................2 3 Grammar of ACE..............................................................................................................9  3.1 Some Terminology............................................................................................................................. 9 3.2 Simple Sentences ............................................................................................................................. 10 3.2.1 Basic Form .......................................................................................................................... 10 3.2.2 Modifying Nouns and Noun Phrases .................................................................................. 12 3.2.2.1 Adjectives ...................
10.1.1.22.1835,MPEGTool: An X Window Based MPEG Encoder and Statistics Tool,1993,"In this paper, we describe MPEGTool, an X window based  tool which can be used to generate an MPEG  2  encoded bit  stream for video sequences and to study the statistical  properties of the encoded data. It is a very versatile tool that  was designed to study the characteristics of variable bit rate  video sources for transmission over ATM  3  based BISDN  4  .  The tool, which has a window based graphical user  interface, allows a user to specify several of the MPEG  parameters such as the intraframe to interframe ratio, and  the quantizer scale. The tool also includes a statistical  package which allows the user to plot graphs of various  statistics including bit distributions, ATM cell distributions,  time series, autocorrelation functions and cell interarrival  times.  1.0 "
10.1.1.219.6360,ACKNOWLEDGEMENT,,"(CIFOR), 2007."
10.1.1.219.5099,Geodesic distance-weighted shape vector image diffusion,,"Abstract—This paper presents a novel and efficient surface matching and visualization framework through the geodesic distanceweighted shape vector image diffusion. Based on conformal geometry, our approach can uniquely map a 3D surface to a canonical rectangular domain and encode the shape characteristics (e.g., mean curvatures and conformal factors) of the surface in the 2D domain to construct a geodesic distance-weighted shape vector image, where the distances between sampling pixels are not uniform but the actual geodesic distances on the manifold. Through the novel geodesic distance-weighted shape vector image diffusion presented in this paper, we can create a multiscale diffusion space, in which the cross-scale extrema can be detected as the robust geometric features for the matching and registration of surfaces. Therefore, statistical analysis and visualization of surface properties across subjects become readily available. The experiments on scanned surface models show that our method is very robust for feature extraction and surface matching even under noise and resolution change. We have also applied the framework on the real 3D human neocortical surfaces, and demonstrated the excellent performance of our approach in statistical analysis and integrated visualization of the multimodality volumetric data over the shape vector image. Index Terms—Surface Matching, Shape Vector Image, Multiscale Diffusion, Visualization. 1"
10.1.1.219.4963,3Center for the Neural Basis of Cognition,,"of the eye movement command is thought to trigger remapping. Updating creates a stable representation of space by compensating for the displacement of objects on the retina. We hypothesized that spatial updating also occurs in humans. Behavioral results in humans and nonhuman primates have shown that they have similar abilities in double-step eye movement tasks that require the use of updated visual information (Baizer and Bender, 1989; Hallett and Lightstone, 1976). Moreover, the parietal lobe is critical for task performance. Humans with parietal lobe damage are unable to perform double-step tasks (Duhamel et al., 1992b; Heide et al., 1995), and parietal neurons in monkeys are specifically active in these tasks (Goldberg et al., 1990). We thus hypothesized that up-"
10.1.1.218.9310,Status of this Memo Audio/Video Transport Working Group,2000,"RTP: A Transport Protocol for Real-Time Applications This document is an Internet-Draft and is in full conformance with all provisions of Section 10 of RFC2026. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months and may be updated, replaced, or obsoleted by other documents at any time. It is inappropriate to use Internet-Drafts as reference material or to cite them other than as “work in progress.” The list of current Internet-Drafts can be accessed at"
10.1.1.218.7681,Parameter synthesis in nonlinear dynamical systems: Application to systems biology,2009,"Abstract. The dynamics of biological processes are often modeled as systems of nonlinear ordinary differential equations (ODE). An important feature of nonlinear ODEs is that seemingly minor changes in initial conditions or parameters can lead to radically different behaviors. This is problematic because in general it is never possible to know/measure the precise state of any biological system due to measurement errors. The parameter synthesis problem is to identify sets of parameters (including initial conditions) for which a given system of nonlinear ODEs does not reach a given set of undesirable states. We present an efficient algorithm for solving this problem that combines sensitivity analysis with an efficient search over initial conditions. It scales to high-dimensional models and is exact if the given model is affine. We demonstrate our method on a model of the acute inflammatory response to bacterial infection, and identify initial conditions consistent with 3 biologically relevant outcomes."
10.1.1.216.977,5 Don’t All Running Programs Introspect? 5,2010,"This paper extends three decades of work arguing that researchers who discuss consciousness should not restrict themselves only to (adult) human minds, but should study (and attempt to model) many kinds of minds, natural and artificial, thereby contributing to our understanding of the space containing all of them. We need to study what they do or can do, how they can do it, and how the natural ones can be emulated in synthetic minds. That requires: (a) understanding sets of requirements that are met by different sorts of minds, i.e. the niches that they occupy, (b) understanding the space of possible designs, and (c) understanding complex and varied relationships between requirements and designs. Attempts to model or explain any particular phenomenon, such as vision, emotion, learning, language use, or consciousness lead to muddle and confusion unless they are placed in that broader context. A methodology for making progress is summarised and a novel requirement proposed for a theory of how human minds work: the theory should support a single generic design for a learning, developing system that, in addition to meeting familiar requirements, should be capable of developing different and opposed philosophical viewpoints about consciousness, and the so-called hard problem. In other words, we need a common explanation for the mental machinations of mysterians, materialists, functionalists,"
10.1.1.216.9539,Comparison of generative and discriminative techniques for object detection and classification,2006,"Abstract. Many approaches to object recognition are founded on probability theory, and can be broadly characterized as either generative or discriminative according to whether or not the distribution of the image features is modelled. Generative and discriminative methods have very different characteristics, as well as complementary strengths and weaknesses. In this chapter we introduce new generative and discriminative models for object detection and classification based on weakly labelled training data. We use these models to illustrate the relative merits of the two approaches in the context of a data set of widely varying images of non-rigid objects (animals). Our results support the assertion that neither approach alone will be sufficient for large scale object recognition, and we discuss techniques for combining the strengths of generative and discriminative approaches. 1"
10.1.1.216.225,"PRODUCTS, INTEL ASSUMES NO LIABILITY WHATSOEVER AND INTEL DISCLAIMS ANY EXPRESS OR IMPLIED WARRANTY, RELATING TO SALE AND/OR USE OF INTEL PRODUCTS INCLUDING LIABILITY OR WARRANTIES",,"TION WHERE PERSONAL INJURY OR DEATH MAY OCCUR. Intel may make changes to specifications and product descriptions at any time, without notice. Designers must not rely on the absence or characteristics of any features or instructions marked ""reserved "" or ""undefined."" Intel reserves these for future definition and shall have no responsibility whatsoever for conflicts or incompatibilities arising from future changes to them. The information here is subject to change without notice. Do not finalize a design with this information. The Intel ® 64 architecture processors may contain design defects or errors known as errata. Current characterized errata are available on request. Intel ® Hyper-Threading Technology requires a computer system with an Intel ® processor supporting Intel Hyper-Threading Technology and an Intel ® HT Technology enabled chipset, BIOS and operating system. Performance will vary depending on the specific hardware and software you use. For more information, see"
10.1.1.214.249,Semi-Supervised Bio-Named Entity Recognition with Word-Codebook Learning,,"We describe a novel semi-supervised method called Word-Codebook Learning (WCL), and apply it to the task of bionamed entity recognition (bioNER). Typical bioNER systems can be seen as tasks of assigning labels to words in bioliterature text. To improve supervised tagging, WCL learns a class of word-level feature embeddings to capture word semantic meanings or word label patterns from a large unlabeled corpus. Words are then clustered according to their embedding vectors through a vector quantization step, where each word is assigned into one of the codewords in a codebook. Finally codewords are treated as new word attributes and are added for entity labeling. Two types of wordcodebook learning are proposed: (1) General WCL, where an unsupervised method uses contextual semantic similarity of words to learn accurate word representations; (2) Task-oriented WCL, where for every word a semi-supervised method learns target-class label patterns from unlabeled data using supervised signals from trained bioNER model. Without the need for complex linguistic features, we demonstrate utility of WCL on the BioCreativeII gene name recognition competition data, where WCL yields state-of-the-art performance and shows great improvements over supervised baselines and semi-supervised counter peers."
10.1.1.212.5243,Some demonstrations of the effects of structural descriptions in mental imagery,1979,"A visual imagery task is presented which is beyond the limits of normal human ability, and some of the factors contributing to its difficulty are isolated by comparing the difficulty of related tasks. It is argu~,cl that complex objects are assigned hierarchical structural descriptions by being parsed into parts, each of which has its own local system of significant directions. Two quite different schemas for a wire-frame cube are used to illustrate this theory, and some striking perceptual differences to which they give rise are described. The difficulty of certain mental imagery tasks is shown to depend on which of the alternative structural descriptions of an object is used, and this is interpreted as evidence that structural descriptions are an important component of mental images. Finally, it is argued that analog transformations like mental folding involve changing the values of continuous variables in a structural description. 1."
10.1.1.212.3815,References,,"edit math.chapman.edu/structures 1 Definition 1. A... is a structure A = 〈A,... 〉 of type 〈... 〉 such that 〈A,... 〉 is a name of class op1 is (name of property): axiom1 op2 is...:... Remark: This is a template. If you know something about this class, click on the ’Edit text of this page ’ link at the bottom and fill out this page. It is not unusual to give several (equivalent) definitions. Ideally, one of the definitions would give an irredundant axiomatization that does not refer to other classes. Morphisms. Let A and B be.... A morphism from A to B is a function h: A → B that is a homomorphism: h(x...y)  = h(x)...h(y) Definition 2. An... is a structure A = 〈A,... 〉 of type 〈... 〉 such that... is...: axiom... is...: axiom"
10.1.1.212.1256,References,,"edit math.chapman.edu/structures 1 Definition 1. A... is a structure A = 〈A,... 〉 of type 〈... 〉 such that 〈A,... 〉 is a name of class op1 is (name of property): axiom1 op2 is...:... Remark: This is a template. If you know something about this class, click on the “Edit text of this page ” link at the bottom and fill out this page. It is not unusual to give several (equivalent) definitions. Ideally, one of the definitions would give an irredundant axiomatization that does not refer to other classes. Morphisms. Let A and B be.... A morphism from A to B is a function h: A → B that is a homomorphism: h(x...y)  = h(x)...h(y) Definition 2. A... is a structure A = 〈A,... 〉 of type 〈... 〉 such that... is...: axiom... is...: axiom"
10.1.1.211.9857,of LaborComparing the Early Research Performance of PhD Graduates in Labor Economics in Europe and the USA,2008,"Any opinions expressed here are those of the author(s) and not those of IZA. Research published in this series may include views on policy, but the institute itself takes no institutional policy positions. The Institute for the Study of Labor (IZA) in Bonn is a local and virtual international research center and a place of communication between science, politics and business. IZA is an independent nonprofit organization supported by Deutsche Post World Net. The center is associated with the University of Bonn and offers a stimulating research environment through its international network, workshops and conferences, data service, project support, research visits and doctoral program. IZA engages in (i) original and internationally competitive research in all fields of labor economics, (ii) development of policy concepts, and (iii) dissemination of research results and concepts to the interested public. IZA Discussion Papers often represent preliminary work and are circulated to encourage discussion. Citation of such a paper should account for its provisional character. A revised version may be"
10.1.1.211.7178,An overview of data warehousing and OLAP technology,1997,"Data warehousing and on-line analytical processing (OLAP) are essential elements of decision support, which has increasingly become a focus of the database industry. Many commercial products and services are now available, and all of the principal database management system vendors now have offerings in these areas. Decision support places some rather different requirements on database technology compared to traditional on-line transaction processing applications. This paper provides an overview of data warehousing and OLAP technologies, with an emphasis on their new requirements. We describe back end tools for extracting, cleaning and loading data into a data warehouse; multidimensional data models typical of OLAP; front end client tools for querying and data analysis; server extensions for efficient query processing; and tools for metadata management and for managing the warehouse. In addition to surveying the state of the art, this paper also identifies some promising research issues, some of which are related to problems that the database research community has worked on for years, but others are only just beginning to be addressed. This overview is based on a tutorial that the authors presented at the VLDB Conference, 1996. 1."
10.1.1.211.6928,3Center for the Neural Basis of Cognition,,"of the eye movement command is thought to trigger remapping. Updating creates a stable representation of space by compensating for the displacement of objects on the retina. We hypothesized that spatial updating also occurs in humans. Behavioral results in humans and nonhuman primates have shown that they have similar abilities in double-step eye movement tasks that require the use of updated visual information (Baizer and Bender, 1989; Hallett and Lightstone, 1976). Moreover, the parietal lobe is critical for task performance. Humans with parietal lobe damage are unable to perform double-step tasks (Duhamel et al., 1992b; Heide et al., 1995), and parietal neurons in monkeys are specifically active in these tasks (Goldberg et al., 1990). We thus hypothesized that up-"
10.1.1.210.5731,Nonsense paper588 by Tim Moors,,"This is one of a series of nonsensical documents, created as part of a study described in [1] available at"
10.1.1.210.5560,Nonsense paper587 by Tim Moors,,"This is one of a series of nonsensical documents, created as part of a study described in [1] available at"
10.1.1.210.4195,A case for adapting channel width in wireless networks,2008,"We study a fundamental yet under-explored facet in wireless communication – the width of the spectrum over which transmitters spread their signals, or the channel width. Through detailed measurements in controlled and live environments, and using only commodity 802.11 hardware, we first quantify the impact of channel width on throughput, range, and power consumption. Taken together, our findings make a strong case for wireless systems that adapt channel width. Such adaptation brings unique benefits. For instance, when the throughput required is low, moving to a narrower channel increases range and reduces power consumption; in fixed-width systems, these two quantities are always in conflict. We then present SampleWidth, a channel width adaptation algorithm for the base case of two communicating nodes. This algorithm is based on a simple search process that builds on top of existing techniques for adapting modulation. Per specified policy, it can maximize throughput or minimize power consumption. Evaluation using a prototype implementation shows that SampleWidth correctly identities the optimal width under a range of scenarios. In our experiments with mobility, it increases throughput by more than 60 % compared to the best fixed-width configuration.  "
10.1.1.210.3329,Comparative Logic and Modern Quantum Theory Table of Contents,,S ' a b f1 v a / b σe a1 a2 cos(θ) (4) sec(θ) (4) cos 2 (θ) (4) 1 (4) sin(θ)cos(θ) (5.1) tan(θ) (5.1)
10.1.1.210.1687,Not for Quotation TEACHER PERFORMANCE INCENTIVES AND STUDENT OUTCOMES,2000,The paper was prepared for a National Academy of Sciences Conference entitled “Devising Incentives
10.1.1.21.7402,Naturally Conveyed Explanations of Device Behavior,2001,"Designers routinely explain their designs to one another using sketches and verbal descriptions of behavior, both of which can be understood long before the device has been fully specified. But current design tools fail almost completely to support this sort of interaction, instead not only forcing designers to specify details of the design, but typically requiring that they do so by navigating a forest of menus and dialog boxes, rather than directly describing the behaviors with sketches and verbal explanations. We have created a prototype system, called assistance, capable of interpreting multimodal explanations for simple 2-D kinematic devices. The program generates a model of the events and the causal relationships between events that have been described via hand drawn sketches, sketched annotations, and verbal descriptions. Our goal is to make the designer's interaction with the computer more like interacting with another designer. This requires the ability not only to understand physical devices but also to understand the means by which the explanations of these devices are conveyed."
10.1.1.21.6897,Using Prior Knowledge with Adaptive Probing,2001,"When searching a tree to find the best leaf, complete search  methods such as depth-first search and depth-bounded discrepancy  search use a fixed deterministic order that may or  may not be appropriate for the tree at hand. Adaptive probing  is a recently-proposed stochastic method that attempts to  adjust its sampling on-line to focus on areas of the tree that  seem to contain good solutions. While effective on a variety  of trees, adaptive probing wastes time learning basic features  of the problem that are built into other algorithms, such as the  fact that the heuristic is often helpful. In this paper, we investigate  two simple methods for adding such prior knowledge to  adaptive probing. The first simply reuses the model learned  during a previous run on a similar problem. The second uses a  heuristically biased policy at the start of the search, gradually  deferring to learned information in later iterations. Empirical  results on two different representations of number partitioning  confirm that these methods can allow adaptive probing  to search efficiently from the very start of a run. However,  reusing previous models seems to more frequently preserve  the ability of the algorithm to adapt to the search space.  "
10.1.1.21.5977,A Parameterized Type System for Race-Free Java Programs,2001,"This paper presents a new static type system for multithreaded programs; any well-typed program in our system is free of data races. Our type system is significantly more expressive than previous such type systems. In particular, our system lets programmers write generic code to implement a class, then create different objects of the same class that have different protection mechanisms. This exibility enables programmers to reduce the number of unnecessary synchronization operations in a program without risking data races. We also support default types which reduce the burden of writing the extra type annotations. Our experience indicates that our system provides a promising approach to make multithreaded programs more reliable and efficient."
10.1.1.21.4673,Resolving Ambiguity for Cross-language Retrieval,1998,"One of the main hurdles to improved CLIR effectiveness is resolving ambiguity associated with translation. Availability of resources is also a problem. First we present a technique based on co-occurrence statistics from unlinked corpora which can be used to reduce the ambiguity associated with phrasal and term translation. We then combine this method with other techniques for reducing ambiguity and achieve more than 90% monolingual effectiveness. Finally, we compare the co-occurrence method with parallel corpus and machine translation techniques and show that good retrieval effectiveness can be achieved without complex resources.  1 "
10.1.1.21.2382,Algorithms for Index-Assisted Selectivity Estimation,1998,"The standard mechanisms for query selectivity estimation used in relational database systems rely on properties specific to the attribute types. The query optimizer in an extensible database system will, in general, be unable to exploit these mechanisms for user-defined types, forcing the database extender to invent new estimation mechanisms. In this work, we discuss extensions to the generalized search tree, or GiST, that simplify the creation of user-defined selectivity estimation methods. An experimental comparison of such methods with multidimensional estimators from the literature has demonstrated very competitive results.  1. Motivation and General Approach  There has been considerable research in the development of query selectivity estimation techniques, but relatively little in the context of frameworks that can be applied to arbitrary user-defined types. The general frameworks that have been proposed tend to be conceptually simple APIs that impose significant implementation c..."
10.1.1.208.4638,"PRODUCTS, INTEL ASSUMES NO LIABILITY WHATSOEVER AND INTEL DISCLAIMS ANY EXPRESS OR IMPLIED WARRANTY, RELATING TO SALE AND/OR USE OF INTEL PRODUCTS INCLUDING LIABILITY OR WARRANTIES",,"TION WHERE PERSONAL INJURY OR DEATH MAY OCCUR. Intel may make changes to specifications and product descriptions at any time, without notice. Designers must not rely on the absence or characteristics of any features or instructions marked ""reserved "" or ""undefined."" Intel reserves these for future definition and shall have no responsibility whatsoever for conflicts or incompatibilities arising from future changes to them. The information here is subject to change without notice. Do not finalize a design with this information. The Intel ® 64 architecture processors may contain design defects or errors known as errata. Current characterized errata are available on request. Intel ® Hyper-Threading Technology requires a computer system with an Intel ® processor supporting Intel Hyper-Threading Technology and an Intel ® HT Technology enabled chipset, BIOS and operating system. Performance will vary depending on the specific hardware and software you use. For more information, see"
10.1.1.207.7915,Eigenvalues of random power law graphs,2003,"Many graphs arising in various information networks exhibit the “power law ” behavior – the number of vertices of degree k is proportional to k −β for some positive β. We show that if β>2.5, the largest eigenvalue of a random power law graph is almost surely (1 + o(1))  √ m where m is the maximum degree. Moreover, the k largest eigenvalues of a random power law graph with exponent β have power law distribution with exponent 2β − 1 if the maximum degree is sufficiently large, where k is a function depending on β,m and d, the average degree. When 2 <β<2.5, the largest eigenvalue is heavily concentrated at cm 3−β for some constant c depending on β and the average degree. This result follows from a more general theorem which shows that the largest eigenvalue of a random graph with a given expected degree sequence is determined by m, the maximum degree, and ˜ d, the weighted average of the squares of the expected degrees. We show that the k-th largest eigenvalue is almost surely (1 + o(1))  √ mk where mk is the k-th largest expected degree provided mk is large enough. These results have implications on the usage of spectral techniques in many areas related to pattern detection and information retrieval. 1"
10.1.1.207.5636,Jointly Learning to Extract and Compress,,"We learn a joint model of sentence extraction and compression for multi-document summarization. Our model scores candidate summaries according to a combined linear model whose features factor over (1) the n-gram types in the summary and (2) the compressions used. We train the model using a marginbased objective whose loss captures end summary quality. Because of the exponentially large set of candidate summaries, we use a cutting-plane algorithm to incrementally detect and add active constraints efficiently. Inference in our model can be cast as an ILP and thereby solved in reasonable time; we also present a fast approximation scheme which achieves similar performance. Our jointly extracted and compressed summaries outperform both unlearned baselines and our learned extraction-only system on both ROUGE and Pyramid, without a drop in judged linguistic quality. We achieve the highest published ROUGE results to date on the TAC 2008 data set. 1"
10.1.1.207.214,Strong hydrodynamic limit for attractive particle systems on Z,2010, 
10.1.1.207.1180,Generic sentence fusion is an ill-defined summarization task,2004,"We report on a series of human evaluations of the task of sentence fusion. In this task, a human is given two sentences and asked to produce a single coherent sentence that contains only the important information from the original two. Thus, this is a highly constrained summarization task. Our investigations show that even at this restricted level, there is no measurable agreement between humans regarding what information should be considered important. We further investigate the ability of separate evaluators to assess summaries, and find similarly disturbing lack of agreement. 1"
10.1.1.206.1054,Automatic Learning for Semantic Collocation,,"The real difficulty indevelopment of practical NLP systems comes from the fact that we do not have effective means for gathering ""knowledge"". In this paper, we propose an algorithm which acquires automatically knowledge of semantic collocations among ""words"" from sample corpora. The algorithm proposed in this paper tries to discover semantic collocations which will be useful for disambiguating structurally ambiguous sentences, by a statistical approach. The algorithm requires a corpus and minimum linguistic knowledge (parts-of-speech of words, simple inflection rules, and a small number of general syntactic rules). We conducted two experiments of applying the algorithm to different corpora to extract different types of semantic collocations. Though there are some unsolved problems, the results showed the effectiveness of the proposed algorithm."
10.1.1.205.4331,0.1 ls.mixed: Mixed effects Linear Regression Use,,"multi-level linear regression if you have covariates that are grouped according to one or more classification factors and a continuous dependent variable. While generally called multi-level models in the social sciences, this class of models is often referred to as mixed-effects models in the statistics literature and as hierarchical models in a Bayesian setting. This general class of models consists of linear models that are expressed as a function of both fixed effects, parameters corresponding to an entire population or certain repeatable levels of experimental factors, and random effects, parameters corresponding to individual experimental units drawn at random from a population."
10.1.1.205.3718,PRISM User’s Manual (Version 2.0 beta 4),,"The past several years have witnessed a tremendous interest in logic-based probabilistic learning as testified by the number of formalisms and systems and their applications. Logic-based probabilistic learning is a multidisciplinary research area that integrates relational or logic formalisms, probabilistic reasoning mechanisms, and machine learning and data mining principles. Logic-based probabilistic learning has found its way into many application areas including bioinformatics, diagnosis and troubleshooting, stochastic language processing, information retrieval, linkage analysis and discovery, robot control, and probabilistic constraint solving. PRISM (PRogramming In Statistical Modeling) is a logic-based language that integrates logic programming and probabilistic reasoning including parameter learning. It allows for the description of independent probabilistic choices and their consequences in general logic programs. PRISM supports parameter learning, i.e. for a given set of possibly incomplete observed data, PRISM can estimate the probability distributions to best explain the data. This power is suitable for applications such as learning parameters of stochastic grammars, training stochastic models for gene sequence analysis, game record analysis, user modeling, and obtaining probabilistic information for tuning systems performance. PRISM offers incomparable flexibility compared with specific statistical tools such as hidden Markov models (HMMs) [4, 30], probabilistic context free grammars (PCFGs) [4] and discrete Bayesian networks. PRISM employs a proof-theoretic approach to learning. It conducts learning in two phases: the first phase searches for all the explanations for the observed data, and the second phase estimates the probability distributions by using the EM algorithm. Learning from flat explanations can be exponential in both space and time. To speed up learning, the authors proposed learning from explanation graphs and using tabling to reduce redundancy in the construction of explanation graphs. The PRISM programming system is implemented on top of B-Prolog"
10.1.1.204.8931,PostScript LETTER,2010,"Author’s response I want to thank Drs Mahut and Delclaux for their interesting letter concerning our recent paper 1 and would offer the following response. During acute asthma exacerbation only two of 15 patients with asthma (13%) had a combined abnormally elevated central airways nitric oxide (NO) flux and elevated peripheral airway/alveolar NO concentration after correction for NO axial backdiffusion. Central airways NO flux remained the major site of ‘NO-mediated inflammation’ in 13 of 15 patients with asthma since two had normal NO gas exchange despite acute exacerbation. 1 This latter observation needs to be further investigated since the clinical response was similar to that in patients with asthma with abnormal NO gas exchange. Many years ago we investigated the simplified detection of peripheral airway disease and showed that analyses of the distal part of the maximum expiratory flowevolume curve were helpful. 2 However, in a subsequent study 3 we reported that, if the ratio of forced expiratory volume in 1 s to forced vital capacity (FEV1/FVC) was $75%, the occurrence of an isolated abnormal mid forced expiratory flow (FEF25e75) was rare. However, if the FEV1 / FVC was <75%, it would not be unusual to find an abnormal FEF25e75, but it would not discriminate peripheral from large central airways obstruction. 3 I hope these comments are helpful and appreciate their interest."
10.1.1.204.1518,"Item 3.2 of the provisional agenda* COMPILATION OF VIEWS ON THE POTENTIAL ENVIRONMENTAL, CULTURAL AND SOCIO-ECONOMIC IMPACTS OF GENETICALLY MODIFIED TREES",2008,"1. In paragraph 3 of decision VIII/19 B the Conference of the Parties (COP) requested the Executive Secretary to collect and collate existing information, including peer-reviewed published literature, in order to allow the Subsidiary Body on Scientific, Technical and Technological Advice (SBSTTA) to consider and assess the potential environmental, cultural, and socio-economic impacts of genetically modified trees on the conservation and sustainable use of forest biological diversity, and to report to the ninth meeting of the Conference of the Parties. In order to facilitate the collation of information on the potential environmental, cultural and socio-economic impacts of genetically modified trees, the Secretariat distributed, through notification 2006-027 of 4 May 2006, a questionnaire to Parties and relevant organizations inviting them to provide information. 2. Accordingly, the Executive Secretary is circulating herewith, for the information of participants of the thirteenth session of the Subsidiary Body on Scientific, Technical and Technological Advice, a compilation of views received in response to the questionnaire. 3. The submissions are reproduced in the form and the language in which they were received by the Secretariat and without annexes. * UNEP/CBD/SBSTTA/13/1."
10.1.1.20.9835,Subtyping Patterns for Active Objects,2000,"Subtyping relations for object-oriented formalisms describe relationships between super- and subclasses which satisfy the substitutability requirement imposed on types and their subtypes. For active objects with an associated behaviour description subtyping relations also have to guarantee substitutability with respect to the dynamic behaviour of classes.  In this paper, we give subtyping patterns for integrated  object-oriented formalisms, that is, with a description of a static as well as a dynamic part. The patterns can be used to obtain subtypes by construction without computing the semantics of classes at all. "
10.1.1.20.8918,"Is It ""Economics and Psychology""?: The Case of Hyperbolic Discounting",,"The paper questions the methodology of ""economics and psychology"". It focuses on the case of hyperbolic discounting. Using some experimental results, I argue that the same sort of evidence which rejects the standard constant discount utility functions can just as easily reject hyperbolic discounting as well. Furthermore, a decision-making procedure based on similarity relations better explains the observations and is more intuitive. The paper concludes that combining ""economics and psychology"" requires opening the black box of decision-makers rather than modifying functional forms. Key words: Hyperbolic discounting, choice over time, similarity, procedure, decision making.  "
10.1.1.20.4800,A Bibliography of Papers in Lecture Notes in Computer Science (2002) (Part 4 of 4),2002,Version 1.01 Title word cross-reference (2
10.1.1.2.9400,Design of Large-Scale Polylingual Systems,2004,"Building systems from existing applications written  in two or more languages is common practice. Such systems  are polylingual. Polylingual systems are relatively easy  to build when the number of APIs needed to achieve language  interoperability is small. However, when the number  of distinct APIs become large, maintaining and evolving  polylingual systems becomes a notoriously difficult task."
10.1.1.2.3165,Complexity of Computing Semi-algebraic Descriptions of the Connected Components of a Semi-algebraic Set,1997,"Given Q 2 R[X1 ; : : : ; Xk ] with deg(Q)  d; we give an algorithm that outputs a semi-algebraic description for each of the semi-algebraically connected components of  Z(Q) ae R  k  : The complexity of the algorithm as well as the size of the output are bounded by d  O(k  3  )  :  More generally, given any semi-algebraic set S defined by a quantifier-free formula involving a family of polynomials, P = fP1 ; : : : ; Psg ae R[X1 ; : : : ; Xk ] whose degrees are at most d; we give an algorithm that outputs a semi-algebraic description for each of the semialgebraically connected components of S: The complexity of the algorithm as well as the size of the output is bounded by s  k+1  d  O(k  3  )  : This improves the previously best known bound of (sd)  k O(1)  for this problem due to Canny, Grigor'ev, Vorobjov and Heintz, Roy and Solern`o [9, 14].  1 Introduction  Let R be a real closed field. A semi-algebraic set in R  k  is the set of points which satisfy a boolean combination of polynom..."
10.1.1.2.1307,CyberCut: An Internet-based CAD/CAM System,2001,"Introduction  In 21st century manufacturing, the 20th century concept of a monolithic organization clinging to one centralized corporate may fade. The new culture may well be smaller, more agile corporations that spring up for specific purposes, exist while the market sustains the new product, and then gracefully disband as the market changes #1#. In other words, the roles in different stages of a product development cycle, usually played by specific departments in a large corporation, now may be played by smaller networked companies in various locations. The rapidly expanding Internet provides the information infrastructure for such new manufacturing enterprises. However, it also creates more challenges in the traditional communications between design and manufacturing, that are often colloquially referred to as ""over the wall manufacturing."" While the Internet has the potential to integrate many sub-contractors, this ""wall"" can often be even hi"
10.1.1.194.2559,Hazardous Weather Conditions Policy,2003,"With the potential for extreme weather conditions, it is necessary to clarify the policy for office closure, excused absences and the coding of time and attendance (T&A) reports. Please read this policy carefully. Since all employees will be expected to be familiar with this policy if weather conditions so warrant, it is strongly encouraged that each employee keep a copy of this policy handy at home. First, and most important, employees are to presume the office is open for business, regardless of weather conditions or emergency situations that may exist. Each employee is responsible for assessing the weather and transportation conditions that may exist and determining the risk of commuting to work. The Bureau of Land Management (BLM) and Forest Service (FS) do not require employees to travel during inclement weather; therefore, a liberal leave policy will be in effect during these periods. Employees who determine the risk is too great will be granted personal leave (annual leave, credit hours, compensatory time and leave without pay [LWOP]). Employees under a flexible schedule may adjust their hours to complete their basic work requirement. However, if conditions are so severe (e.g., public safety authorities urge people to stay home), the office may be closed for all or part of the day. The primary source of information for FS and BLM employees concerning office closure or delayed arrival due to weather conditions is by contacting either of the following numbers: BLM-OSO Status Lines: (503) 808-6000 or 1-800-935-4884; or"
10.1.1.194.2545,All Forest Service Employees in the Portland Commuting Area,,"With the potential for extreme weather conditions, it is necessary to clarify the policy for office closure, excused absences and the coding of time and attendance (T&A) reports. Please read this policy carefully. Since all employees will be expected to be familiar with this policy if weather conditions so warrant, it is strongly encouraged that each employee keep a copy of this policy handy at home. First, and most important, employees are to presume the office is open for business, regardless of weather conditions or emergency situations that may exist. Each employee is responsible for assessing the weather and transportation conditions that may exist and determining the risk of commuting to work. The Bureau of Land Management (BLM) and Forest Service (FS) do not require employees to travel during inclement weather; therefore, a liberal leave policy will be in effect during these periods. Employees who determine the risk is too great will be granted personal leave (annual leave, credit hours, compensatory time and leave without pay [LWOP]). Employees under a flexible schedule may adjust their hours to complete their basic work requirement. However, if conditions are so severe (e.g., public safety authorities urge people to stay home), the office may be closed for all or part of the day. The primary source of information for FS and BLM employees concerning office closure or delayed arrival due to weather conditions is by contacting either of the following numbers:"
10.1.1.193.9150,Contents,,2. Equivariant orthogonal spectra 6 3. Equivariant homotopy groups 12 4. Constructions with equivariant spectra 30
10.1.1.192.4038,Decoupled Simulation in Virtual Reality with The MR Toolkit,1993,"The Virtual Reality (VR) user interface style allows natural hand and body motions to manipulate virtual objects in 3D environments using one or more 3D input devices. This style is best suited to application areas where traditional two-dimensional styles fall short, such as scientific visualization, architectural visualization, and remote manipulation. Currently, the programming effort required to produce a VR application is too large, and many pitfalls must be avoided in the creation of successful VR programs. In this paper we describe the Decoupled Simulation Model for creating successful VR applications, and a software system that embodies this model. The MR Toolkit simplifies the development of VR applications by providing standard facilities required by a wide range of VR user interfaces. These facilities include support for distributed computing, head-mounted displays, room geometry management, performance monitoring, hand input devices, and sound feedback. The MR Toolkit encourages programmers to structure their applications to take advantage of the distributed computing capabilities of workstation networks improving the application's performance. In this paper, the motivations and the architecture of the toolkit are outlined, the programmer's view is described, and a simple application is briefly described."
10.1.1.192.1996,Climate,2007,change and trace gases
10.1.1.190.9025,Modular curves and Ramanujan’s continued fraction,,"Abstract. We use arithmetic models of modular curves to establish some properties of Ramanujan’s continued fraction. In particular, we give a new geometric proof that its singular values are algebraic units that generate specific abelian extensions of imaginary quadratic fields, and we use a mixture of geometric and analytic methods to construct and study an infinite family of two-variable polynomials over Z that are related to Ramanujan’s function in the same way that the classical modular polynomials are related to the classical j-function. We also prove that a singular value on the imaginary axis, necessarily real, lies in a radical tower in R only if all odd prime factors of its degree over Q are Fermat primes; by computing some ray class groups, we give many examples where this necessary condition is not satisfied. 1."
10.1.1.190.7193,Previous Up Next Article Citations From References: 12 From Reviews: 2,,"A multifractal formalism for growth rates and applications to geometrically finite Kleinian groups. (English summary) Ergodic Theory Dynam. Systems 24 (2004), no. 1, 141–170. In this paper the authors set up a thermodynamic formalism for a general class of functions defined over a subshift of finite type (X, σ). These functions are given by tuples (∆, A) of certain sequences of continuous real valued functions {∆n} and {An}, where the family of functions ∆n satisfies some growth properties with respect to the family of functions An. They introduce a generalized notion of pressure for such functions for which variational principles can be shown. In the second part of the paper these results are then applied to families ∆ of so called distance functions, which are positive, increasing and pointwise unbounded. They can be used to introduce a metric on X and the notion of Hausdorff dimension dim∆. The authors determine this dimension for the level sets F∆(a)  = {x ∈ X: limn→ ∞ ∆n(x) n = a}. In the third part of the paper the general formalism is applied to derive a multifractal analysis of the limit sets L(G) of geometrically finite Kleinian groups G with possible parabolic fixed points of different ranks. Finally the authors derive necessary and sufficient conditions for the existence of phase transitions in shifts over infinite alphabets."
10.1.1.190.6952,References,,"Constructing restricted Patterson measures for geometrically infinite Kleinian groups. (English summary) Acta Math. Sin. (Engl. Ser.) 22 (2006), no. 2, 431–446. In this paper, the focus of which is on geometrically infinite Kleinian groups, the authors study exhaustions of such groups by specific subsets which have properties reminiscent of geometrically finite groups. Each of these subsets (the authors call them ρ-restrictions) defines a ρ-restricted limit set which is a closed subset of the limit set of the original group and which carries a (ρ-restricted) Patterson-Sullivan measure. This ρ-restricted Patterson-Sullivan measure shows similarities with the classical Patterson-Sullivan measure for geometrically finite Kleinian groups. The exponent of convergence of the ρ-restriction agrees with the Hausdorff dimension of the ρ-restricted limit set, and the Poincaré series of the ρ-restriction diverges at the critical exponent. Furthermore, as ρ → ∞, the exponent of convergence of the ρ-restriction converges to the exponent of convergence of the original group. Reviewed by Petra Bonfert-Taylor"
10.1.1.190.6279,References,,"The Hausdorff dimension of the set of dissipative points for a Cantor-like model set for singly cusped parabolic dynamics. (English summary) Kodai Math. J. 32 (2009), no. 2, 179–196. {A review for this item is in process.}"
10.1.1.190.4394,Beautiful Serbia,2006,"This paper evaluates Beautiful Serbia, an active labor market program operating in Serbia and Montenegro since January 2004, administered and co-financed by the United Nations Development Program. Program participants proceed through two stages: a vocational training stage and a temporary employment stage in private firms contracted for refurbishment projects. Accession to the second stage is competitive, and participants receive a market wage. We evaluate the program impacts on unemployment probabilities, employment probabilities, employment structure and a range of individual welfare indicators applying matching techniques to a rich survey data set covering the universe of participants and a sample of non-participants. Our findings suggest that both vocational training and temporary employment have a positive net impact on individuals. However, on the basis of cost-benefit analysis, we conclude that only the temporary"
10.1.1.190.2303,e-Business Applications,2006,ebXML for e-Business Applications
10.1.1.19.5722,Perceptual Watermarks for Digital Images and Video,0,The growth of new imaging technologies has created a need for techniques that can be used for copyright protection of digital images. Copyright protection involves the authentication of image content and/or ownership and can be used to identify illegal copies of a (possibly forged) image. One approach for copyright protection is to introduce an invisible signal known as a digital watermark in the image.
10.1.1.19.4414,Design for a Decentralized Security System,2000,"This paper describes an architecture for a secure file system based on networkattached  storage that guarantees end-to-end encryption for all user data. We  describe the design of this system, focusing on the features that allow it to ensure  that data is written and read only by authorized users, even in the face of attacks  such as network snooping and physically capturing the storage media."
10.1.1.19.2460,Integration of Advice in an Action-Selection Architecture,2002,"The introduction of a coach competition in the RoboCup2001 simulation league raised many questions concerning the development of a ""coachable"" team. This paper addresses the issues of dealing with conflicting advice and knowing when to listen to advice. An action-selection architecture is proposed to support the integration of advice into an agent's set of beliefs. The results from the coach competition are discussed and provide a basis for experiments. Results are provided to support the claim that the architecture is well-suited for such a task."
10.1.1.188.8583,"Measurement, Experimentation, Performance",,"The commoditization of hardware, data center economies of scale, and Internet-scale workload growth all demand greater power efficiency to sustain scalability. Traditional enterprise workloads, which are typically memory and I/O bound, have been well served by chip multiprocessorscom prising of small, power-efficient cores. Recent advances in mobile computing have led to modern small cores capable of delivering even better power efficiency. While these cores can deliver performance-per-Watt efficiency for data center workloads, small cores impact application quality-of-service robustness, and flexibility, as these workloads increasingly invoke computationally intensive kernels. These challenges constitute the price of efficiency. We quantify efficiency for an industry-strength online web search engine in production at both the microarchitecture- and system-level, evaluating search on server and mobile-class architectures using Xeon and Atom processors."
10.1.1.188.6840,PostScript LETTER,2010,"Author’s response I want to thank Drs Mahut and Delclaux for their interesting letter concerning our recent paper 1 and would offer the following response. During acute asthma exacerbation only two of 15 patients with asthma (13%) had a combined abnormally elevated central airways nitric oxide (NO) flux and elevated peripheral airway/alveolar NO concentration after correction for NO axial backdiffusion. Central airways NO flux remained the major site of ‘NO-mediated inflammation’ in 13 of 15 patients with asthma since two had normal NO gas exchange despite acute exacerbation. 1 This latter observation needs to be further investigated since the clinical response was similar to that in patients with asthma with abnormal NO gas exchange. Many years ago we investigated the simplified detection of peripheral airway disease and showed that analyses of the distal part of the maximum expiratory flowevolume curve were helpful. 2 However, in a subsequent study 3 we reported that, if the ratio of forced expiratory volume in 1 s to forced vital capacity (FEV1/FVC) was $75%, the occurrence of an isolated abnormal mid forced expiratory flow (FEF25e75) was rare. However, if the FEV1 / FVC was <75%, it would not be unusual to find an abnormal FEF25e75, but it would not discriminate peripheral from large central airways obstruction. 3 I hope these comments are helpful and appreciate their interest."
10.1.1.188.5446,AMENDED AND RESTATED DELEGATION AGREEMENT BETWEEN NORTH AMERICAN ELECTRIC RELIABILITY CORPORATION AND SERC RELIABILITY CORPORATION AMENDED AND RESTATED DELEGATION AGREEMENT (“Agreement”) made as of,,"organization certified by the Federal Energy Regulatory Commission (“Commission”) pursuant to Section 215(c) of the Federal Power Act to establish and enforce Reliability Standards for the Bulk-Power System, and SERC Reliability Corporation (“SERC”), an organization established to develop and enforce Reliability Standards within the geographic boundaries identified on Exhibit A to this Agreement, and for other purposes. NERC and SERC may be individually referred to herein as “Party ” or collectively as “Parties.” WITNESSETH WHEREAS, Subtitle A of the Electricity Modernization Act of 2005 added Section 215 to the Federal Power Act (16 U.S.C. § 824n) (hereafter “the Act”), which, among other things, provides for the establishment of an electric reliability organization (“ERO”) to develop and enforce Reliability Standards applicable to all owners, operators, and users of the Bulk-Power System; WHEREAS, the Commission has adopted regulations for the implementation of the Act, which are set forth at Chapter I, Title 18, Code of Federal Regulations, Part 39 (the “ERO Regulations”); WHEREAS, the Commission has certified NERC as the ERO that will, in accordance with the Act, establish and enforce Reliability Standards for the Bulk-Power System, subject to certain delegation provisions described below; WHEREAS, the Act recognizes the international interdependency of electric reliability within North America and envisions the ERO and such applicable Regional Entities as international organizations; WHEREAS, the Act and Section 39.8 of the ERO Regulations provide for the delegation by the ERO of authority to propose and enforce Reliability Standards to regional entities (“Regional Entities”) such as SERC provided that: (A) The Regional Entity is governed by —"
10.1.1.188.5331,Tutorial to Locales and Locale Interpretation,,"Locales are Isabelle’s approach for dealing with parametric theories. They have been designed as a module system for a theorem prover that can adequately represent the complex inter-dependencies between structures found in abstract algebra, but have proven fruitful also in other applications — for example, software verification. Both design and implementation of locales have evolved considerably since Kammüller did his initial experiments. Today, locales are a simple yet powerful extension of the Isar proof language. The present tutorial covers all major facilities of locales. It is intended for locale novices; familiarity with Isabelle and Isar is presumed. 1"
10.1.1.187.9582,"Measurement, Experimentation, Performance",,"The commoditization of hardware, data center economies of scale, and Internet-scale workload growth all demand greater power efficiency to sustain scalability. Traditional enterprise workloads, which are typically memory and I/O bound, have been well served by chip multiprocessors comprising of small, power-efficient cores. Recent advances in mobile computing have led to modern small cores capable of delivering even better power efficiency. While these cores can deliver performance-per-Watt efficiency for data center workloads, small cores impact application quality-of-service robustness, and flexibility as these workloads increasingly invoke computationally intensive kernels. These challenges constitute the price of efficiency, which we quantify for an industry-strength, online web search engine. We evaluate search on server- and mobile-class architectures using Xeon and Atom processors, quantifying search efficiency at the microarchitecture- and system-level."
10.1.1.187.7534,Previous Up Next Article Citations From References: 7 From Reviews: 2,,Divisibility of countable metric spaces. (English summary)
10.1.1.187.7185,Previous Up Next Article Citations From References: 7 From Reviews: 2,,Divisibility of countable metric spaces. (English summary)
10.1.1.187.6602,Practical Large-Scale Optimization for Max-Norm Regularization,,"The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative filtering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable first-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative filtering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas. 1"
10.1.1.187.6466,Summary Report Tables 3,,CONTENTS
10.1.1.187.5964,Summary Report Tables 3,,CONTENTS
10.1.1.187.4658,Language Reference,1997,Chromatic numbers and products. (English summary)
10.1.1.187.388,Contents,,2. Equivariant orthogonal spectra 6 3. Equivariant homotopy groups 10 4. Constructions with equivariant spectra 27
10.1.1.187.3301,Semi-Supervised Bio-Named Entity Recognition with Word-Codebook Learning,,"We describe a novel semi-supervised method called Word-Codebook Learning (WCL), and apply it to the task of bionamed entity recognition (bioNER). Typical bioNER systems can be seen as tasks of assigning labels to words in bioliterature text. To improve supervised tagging, WCL learns a class of word-level feature embeddings to capture word semantic meanings or word label patterns from a large unlabeled corpus. Words are then clustered according to their embedding vectors through a vector quantization step, where each word is assigned into one of the codewords in a codebook. Finally codewords are treated as new word attributes and are added for entity labeling. Two types of wordcodebook learning are proposed: (1) General WCL, where an unsupervised method uses contextual semantic similarity of words to learn accurate word representations; (2) Task-oriented WCL, where for every word a semi-supervised method learns target-class label patterns from unlabeled data using supervised signals from trained bioNER model. Without the need for complex linguistic features, we demonstrate utility of WCL on the BioCreativeII gene name recognition competition data, where WCL yields state-of-the-art performance and shows great improvements over supervised baselines and semi-supervised counter peers."
10.1.1.187.2690,Practical Large-Scale Optimization for Max-Norm Regularization,,"The max-norm was proposed as a convex matrix regularizer in [1] and was shown to be empirically superior to the trace-norm for collaborative filtering problems. Although the max-norm can be computed in polynomial time, there are currently no practical algorithms for solving large-scale optimization problems that incorporate the max-norm. The present work uses a factorization technique of Burer and Monteiro [2] to devise scalable first-order algorithms for convex programs involving the max-norm. These algorithms are applied to solve huge collaborative filtering, graph cut, and clustering problems. Empirically, the new methods outperform mature techniques from all three areas. 1"
10.1.1.186.7557,Aggregates for Constraint Handling Rules,2007,"Abstract. We extend the Constraint Handling Rules language with aggregates such as sum, count, findall, and min. The proposed extension features nested aggregate expressions over guarded conjunctions of constraints, a series of predefined aggregates, and application-tailored user-defined aggregates. We formally define the operational semantics of aggregates, and show how incremental aggregate computation facilitates efficient implementations. Case studies demonstrate that language support for aggregates significantly reduces program size, thus improving readability and maintainability considerably. 1"
10.1.1.186.7519,User grouping behavior in online forums,2009,"Online forums represent one type of social media that is particularly rich for studying human behavior in information seeking and diffusing. The way users join communities is a reflection of the changing and expanding of their interests toward information. In this paper, we study the patterns of user participation behavior, and the feature factors that influence such behavior on different forum datasets. We find that, despite the relative randomness and lesser commitment of structural relationships in online forums, users’ community joining behaviors display some strong regularities. One particularly interesting observation is that the very weak relationships between users defined by online replies have similar diffusion curves as those of real friendships or co-authorships. We build social selection models, Bipartite Markov Random Field (BiMRF), to quantitatively evaluate the prediction performance of those feature factors and their relationships. Using these models, we show that some features carry supplementary information, and the effectiveness of different features vary in different types of forums. Moreover, the results of BiMRF with two-star configurations suggest that the feature of user similarity defined by frequency of communication or number of common friends is inadequate to predict grouping behavior, but adding node-level features can improve the fit of the model."
10.1.1.186.3782,"Manageability, availability and performance in Porcupine: a highly-scalable cluster-based mail service",1999,"This paper describes the motivation, design, and performance of Porcupine, a scalable mail server. The goal of Porcupine is to provide a highly available and scalable electronic mail service using a large cluster of commodity PCs. We designed Porcupine to be easy to manage by emphasizing dynamic load balancing, automatic configuration, and graceful degradation in the presence of failures. Key to the system’s manageability, availability, and performance is that sessions, data, and underlying services are distributed homogeneously and dynamically across nodes in a cluster. 1"
10.1.1.186.2258,"OLAP Visualization: Models, Issues, and Techniques",1439,"The problem of efficiently visualizing multidimensional data sets produced by scientific and statistical tasks/ processes is becoming increasingly challenging, and is attracting the attention of a wide multidisciplinary"
10.1.1.186.1251,How to Play Unique Games on Expanders,,"Abstract. In this paper, we improve a result by Arora, Khot, Kolla, Steurer, Tulsiani, and Vishnoi on solving the Unique Games problem on expanders. Given a (1−ε)-satisfiable instance of Unique Games with the constraint graph G, our algorithm finds an assignment satisfying at least a 1 − Cε/hG fraction of all constraints if ε < cλG where hG is the edge expansion of G, λG is the second smallest eigenvalue of the Laplacian of G, and C and c are some absolute constants. 1"
10.1.1.185.2397,Linear Filtering for Optimized Approach in Satellite Image Enhancement 1,,"Abstract: Problem statement: For decades, several image enhancement techniques have been proposed. Although most techniques require profuse amount of advance and critical steps, the result for the perceive image are not as satisfied. Approach: In this study, we proposed a new method to enhance the satellite image which compares two procedures using two different kinds of filtering technique with an additional step in order to obtain the perceived image. In this new algorithm we first transform the color image into grayscale. The image is then preceded to the edge detection and brightness enhancement step using Laplacian and Sobel technique individually. Results: From the results, the Tenengrad averred that the enhancement result of the dimension and depth in the image were successfully classified. We also evaluate the image quality, adjusting by the PSNR and Tenengrad criterion which indicates that the proposed method shows dramatically increase in pixel distribution throughout the range of RGB. Conclusion: The result of this research is also beneficial in terms of geographical views due to the process which determined the difference appeared on each area. Eventually, this research also performed a comparison for the enhancement step mentioned in this study. Key words: Image enhancement, sharpening, satellite image, edge detecting"
10.1.1.184.6246,SERC Reliability CorporationAMENDED AND RESTATED DELEGATION AGREEMENT BETWEEN NORTH AMERICAN ELECTRIC RELIABILITY CORPORATION AND SERC RELIABILITY CORPORATION AMENDED AND RESTATED DELEGATION AGREEMENT (“Agreement”),2009,"Corporation (“NERC”), an organization certified by the Federal Energy Regulatory Commission (“Commission”) pursuant to Section 215(c) of the Federal Power Act to establish and enforce Reliability Standards for the bulk power system, and SERC Reliability Corporation (“SERC”), an organization established to develop and enforce Reliability Standards within the geographic boundaries identified on Exhibit A to this Agreement, and for other purposes. NERC and SERC may be individually referred to herein as “Party ” or collectively as “Parties.” WITNESSETH WHEREAS, Subtitle A of the Electricity Modernization Act of 2005 added Section 215 to the Federal Power Act (16 U.S.C. § 824n) (hereafter “the Act”) and, among other things, provides for the establishment of an electric reliability organization (“ERO”) to develop and enforce Reliability Standards applicable to all owners, operators, and users of the bulk power system; WHEREAS, the Commission has adopted regulations for the implementation of"
10.1.1.184.2855,A Multimedia Example (Please Click on Icons – Visit:,,Abstract — This page shows some examples of multimedia files. It is
10.1.1.183.9650,A Multimedia Example (Please Click on Icons – Visit:,,Abstract — This page shows some examples of multimedia files. It is
10.1.1.183.9194,Previous Up Next Article Citations From References: 8 From Reviews: 0,,"The Hausdorff dimension of bounded geodesics on geometrically finite manifolds. (English summary) Ergodic Theory Dynam. Systems 17 (1997), no. 1, 227–246. In this interesting paper, the author shows that for a geometrically finite Kleinian group, the critical exponent equals the Hausdorff dimension of the bounded geodesics. To be more precise, suppose G is a discrete group of isometries acting without fixed points on hyperbolic space, DN+1, and let M = DN+1 /G denote the corresponding constant negative curvature manifold. The critical exponent δ of G is defined as the infimum of s such that ∑ g∈G exp(−sd(0, g(0))) converges. Let SN be the boundary of DN+1 and let B ∗ (G)  ⊂ SN × SN be the set of pairs so that the geodesic connecting them in DN+1 projects to a bounded geodesic in M. Let B(G)  ⊂ SN be the set of points so the geodesic ray connecting the origin to the point projects to a bounded ray in M. Then the two main results of the paper are that if G is non-elementary and geometrically finite then:"
10.1.1.183.8410,Previous Up Next Article Citations From References: 12 From Reviews: 2,,"A multifractal formalism for growth rates and applications to geometrically finite Kleinian groups. (English summary) Ergodic Theory Dynam. Systems 24 (2004), no. 1, 141–170. In this paper the authors set up a thermodynamic formalism for a general class of functions defined over a subshift of finite type (X, σ). These functions are given by tuples (∆, A) of certain sequences of continuous real valued functions {∆n} and {An}, where the family of functions ∆n satisfies some growth properties with respect to the family of functions An. They introduce a generalized notion of pressure for such functions for which variational principles can be shown. In the second part of the paper these results are then applied to families ∆ of so called distance functions, which are positive, increasing and pointwise unbounded. They can be used to introduce a metric on X and the notion of Hausdorff dimension dim∆. The authors determine this dimension for the level sets F∆(a)  = {x ∈ X: limn→ ∞ ∆n(x) n = a}. In the third part of the paper the general formalism is applied to derive a multifractal analysis of the limit sets L(G) of geometrically finite Kleinian groups G with possible parabolic fixed points of different ranks. Finally the authors derive necessary and sufficient conditions for the existence of phase transitions in shifts over infinite alphabets."
10.1.1.183.8111,References,,"Constructing restricted Patterson measures for geometrically infinite Kleinian groups. (English summary) Acta Math. Sin. (Engl. Ser.) 22 (2006), no. 2, 431–446. In this paper, the focus of which is on geometrically infinite Kleinian groups, the authors study exhaustions of such groups by specific subsets which have properties reminiscent of geometrically finite groups. Each of these subsets (the authors call them ρ-restrictions) defines a ρ-restricted limit set which is a closed subset of the limit set of the original group and which carries a (ρ-restricted) Patterson-Sullivan measure. This ρ-restricted Patterson-Sullivan measure shows similarities with the classical Patterson-Sullivan measure for geometrically finite Kleinian groups. The exponent of convergence of the ρ-restriction agrees with the Hausdorff dimension of the ρ-restricted limit set, and the Poincaré series of the ρ-restriction diverges at the critical exponent. Furthermore, as ρ → ∞, the exponent of convergence of the ρ-restriction converges to the exponent of convergence of the original group. Reviewed by Petra Bonfert-Taylor"
10.1.1.183.7905,Set Systems and Families of Permutations with Small Traces,,"We study the maximum size of a set system on n elements whose trace on any b elements has size at most k. This question extends to hypergraphs the classical Dirac-type problems from extremal graph theory. We show that if for some b ≥ i ≥ 0 the shatter function fR of a set system ([n], R) satisfies fR(b) < 2 i (b − i + 1) then |R |  = O(n i); this generalizes Sauer’s Lemma on the size of set systems with bounded VC-dimension. We use this bound to delineate the main growth rates for the same problem on families of permutations, where the trace corresponds to the inclusion for permutations. This is related to a question of Raz on families of permutations with bounded VC-dimension that generalizes the Stanley-Wilf conjecture on permutations with excluded patterns. 1"
10.1.1.183.7499,References,,"The Hausdorff dimension of the set of dissipative points for a Cantor-like model set for singly cusped parabolic dynamics. (English summary) Kodai Math. J. 32 (2009), no. 2, 179–196. {A review for this item is in process.}"
10.1.1.183.64,Language Reference,1997,Chromatic numbers and products. (English summary)
10.1.1.183.5765,Set Systems and Families of Permutations with Small Traces,2009, 
10.1.1.183.3710,"Evaluation of  the Active Labor Market Program ""Beautiful Serbia""",2006, 
10.1.1.183.1875,Exploring a two-market genetic algorithm,2002,"The ordinary genetic algorithm may be thought of as conducting a single market in which solutions compete for success, as measured by the fitness funtion. We introduce a two-market genetic algorithm, consisting of two phases, each of which is an ordinary single-market genetic algorithm. The twomarket genetic algorithm has a natural interpretation as a method of solving constrained optimization problems. Phase 1 is optimality improvement; it works on the problem without regard to constraints. Phase 2 is feasibility improvement; it works on the existing population of solutions and drives it towards feasibility. We tested this concept on 14 standard knapsack test problems for genetic algorithms, with excellent results. The paper concludes with discussions of why the twomarket genetic algorithm is successful and of how this work can be extended."
10.1.1.183.167,References,,Representation of ideals of relational structures. (English summary)
10.1.1.182.1598,Previous Up Next Article Citations From References: 7 From Reviews: 2,,Divisibility of countable metric spaces. (English summary)
10.1.1.180.9027,Editor: Paul Hoffman Internet Mail Consortium Enhanced Security Services for S/MIME,1998,draft-ietf-smime-ess-02.txt
10.1.1.180.8177,Editor: Paul Hoffman Internet Mail Consortium Enhanced Security Services for S/MIME,1998,draft-ietf-smime-ess-01.txt
10.1.1.180.7663,Status of this Memo Tags for Identifying Languages,2007,"draft-ietf-ltru-4646bis-06 By submitting this Internet-Draft, each author represents that any applicable patent or other IPR claims of which he or she is aware have been or will be disclosed, and any of which he or she becomes aware will be disclosed, in accordance with Section 6 of BCP 79. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months and may be updated, replaced, or obsoleted by other documents at any time. It is inappropriate to use Internet-Drafts as reference material or to cite them other than as ""work in progress."" The list of current Internet-Drafts can be accessed at"
10.1.1.180.2178,the American Legislative Process Summary,2008,"The Library of Congress, as its name suggests, is a library dedicated to serving the United States Congress and its Members. It serves additionally as an unexcelled national library. The Library was located in the Capitol Building with the House of Representatives and the Senate until 1897, and its collections always have been available for use by Congress. Building upon a concept developed by the New York State Library and then the Wisconsin legislative reference department, Wisconsin’s Senator Robert LaFollette and Representative John M. Nelson led an effort to direct the establishment of a special reference unit within the Library in 1914. Later known as the Legislative Reference Service, it was charged with responding to congressional requests for information. For more than 50 years, this department assisted Congress primarily by providing facts and publications and by transmitting research and analysis done largely by other government agencies, private organizations, and individual scholars. In 1970, Congress enacted a law transforming the Legislative Reference Service into the Congressional Research Service (CRS) and directing CRS to devote more of its efforts and increased resources to performing research and analysis that assists Congress in direct support of the legislative process. Joined today by two other congressional support agencies, including the"
10.1.1.180.1847,License GPL-2 Repository CRAN,2010,"978-0-387-77316-2. (See the vignette for a package overview.) LazyLoad yes Depends R (> = 2.5.0), stats, car (> = 2.0-1), Formula (> = 0.2-0), lmtest, sandwich, strucchange, survival, zoo"
10.1.1.180.1751,Title of Each Class,2009,Commission file number: 000-51788
10.1.1.180.1060,Director,2010,"IES evaluation reports present objective information on the conditions of implementation and impacts of the programs being evaluated. IES evaluation reports do not include conclusions or recommendations or views with regard to actions policymakers or practitioners should take in light of the findings in the reports. This report is in the public domain. Authorization to reproduce it in whole or in part is granted. While permission to reprint this publication is not necessary, the citation should be: Gleason, P., Clark, M.,"
10.1.1.18.487,Mapping Image Restoration to a Graph Problem,1999,"We propose a graph optimization method for the restoration of gray-scale images. We consider an arbitrary noise model for each pixel location. We also consider a smooth constraint where the potentials between neighbor pixels are convex functionals. We show how to map this problem to a directed flow graph. Then, a global optimal solution is obtained via the use of the maximumflow algorithm. The algorithm runs in a polynomial time with respect to the size of the image.  1. Introduction  In image restoration, a ""true"" image is corrupted by noise and the goal is to recover the ""true"" image from the noisy one. The modeling needs to remove the noise without removing the intensity discontinuities of an image, i.e., one can not simply remove the high frequency component of the image signal. In the segmentation problem, one seek a map from the set of pixels to a small set of levels such that each connected component of the set of pixels with the same level forms a relatively large and ""meaningf..."
10.1.1.18.2328,Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications,2001,Efficiently determining the node that stores a data item in a distributed network is an important and challenging problem. This paper describes the motivation and design...
10.1.1.18.1665,Code Compression,1997,"Current research in compiler optimization counts mainly CPU time and perhaps the first cache level or two. This view has been important but is becoming myopic, at least from a system-wide viewpoint, as the ratio of network and disk speeds to CPU speeds grows exponentially.  For example, we have seen the CPU idle for most of the time during paging, so compressing pages can increase total performance even though the CPU must decompress or interpret the page contents. Another profile shows that many functions are called just once, so reduced paging could pay for their interpretation overhead.  This paper describes:  . Measurements that show how code compression can save space and total time in some important real-world scenarios.  . A compressed executable representation that is roughly the same size as gzipped x86 programs and can be interpreted without decompression. It can also be compiled to high-quality machine code at 2.5 megabytes per second on a 120MHz Pentium processor.  . A comp..."
10.1.1.178.9490,References,,"Holomorphic principal bundles over a compact Kähler manifold. (English, French summaries) C. R. Acad. Sci. Paris Sér. I Math. 330 (2000), no. 2, 109–114. Let G be a connected reductive algebraic group over C. Analogues of a theorem of Uhlenbeck and Yau on Hermite-Einstein metrics on polystable vector bundles and of a reduction theorem of Harder and Narasimhan are given for any principal G-bundle EG over a compact Kähler manifold. The authors ’ method is to investigate links between stability and connection properties of EG and those of ad(EG). Reviewed by Christophe Mourougane"
10.1.1.178.9381,and all its “references,1981,On the principal bundles over a flag manifold. II. (English summary)
10.1.1.178.7131,Tags for Identifying Languages,2005,draft-ietf-ltru-registry-05
10.1.1.178.4839,Reviewed by Mihyun Kang References,,Connectivity of addable graph classes. (English summary)
10.1.1.178.4377,Previous Up Next Article Citations From References: 2 From Reviews: 1,,Weighting operator patterns of Pritchard-Salamon realizations. (English summary)
10.1.1.178.4340,385–395; MR1476320 (99a:52002)]. An overview of other related results is also given.,,Slicing convex sets and measures by a hyperplane. (English summary)
10.1.1.178.2162,"Item 3.4 of the provisional agenda* COMPILATION OF VIEWS ON THE POTENTIAL ENVIRONMENTAL, CULTURAL AND SOCIO-ECONOMIC IMPACTS OF GENETICALLY MODIFIED TREES",2008,"Secretary to collect and collate existing information, including peer-reviewed published literature, in order to allow the Subsidiary Body on Scientific, Technical and Technological Advice (SBSTTA) to consider and assess the potential environmental, cultural, and socio-economic impacts of genetically modified trees on the conservation and sustainable use of forest biological diversity, and to report to the ninth meeting of the Conference of the Parties. In order to facilitate the collation of information on the potential environmental, cultural and socio-economic impacts of genetically modified trees, the Secretariat distributed, through notification 2006-027 of 4 May 2006, a questionnaire to Parties and relevant organizations inviting them to provide information. The views were compiled by the Secretariat, and circulated as document UNEP/CBD/SBSTTA/13/INF/7. 2. The Subsidiary Body, at its thirteenth meeting, requested the Executive Secretary to forward documents UNEP/CBD/SBSTTA/13/INF/6 and INF/7, recognizing that SBSTTA has not assessed them in detail, to ninth meeting of the Conference of the Parties for information. 3. Accordingly, the Executive Secretary is circulating herewith, for the information of participants of the ninth meeting of the Conference of the Parties, a compilation of views received in response to the questionnaire on the potential environmental, cultural and socio-economic impacts of genetically"
10.1.1.178.1853,References,,Representation of ideals of relational structures. (English summary)
10.1.1.177.8464,INTEGER,,"Note: before using this routine, please read the Users ’ Note for your implementation to check for implementation-dependent details. You are advised to enclose any calls to NAG Parallel Library routines between calls to Z01AAFP and Z01ABFP. 1 Description F01WAFP gathers an m by n real distributed matrix As to a user specified logical processor (the destination processor) and stores it in the natural (non-distributed) format. The matrix As can be considered as a submatrix of a larger mA by nA distributed matrix A, i.e., As(1: m, 1:n)  ≡ A(iA: iA + m − 1,jA: jA + n − 1). Note: if i = j =1,m = mA and n = nA, thenAs = A. It is assumed that the matrix A has been distributed on a logical grid of processors in the cyclic twodimensional block format. However, only the elements of the submatrix As are referenced by this routine. It is also possible to gather copies of the matrix As on processors which are either on a particular row or column of the processor grid. Alternatively, all processors on the Library Grid can receive a copy of As. This routine is useful for gathering full or partial solutions which have been computed using (ScaLAPACK)"
10.1.1.177.8388,INTEGER,,"Note: before using this routine, please read the Users ’ Note for your implementation to check for implementation-dependent details. You are advised to enclose any calls to NAG Parallel Library routines between calls to Z01AAFP and Z01ABFP. 1 Description F01WGFP gathers an m by n complex distributed matrix As to a user specified logical processor (the destination processor) and stores it in the natural (non-distributed) format. The matrix As can be considered as a submatrix of a larger mA by nA distributed matrix A, i.e., As(1: m, 1:n)  ≡ A(iA: iA + m − 1,jA: jA + n − 1). Note: if i = j =1,m = mA and n = nA, thenAs = A. It is assumed that the matrix A has been distributed on a logical grid of processors in the cyclic twodimensional block format. However, only the elements of the submatrix As are referenced by this routine. It is also possible to gather copies of the matrix As on processors which are either on a particular row or column of the processor grid. Alternatively, all processors on the grid can receive a copy of As. This routine is useful for gathering full or partial solutions which have been computed using (ScaLAPACK)"
10.1.1.177.641,Language Reference,1997,Vertex covers by edge disjoint cliques. (English summary)
10.1.1.176.4946,Linear Filtering for Optimized Approach in Satellite Image Enhancement 1,,"Abstract: Problem statement: For decades, several image enhancement techniques have been proposed. Although most techniques require profuse amount of advance and critical steps, the result for the perceive image are not as satisfied. Approach: In this study, we proposed a new method to enhance the satellite image which compares two procedures using two different kinds of filtering technique with an additional step in order to obtain the perceived image. In this new algorithm we first transform the color image into grayscale. The image is then preceded to the edge detection and brightness enhancement step using Laplacian and Sobel technique individually. Results: From the results, the Tenengrad averred that the enhancement result of the dimension and depth in the image were successfully classified. We also evaluate the image quality, adjusting by the PSNR and Tenengrad criterion which indicates that the proposed method shows dramatically increase in pixel distribution throughout the range of RGB. Conclusion: The result of this research is also beneficial in terms of geographical views due to the process which determined the difference appeared on each area. Eventually, this research also performed a comparison for the enhancement step mentioned in this study. Key words: Image enhancement, sharpening, satellite image, edge detecting"
10.1.1.175.9170,Technical report HW-MACS-TR-0079 A constraint system for a SML type error slicer,,"Existing compilers for many languages have confusing type error messages. Type error slicing (TES) helps the programmer by isolating the part of a program contributing to a type error, but unfortunately TES was initially done for a tiny toy language. Extending TES to a full programming language is extremely challenging, and for SML we needed a number of innovations and generalisations. Some issues would be faced for any language, and some are SMLspecific but representative of the complexity of language-specific issues likely to be faced for other languages. We solve both kinds of issues and present a simple, general constraint system for providing type error slices for ill-typed programs. Our constraint system elegantly and efficiently handles features like the intricate open SML feature. We show how the simple clarity of type error slices can demystify language features known to confuse users. We also provide in an appendix a case study on how to use our TES to help modifying user data types, and extend the core language presented in the main body of this report to handle more of the implementation of our system. These extensions allow handling local declarations, type declarations and some uses of signatures. 1."
10.1.1.175.7787,Technical report HW-MACS-TR-0079 A constraint system for a SML type error slicer,,"Existing compilers for many languages have confusing type error messages. Type error slicing (TES) helps the programmer by isolating the part of a program contributing to a type error, but unfortunately TES was initially done for a tiny toy language. Extending TES to a full programming language is extremely challenging, and for SML we needed a number of innovations and generalisations. Some issues would be faced for any language, and some are SMLspecific but representative of the complexity of language-specific issues likely to be faced for other languages. We solve both kinds of issues and present a simple, general constraint system for providing type error slices for ill-typed programs. Our constraint system elegantly and efficiently handles features like the intricate open SML feature. We show how the simple clarity of type error slices can demystify language features known to confuse users. We also provide in an appendix a case study on how to use our TES to help modifying user data types, and extend the core language presented in the main body of this report to handle more of the implementation of our system. These extensions allow handling local declarations, type declarations and some uses of signatures. 1."
10.1.1.175.3795,Real Wage Inequality,2008,"Abstract. A large literature has documented a significant increase in the difference between the wage of college graduates and high school graduates over the past 30 years. I show that from 1980 to 2000, college graduates have experienced relatively larger increases in cost of living, because they have increasingly concentrated in metropolitan areas that are characterized by a high cost of housing. When I deflate nominal wages using a location-specific CPI, I find that the difference between the wage of college graduates and high school graduates is lower in real terms than in nominal terms and has grown less. At least 22 % of the documented increase in college premium is accounted for by spatial differences in the cost of living. The implications of this finding for changes in well-being inequality depend on why college graduates sort into expensive cities. Using a simple general equilibrium model of the labor and housing markets, I consider two alternative explanations. First, it is possible that the relative supply of college graduates increases in expensive cities because college graduates are increasingly attracted by amenities located in those cities. In this case, the higher cost of housing reflects consumption of desirable local amenities, and there may still be a significant increase in well-being inequality even if the increase in real wage inequality is limited. Alternatively, it is possible that the relative demand for college graduates increases in expensive cities due to shifts in the relative productivity of skilled labor. In this case, the relative increase in skilled workers ’ standard of living is offset by the higher cost of living. The evidence indicates that changes in the geographical location of different skill groups are mostly driven by changes in their relative demand. I conclude that the increase in well-being disparities between"
10.1.1.174.9094,"October 18th, 2010Using a Free Theorem [Wadler 1989] For every",,"we have for arbitrary f and l, where get:: [α]  → [α] map f (get l)  = get (map f l) map:: (α → β)  → [α]  → [β] map f [ ]  =  [] map f (a: as)  = (f a) : (map f as) 1 Using a Free Theorem [Wadler 1989] For every we have for arbitrary f and l, where But how do we know this? get:: [α]  → [α] map f (get l)  = get (map f l) map:: (α → β)  → [α]  → [β] map f [ ]  =  [] map f (a: as)  = (f a) : (map f as)"
10.1.1.174.1952,RELB−RUPA,,Figure 1: Reconstructing two marker positions on the right arm from frame 100 to 500 of a walking motion (#132.43). Graphs show bone lengths over time; markers are stills at frame 241. LDS/DynaMMo (textbfmiddle) fails to preserve inter-marker
10.1.1.173.8454,e-Business Applications,2006,ebXML for e-Business Applications / Services Web et ebXML: une évaluation des services Web et de ebXML pour les applications de commerce électronique
10.1.1.172.9312,Obsoletes: 2253 Lightweight Directory Access Protocol (v3): UTF-8 String Representation of Distinguished Names,2001,This document is an Internet-Draft and is in full conformance with all
10.1.1.172.9216,Obsoletes: 2253 Lightweight Directory Access Protocol (v3): UTF-8 String Representation of Distinguished Names,2001,This document is an Internet-Draft and is in full conformance with all
10.1.1.172.5544," Algorithm, Implementation and Application of the SIM-DL Similarity Server",2007," Semantic similarity measurement gained attention as a methodology for ontology-based information retrieval within GIScience over the last years. Several theories explain how to determine the similarity between entities, concepts or spatial scenes, while concrete implementations and applications are still missing. In addition, most existing similarity theories use their own representation language while the majority of geoontologies is annotated using the Web Ontology Language (OWL). This paper presents a context and blocking aware semantic similarity theory for the description logic ALCHQ as well as its prototypical implementation within the open source SIM-DL similarity server. An application scenario is introduced showing how the Alexandria Digital Library Gazetteer can benefit from similarity in terms of improved search and annotation capabilities. Directions for further work are discussed.  "
10.1.1.172.5408,Previous Up Next Article Citations From References: 8 From Reviews: 2,,Divisibility of countable metric spaces. (English summary)
10.1.1.172.2644,References,,A stability property of the octahedron and the icosahedron. (English summary)
10.1.1.170.8142,Previous Up Next Article Citations From References: 0 From Reviews: 0,,The distribution of the root degree of a random permutation. (English summary)
10.1.1.170.8062,Reviewed by Mihyun Kang References,,Connectivity of addable graph classes. (English summary)
10.1.1.170.7768,Language Reference,1997,Vertex covers by edge disjoint cliques. (English summary)
10.1.1.170.7249,and all its “references,1981,On the principal bundles over a flag manifold. II. (English summary)
10.1.1.170.7211,References,,"Holomorphic principal bundles over a compact Kähler manifold. (English, French summaries) C. R. Acad. Sci. Paris Sér. I Math. 330 (2000), no. 2, 109–114. Let G be a connected reductive algebraic group over C. Analogues of a theorem of Uhlenbeck and Yau on Hermite-Einstein metrics on polystable vector bundles and of a reduction theorem of Harder and Narasimhan are given for any principal G-bundle EG over a compact Kähler manifold. The authors ’ method is to investigate links between stability and connection properties of EG and those of ad(EG). Reviewed by Christophe Mourougane"
10.1.1.170.6488,Documentation Check List,2005,The revision starts after page 4 of this document
10.1.1.170.5296,EFFICIENT BLIND SEARCH: OPTIMAL POWER OF DETECTION UNDER COMPUTATIONAL COST CONSTRAINTS,,"Some astronomy projects require a blind search through a vast number of hypotheses to detect objects of interest. The number of hypotheses to test can be in the billions. A naive blind search over every single hypothesis would be far too costly computationally. We propose a hierarchical scheme for blind search, using various “resolution ” levels. At lower resolution levels, “regions” of interest in the search space are singled out with a low computational cost. These regions are refined at intermediate resolution levels and only the most promising candidates are finally tested at the original fine resolution. The optimal search strategy is found by dynamic programming. We demonstrate the procedure for pulsar search from satellite gamma-ray observations and show that the power of the naive blind search can almost be matched with the hierarchical scheme while reducing the computational burden by more than three orders of magnitude. 1. Introduction. “What"
10.1.1.170.3227,Director,2010,"IES evaluation reports present objective information on the conditions of implementation and impacts of the programs being evaluated. IES evaluation reports do not include conclusions or recommendations or views with regard to actions policymakers or practitioners should take in light of the findings in the reports. This report is in the public domain. Authorization to reproduce it in whole or in part is granted. While permission to reprint this publication is not necessary, the citation should be: Gleason, P., Clark, M.,"
10.1.1.17.9518,Knowledge-level Reflection,1992,"This paper presents an overview of the REFLECT project. It defines the notion of knowledge level reflection that has been central to the project, it compares this notion with existing approaches to reflection in related fields, and investigates some of the consequences of the concept of knowledge level reflection: what is a general architecture for knowledge level reflection, how to model the object component in such an architecture, what is the nature of reflective theories, how can we design such architectures, and what are the results of our actual experiments with such systems?"
10.1.1.169.9186,Aggregates for Constraint Handling Rules,2007,"Abstract. We extend the Constraint Handling Rules language with aggregates such as sum, count, findall, and min. The proposed extension features nested aggregate expressions over guarded conjunctions of constraints, a series of predefined aggregates, and application-tailored user-defined aggregates. We formally define the operational semantics of aggregates, and show how incremental aggregate computation facilitates efficient implementations. Case studies demonstrate that language support for aggregates significantly reduces program size, thus improving readability and maintainability considerably. 1"
10.1.1.169.8566,How to Play Unique Games on Expanders,,"Abstract. In this paper, we improve a result by Arora, Khot, Kolla, Steurer, Tulsiani, and Vishnoi on solving the Unique Games problem on expanders. Given a (1−ε)-satisfiable instance of Unique Games with the constraint graph G, our algorithm finds an assignment satisfying at least a 1 − Cε/hG fraction of all constraints if ε < cλG where hG is the edge expansion of G, λG is the second smallest eigenvalue of the Laplacian of G, and C and c are some absolute constants. 1"
10.1.1.169.6422,References,,A stability property of the octahedron and the icosahedron. (English summary)
10.1.1.169.3532,Columbia U./Packet Design/Entera/Packet Design STATUS OF THIS MEMO RTP: A Transport Protocol for Real-Time Applications,2000,"This document is an Internet-Draft and is in full conformance with all provisions of Section 10 of RFC2026. Internet-Drafts are working documents of the Internet Engineering Task Force (IETF), its areas, and its working groups. Note that other groups may also distribute working documents as Internet-Drafts. Internet-Drafts are draft documents valid for a maximum of six months and may be updated, replaced, or obsoleted by other documents at any time. It is inappropriate to use Internet-Drafts as reference material or to cite them other than as ""work in progress"". The list of current Internet-Drafts can be accessed at"
10.1.1.169.3189,Previous Up Next Article Citations From References: 0 From Reviews: 0,,The distribution of the root degree of a random permutation. (English summary)
10.1.1.169.2669,Previous Up Next Article Citations From References: 2 From Reviews: 1,,Weighting operator patterns of Pritchard-Salamon realizations. (English summary)
10.1.1.169.2573,385–395; MR1476320 (99a:52002)]. An overview of other related results is also given.,,Slicing convex sets and measures by a hyperplane. (English summary)
10.1.1.169.2388,On Leighton’s graph covering theorem,,"Abstract. We give short expositions of both Leighton’s proof and the Bass-Kulkarni proof of Leighton’s graph covering theorem, in the context of colored graphs. We discuss a further generalization, needed elsewhere, to “symmetryrestricted graphs. ” We can prove it in some cases, for example, if the “graph of colors ” is a tree, but we do not know if it is true in general. We show that Bass’s Conjugation Theorem, which is a tool in the Bass-Kulkarni approach, does hold in the symmetry-restricted context. Leighton’s graph covering theorem says: Theorem (Leighton [5]). Two finite graphs which have a common covering have a common finite covering. It answered a conjecture of Angluin and Gardiner who had proved the case that both graphs are k–regular [1]. Leighton’s proof is short (two pages), but has been considered by some to lack transparency. It was reframed in terms of Bass-Serre theory by Bass and Kulkarni [2, 3], expanding its length considerably but providing group-theoretic tools which have other uses."
10.1.1.168.8,Title of Each Class,2009,Commission file number: 000-51788
10.1.1.168.6478,"April 22nd, 2009Using a Free Theorem [Wadler 1989]",,"For every we have for arbitrary f and l, where get:: [α]  → [α] map f (get l)  = get (map f l) map:: (α → β)  → [α]  → [β] map f []  = [] map f (a: as)  = (f a) : (map f as) 1 Using a Free Theorem [Wadler 1989] For every we have for arbitrary f and l, where But how do we know this? get:: [α]  → [α] map f (get l)  = get (map f l) map:: (α → β)  → [α]  → [β] map f []  = [] map f (a: as)  = (f a) : (map f as)"
10.1.1.168.1030,Alternate Affy Gene Expression Summary Methods.................... 3,2010,R topics documented:
10.1.1.167.7301,Real Wage Inequality,2008,"Abstract. A large literature has documented a significant increase in the difference between the wage of college graduates and high school graduates over the past 30 years. I show that from 1980 to 2000, college graduates have experienced relatively larger increases in cost of living, because they have increasingly concentrated in metropolitan areas that are characterized by a high cost of housing. When I deflate nominal wages using a location-specific CPI, I find that the difference between the wage of college graduates and high school graduates is lower in real terms than in nominal terms and has grown less. At least 22 % of the documented increase in college premium is accounted for by spatial differences in the cost of living. The implications of this finding for changes in well-being inequality depend on why college graduates sort into expensive cities. Using a simple general equilibrium model of the labor and housing markets, I consider two alternative explanations. First, it is possible that the relative supply of college graduates increases in expensive cities because college graduates are increasingly attracted by amenities located in those cities. In this case, the higher cost of housing reflects consumption of desirable local amenities, and there may still be a significant increase in well-being inequality even if the increase in real wage inequality is limited. Alternatively, it is possible that the relative demand for college graduates increases in"
10.1.1.167.215,RELB−RUPA,,Figure 1: Reconstructing two marker positions on the right arm from frame 100 to 500 of a walking motion (#132.43). Graphs show bone lengths over time; markers are stills at frame 241. LDS/DynaMMo (textbfmiddle) fails to preserve inter-marker
10.1.1.167.1514,Previous Up Next Article Citations From References: 8 From Reviews: 0,,"The Hausdorff dimension of bounded geodesics on geometrically finite manifolds. (English summary) Ergodic Theory Dynam. Systems 17 (1997), no. 1, 227–246. In this interesting paper, the author shows that for a geometrically finite Kleinian group, the critical exponent equals the Hausdorff dimension of the bounded geodesics. To be more precise, suppose G is a discrete group of isometries acting without fixed points on hyperbolic space, DN+1, and let M = DN+1 /G denote the corresponding constant negative curvature manifold. The critical exponent δ of G is defined as the infimum of s such that ∑ g∈G exp(−sd(0, g(0))) converges. Let SN be the boundary of DN+1 and let B ∗ (G)  ⊂ SN × SN be the set of pairs so that the geodesic connecting them in DN+1 projects to a bounded geodesic in M. Let B(G)  ⊂ SN be the set of points so the geodesic ray connecting the origin to the point projects to a bounded ray in M. Then the two main results of the paper are that if G is non-elementary and geometrically finite then:"
10.1.1.165.9793,Strong hydrodynamic limit for attractive particle systems on Z,2010, 
10.1.1.165.8237,Hard-To-Use Interfaces Considered Beneficial (Some of the Time),,Copyright is held by the author/owner(s).
10.1.1.165.6135,Differential complexes and numerical stability,2002,"Differential complexes such as the de Rham complex have recently come to play an important role in the design and analysis of numerical methods for partial differential equations. The design of stable discretizations of systems of partial differential equations often hinges on capturing subtle aspects of the structure of the system in the discretization. In many cases the differential geometric structure captured by a differential complex has proven to be a key element, and a discrete differential complex which is appropriately related to the original complex is essential. This new geometric viewpoint has provided a unifying understanding of a variety of innovative numerical methods developed over recent decades and pointed the way to stable discretizations of problems for which none were previously known, and it appears likely to play an important role in attacking some currently intractable problems in numerical PDE."
10.1.1.165.4526,the American Legislative Process Summary,2008,"The Library of Congress, as its name suggests, is a library dedicated to serving the United States Congress and its Members. It serves additionally as an unexcelled national library. The Library was located in the Capitol Building with the House of Representatives and the Senate until 1897, and its collections always have been available for use by Congress. Building upon a concept developed by the New York State Library and then the Wisconsin legislative reference department, Wisconsin’s Senator Robert LaFollette and Representative John M. Nelson led an effort to direct the establishment of a special reference unit within the Library in 1914. Later known as the Legislative Reference Service, it was charged with responding to congressional requests for information. For more than 50 years, this department assisted Congress primarily by providing facts and publications and by transmitting research and analysis done largely by other government agencies, private organizations, and individual scholars. In 1970, Congress enacted a law transforming the Legislative Reference Service into the Congressional Research Service (CRS) and directing CRS to devote more of its efforts and increased resources to performing research and analysis that assists Congress in direct support of the legislative process. Joined today by two other congressional support agencies, including the"
10.1.1.165.1151,"Mobility Helps Peer-to-Peer Security Srdjan Capkun, Member, IEEE Computer Society,",,"Abstract—We propose a straightforward technique to provide peer-to-peer security in mobile networks. We show that far from being a hurdle, mobility can be exploited to set up security associations among users. We leverage on the temporary vicinity of users, during which appropriate cryptographic protocols are run. We illustrate the operation of the solution in two scenarios, both in the framework of mobile ad hoc networks. In the first scenario, we assume the presence of an offline certification authority and we show how mobility helps to set up security associations for secure routing; in this case, the security protocol runs over one-hop radio links. We further show that mobility can be used for the periodic renewal of vital security information (e.g., the distribution of hash chain/Merkle tree roots). In the second scenario, we consider fully self-organized security: Users authenticate each other by visual contact and by the activation of an appropriate secure side channel of their personal device; we show that the process can be fuelled by taking advantage of trusted acquaintances. We then show that the proposed solution is generic: It can be deployed on any mobile network and it can be implemented either with symmetric or with asymmetric cryptography. We provide a performance analysis by studying the behavior of the solution in various scenarios. Index Terms—Mobile ad hoc networks, network-level security and protection. æ 1"
10.1.1.164.3234,Lesion in a basis function model of spatial representations: Comparison with hemineglect,1996,"The basis function theory of spatial representations explains how neurons i n the parietal cortex can perform nonlinear transformations from sensory to motor coordinates. The authors present computer simulations showing that unilateral parietal lesions leading to a neuronal gradient in basis function maps can account for the behavior of patients with hemineglect, including (a) neglect in line cancellation and line bisection experiments; (b) neglect in multiple frames of reference simultaneously; (c) relative neglect, a form of what is sometime called object-centered neglect; and (d) neglect without optic ataxia. Contralateral neglect arises in the model because the lesion produces an imbalance in the salience of stimuli that is modulated by the orientation of the body in space. These results strongly support the basis function theory for spatial representations in humans and provide a computational model of hemineglect at the single-cell level. A unilateral lesion of the parieto-occipital cortex in humans often produces hemineglect (Heilman, Watson,  & Valenstein, 1985; Pouget & Driver, 1999; Vallar, 1998), a neurologic syndrome characterized by a conspicuous inability to react or respond to stimuli presented in the hemispace contralateral to the lesion. For example, when asked to"
10.1.1.164.2486,MODULAR CURVES AND RAMANUJAN’S CONTINUED FRACTION,,"Abstract. We use arithmetic models of modular curves to establish some properties of Ramanujan’s continued fraction. In particular, we give a new geometric proof that its singular values are algebraic units that generate specific abelian extensions of imaginary quadratic fields, and we use a mixture of geometric and analytic methods to construct and study an infinite family of two-variable polynomials over Z that are related to Ramanujan’s function in the same way that the classical modular polynomials are related to the classical j-function. We also prove that a singular value on the imaginary axis, necessarily real, lies in a radical tower in R only if all odd prime factors of its degree over Q are Fermat primes; by computing some ray class groups, we give many examples where this necessary condition is not satisfied. 1."
10.1.1.164.217,"Actas do Encontro Científico 3º Festival Nacional de Robótica- ROBOTICA2003 Lisboa, 9 de Maio de 2003. MARKOV LOCALIZATION IN THE ROBOCUP SIMULATION LEAGUE 1",,"Abstract: For mobile robots, localization is the process of updating the pose of a robot, given information about its environment and the history of its sensor readings. This paper describes an implementation of the Markov localization method using a probability distribution across a fine-grained grid of robot poses to globally localize a robot even in the presence of noise. In particular, we applied this technique to self-localize a soccer player in the RoboCup Simulation League. This simulation features a highly dynamic environment and inaccurate sensor readings similar to real-world situations. We provide an experimental analysis of this implementation and show results indicating that the robot is able to remain relatively well localized in terms of position."
10.1.1.163.6762,"Effect of genre, speaker, and word class on the realization of given and new information",2006,"There is much evidence in the literature that speakers tend to deaccent discourse-given entities, while accenting new ones. However, speakers do not always follow this simple strategy and the causes for such variation are not yet well understood. In this paper, we describe several new forms of variability in the relationship between given/new information and accenting behavior, variation due to individual differences and to word class. We present results indicating that different speakers have different strategies for making new words prominent. We analyze two word-classes – nouns and verbs – in a corpus of spontaneous and read direction-giving monologues, and show that speakers use different combinations of pitch, intensity and inter-word pauses to distinguish between given and new information. Most interestingly, we find that in both genres all speakers tend to produce given verbs with higher intensity than new verbs. Index Terms: prosody, information status, given/new information, accenting."
10.1.1.163.4923,"Zarki, “MPEGTool: An X window based MPEG encoder and statistical tool",1993,"In this paper, we describe MPEGTool, an X window based tool which can be used to generate an MPEG 2 encoded bit stream for video sequences and to study the statistical properties of the encoded data. It is a very versatile tool that was designed to study the characteristics of variable bit rate video sources for transmission over ATM 3 based BISDN 4. The tool, which has a window based graphical user interface, allows a user to specify several of the MPEG parameters such as the intraframe to interframe ratio, and the quantizer scale. The tool also includes a statistical package which allows the user to plot graphs of various statistics including bit distributions, ATM cell distributions, time series, autocorrelation functions and cell interarrival times."
10.1.1.163.3290,Graph kernels versus graph representations: a case study in parse ranking,,"Abstract. Recently, several kernel functions designed for a data that consists of graphs have been presented. In this paper, we concentrate on designing graph representations and adapting the kernels for these graphs. In particular, we propose graph representations for dependency parses and analyse the applicability of several variations of the graph kernels for the problem of parse ranking in the domain of biomedical texts. The parses used in the study are generated with the link grammar (LG) parser from annotated sentences of BioInfer corpus. The results indicate that designing the graph representation is as important as designing the kernel function that is used as the similarity measure of the graphs. 1"
10.1.1.163.1678,Bichromatic separability with two boxes: a general approach,2009,"MEC MTM2006-03909. Let S be a set of n points on the plane in general position such that its elements are colored red or blue. We study the following problem: Find a largest subset of S which can be enclosed by the union of two, not necessarily disjoint, axis-aligned rectangles R and B such that R (resp. B) contains only red (resp. blue) points. We prove that this problem can be solved in O(n 2 log n) time and O(n) space. Our approach is based on solving some instances of Bentley’s maximum-sum consecutive subsequence problem. We introduce the first known data structure to dynamically maintain the optimal solution of this problem. We show that our techniques can be used to efficiently solve a more general class of problems in data analysis."
10.1.1.163.1517,Integer Programming Model for Automated Structure-based NMR Assignment,,"a protein sequence, and its NMR spectra, automatically interpret the NMR spectra and do backbone resonance assignment. We then propose a solution to solve this problem. The core of the solution is a novel integer linear programming model, which is a general framework for many versions of the structure-based assignment problem. As a proof of concept, our system has generated an automatic assignment on a real protein TM1112 with 91 % recall and 99 % precision, starting from scratch. When we restrict ourselves to the special case where perfect peak lists are given, we are able to compare our results with existing results in the field. In particular, we reduced the assignment error of Xiong-Pandurangan-Bailey-Kellogg’s method by 5 folds on average, with over a thousand fold speed up. Our system also achieves 91 % assignment accuracy on real experimental data for Ubiquitin. These results have direct practical implications. For example, in the protein design process, a protein is modified slightly and its structure is again measured by NMR experiments. Our method automates this process, saving time on tedious peak-picking and resonance assignment. As another example, when there is a homologous protein with known structure, our method increases the assignment accuracy and hence enables automated NMR structure determination. ⋆ The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors. The NMR resonance assignment problem has been extensively studied for twenty years [1–19]. Traditional"
10.1.1.163.1462,Towards Automated Structure-based NMR Assignment,,"a protein sequence, and its NMR spectra, automatically interpret the NMR spectra and do backbone resonance assignment. We then propose a solution to solve this problem. The core of the solution is a novel integer linear programming model, which is a general framework for many versions of the structure-based assignment problem. As a proof of concept, our system has generated an automatic assignment on a real protein TM1112 with 91 % recall and 99 % precision, starting from scratch. When we restrict ourselves to the special case where perfect peak lists are given, we are able to compare our results with existing results in the field. In particular, we reduced the assignment error of Xiong-Pandurangan-Bailey-Kellogg’s method by 5 folds on average, with over a thousand fold speed up. Our system also achieves 91 % assignment accuracy on real experimental data for Ubiquitin. These results have direct practical implications. For example, in the protein design process, a protein is modified slightly and its structure is again measured by NMR experiments. Our method automates this process, saving time on tedious peak-picking and resonance assignment. As another example, when there is a homologous protein with known structure, our method increases the assignment accuracy and hence enables automated NMR structure determination. ⋆ The authors wish it to be known that, in their opinion, the first two authors should be regarded as Joint First Authors. The NMR resonance assignment problem has been extensively studied for twenty years [1–19]. Traditional"
10.1.1.161.533,Learning a Model of a Web User’s Interests,1995,"Abstract. There are many recommender systems that are designed to help users find relevant information on the web. To produce recommendations that are relevant to an individual user, many of these systems first attempt to learn a model of the user’s browsing behavior. This paper presents a novel method for learning such a model from a set of annotated web logs — i.e., web logs that are augmented with the user’s assessment of whether each webpage is an information content (IC) page (i.e., contains the information required to complete her task). Our systems use this to learn what properties of a webpage, within a sequence, identify such IC-pages, and similarly what ”browsing properties ” characterize the words on such pages (“IC-words”). As these methods deal with properties of webpages (or of words), rather than specific URLs (words), they can be used anywhere throughout the web; i.e., they are not specific to a particular website, or a particular task. This paper also describes the enhanced browser, AIE, that we designed and implemented for collecting these annotated web logs, and an empirical study we conducted to investigate the effectiveness of our approach. This empirical evidence shows that our approach, and our algorithms, work effectively. 1"
10.1.1.161.1824,0.1 ls.mixed: Mixed effects Linear Regression Use,,"multi-level linear regression if you have covariates that are grouped according to one or more classification factors and a continuous dependent variable. While generally called multi-level models in the social sciences, this class of models is often referred to as mixed-effects models in the statistics literature and as hierarchical models in a Bayesian setting. This general class of models consists of linear models that are expressed as a function of both fixed effects, parameters corresponding to an entire population or certain repeatable levels of experimental factors, and random effects, parameters corresponding to individual experimental units drawn at random from a population."
10.1.1.161.1616,License GPL-2 Repository CRAN,2009,978-0-387-77316-2. (See the vignette for a package overview.) LazyLoad yes
10.1.1.160.7677,3G and 3.5G Wireless Network Performance Measured from Moving Cars and High-Speed Trains,,"In recent years, the world has witnessed the deployment of several 3G and 3.5G wireless networks based on technologies such as CDMA 1x EVolution Data-Only (EVDO), High-Speed Downlink Packet Access (HSDPA), and mobile WiMax (e.g., WiBro). Although 3G and 3.5G wireless networks support enough bandwidth for typical Internet applications, their performance varies greatly due to the wireless link characteristics. We present a measurement analysis of the performance of UDP and TCP over 3G and 3.5G wireless networks. The novelty of our measurement experiments lies in that we took our measurements in a fast moving car on a highway and in a high-speed train running at 300 km/h. Our results show that mobile nodes experience far worse performance than stationary nodes over the same network. Categories and Subject Descriptors:"
10.1.1.16.8449,Naturally Conveyed Explanations of Device Behavior,2001,"Designers routinely explain their designs to one another using sketches and verbal descriptions of behavior, both of which can be understood long before the device has been fully specified. But current design tools fail almost completely to support this sort of interaction, instead not only forcing designers to specify details of the design, but typically requiring that they do so by navigating a forest of menus and dialog boxes, rather than directly describing the behaviors with sketches and verbal explanations. We have created a prototype system, called assistance, capable of interpreting multimodal explanations for simple 2-D kinematic devices. The program generates a model of the events and the causal relationships between events that have been described via hand drawn sketches, sketched annotations, and verbal descriptions. Our goal is to make the designer's interaction with the computer more like interacting with another designer. This requires the ability not only to understand physical devices but also to understand the means by which the explanations of these devices are conveyed."
10.1.1.16.5136,Functional Abstraction driven Design Space Exploration of Heterogeneous Programmable Architectures,2001,"Rapid Design Space Exploration (DSE) of a programmable architecture is feasible using an automatic toolkit (compiler, simulator, assembler) generation methodology driven byan  Architecture Description Language (ADL). While manycontemporary ADLs can effectively capture one class of architecture, they are typically unable to capture a wide spectrum of processor and memory features present in DSP, VLIW, EPIC and Superscalar processors. The main bottleneck has been the lack of an abstraction underlying the ADL (covering a diverse set of architectural features) that permits reuse of the abstraction primitives to compose the heterogeneous architectures. We present in this paper the functional abstraction needed to capture suchwidevarietyof programmable architectures. We illustrate the usefulness of this approachby specifying twovery different architectures using functional abstraction. Our DSE results demonstrate the power of reuse in composing heterogeneous architectures using functional abstraction primitives allowing for a reduction in the time for specification and exploration by at least an order of magnitude."
10.1.1.16.493,Extensible Virtual Machines,2001,"Virtual machines (vms) have enjoyed a resurgence as a way of allowing the same application program to be used across a range of computer systems. This flexibility comes from the abstraction that the vm provides over the native interface of a particular computer. However, this also means that the application is prevented from taking the features of particular physical machines into account in its implementation. This dissertation addresses the question of why, where and how it is useful, possible and practicable to provide an application with access to lower-level interfaces. It argues that many aspects of vm implementation can be devolved safely to untrusted applications and demonstrates this through a prototype which allows control over run-time compilation, object placement within the heap and thread scheduling. The proposed architecture separates these application-specific policy implementations from the application itself. This allows one application to be used with different policies on different systems and also allows nave or premature optimizations to be removed."
10.1.1.16.2219,Extensible virtual machines,2001,"Virtual machines (vms) have enjoyed a resurgence as a way of allowing the same application program to be used across a range of computer systems. This flexibility comes from the abstraction that the vm provides over the native interface of a particular computer. However, this also means that the application is prevented from taking the features of particular physical machines into account in its implementation. This dissertation addresses the question of why, where and how it is useful, possible and practicable to provide an application with access to lower-level interfaces. It argues that many aspects of vm implementation can be devolved safely to untrusted applications and demonstrates this through a prototype which allows control over run-time compilation, object placement within the heap and thread scheduling. The proposed architecture separates these application-specific policy implementations from the application itself. This allows one application to be used with different policies on different systems and also allows nave or premature optimizations to be removed."
10.1.1.159.5080,Generalization analysis of listwise learning-to-rank algorithms using rademacher average,2008,"This paper presents a theoretical framework for ranking, and demonstrates how to perform generalization analysis of listwise ranking algorithms using the framework. Many learning-to-rank algorithms have been proposed in recent years. Among them, the listwise approach has shown higher empirical ranking performance when compared to the other approaches. However, there is no theoretical study on the listwise approach as far as we know. In this paper, we propose a theoretical framework for ranking, which can naturally describe various listwise learningto-rank algorithms. With this framework, we prove a theorem which gives a generalization bound of a listwise ranking algorithm, on the basis of Rademacher Average of the class of compound functions. The compound functions take listwise loss functions as outer functions and ranking models as inner functions. We then compute the Rademacher Averages for existing listwise algorithms of ListMLE, ListNet, and RankCosine. We also discuss the tightness of the bounds in different situations with regard to the list length and transformation function."
10.1.1.159.3409,User Grouping Behavior in Online Forums ∗,,"Online forums represent one type of social media that is particularly rich for studying human behavior in information seeking and diffusing. The way users join communities is a reflection of the changing and expanding of their interests toward information. In this paper, we study the patterns of user participation behavior, and the feature factors that influence such behavior on different forum datasets. We find that, despite the relative randomness and lesser commitment of structural relationships in online forums, users’ community joining behaviors display some strong regularities. One particularly interesting observation is that the very weak relationships between users defined by online replies have similar diffusion curves as those of real friendships or co-authorships. We build social selection models, Bipartite Markov Random Field (BiMRF), to quantitatively evaluate the prediction performance of those feature factors and their relationships. Using these models, we show that some features carry supplementary information, and the effectiveness of different features vary in different types of forums. Moreover, the results of BiMRF with two-star configurations suggest that the feature of user similarity defined by frequency of communication or number of common friends is inadequate to predict grouping behavior, but adding node-level features can improve the fit of the model."
10.1.1.159.1642,Perceptual Watermarks for Digital Images and Video ,2007,"The growth of new imaging technologies has created a need for techniques that can be used for copyright protection of digital images. Copyright protection involves the authentication of image content and/or ownership and can be used to identify illegal copies of a (possibly forged) image. One approach for copyright protection is to introduce an invisible signal known as a digital watermark in the image. In this paper, we describe digital image watermarking techniques, known as perceptually based watermarks, that are designed to exploit aspects of the human visual system. In the most general sense, any watermarking technique that attempts to incorporate an invisible mark into an image is perceptually based. However, in order to provide transparency (invisibility of the watermark) and robustness to attack, more sophisticated use of perceptual information in the watermarking process is required. Several techniques have been introduced that incorporate a simple visual model in the marking procedure. Such techniques usually take advantage of frequency selectivity and weighing to provide some perceptual criteria in the watermarking process. Even more elaborate visual models are used to develop schemes that not only take advantage of frequency"
10.1.1.155.498, High-Level Synthesis of Nonprogrammable Hardware Accelerators,2000,"  The PICO-N system automatically synthesizes embedded nonprogrammable accelerators to be used as co-processors for functions expressed as loop nests in C. The output is synthesizable VHDL that defines the accelerator at the register transfer level (RTL). The system generates a synchronous array of customized VLIW (very-long instruction word) processors, their controller, local memory, and interfaces. The system also modifies the user's application software to make use of the generated accelerator. The user indicates the throughput to be achieved by specifying the number of processors and their initiation interval. In experimental comparisons, PICO-N designs are slightly more costly than hand-designed accelerators with the same performance."
10.1.1.155.477,An Alternative to Working on Machine Consciousness,2009,"This paper extends three decades of work arguing that researchers who discuss consciousness should not restrict themselves only to (adult) human minds, but should study (and attempt to model) many kinds of minds, natural and artificial, thereby contributing to our understanding of the space containing all of them. We need to study what they do or can do, how they can do it, and how the natural ones can be emulated in synthetic minds. That requires: (a) understanding sets of requirements that are met by different sorts of minds, i.e. the niches that they occupy, (b) understanding the space of possible designs, and (c) understanding complex and varied relationships between requirements and designs. Attempts to model or explain any particular phenomenon, such as vision, emotion, learning, language use, or consciousness lead to muddle and confusion unless they are placed in that broader context. A methodology for making progress is summarised and a novel requirement proposed for a theory of how human minds work: the theory should support a single generic design for a learning, developing system that, in addition to meeting familiar requirements, should be capable of developing different and opposed philosophical viewpoints about consciousness, and the so-called hard problem. In other words, we need a common explanation for the mental machinations of mysterians, materialists, functionalists,"
10.1.1.155.1422,Towards a real time panoramic depth sensor,2003,Abstract. Recently we have presented a system for panoramic depth imaging with a single standard camera. One of the problems of such a system is the fact that we cannot generate a stereo pair of images in real time. This paper presents a possible solution to this problem. Based on a new sensor setup simulations were performed to establish the quality of new results in comparison to results obtained with the old sensor setup. The goal of the paper is to reveal whether the new setup can be used for real time capturing of panoramic depth images and consequently for autonomous navigation of a mobile robot in a room. 1
10.1.1.154.6270,CSMET: Comparative Genomic Motif Detection via Multi- Resolution,2007,"Functional turnover of transcription factor binding sites (TFBSs), such as whole-motif loss or gain, are common events during genome evolution. Conventional probabilistic phylogenetic shadowing methods model the evolution of genomes only at nucleotide level, and lack the ability to capture the evolutionary dynamics of functional turnover of aligned sequence entities. As a result, comparative genomic search of non-conserved motifs across evolutionarily related taxa remains a difficult challenge, especially in higher eukaryotes, where the cis-regulatory regions containing motifs can be long and divergent; existing methods rely heavily on specialized pattern-driven heuristic search or sampling algorithms, which can be difficult to generalize and hard to interpret based on phylogenetic principles. We propose a new method: Conditional Shadowing via Multi-resolution Evolutionary Trees, or CSMET, which uses a context-dependent probabilistic graphical model that allows aligned sites from different taxa in a multiple alignment to be modeled by either a background or an appropriate motif phylogeny conditioning on the functional specifications of each taxon. The functional specifications themselves are the output of a phylogeny which models the evolution not of individual nucleotides, but of the overall functionality (e.g., functional retention or loss) of the aligned sequence segments over lineages. Combining this method with a hidden Markov model that autocorrelates evolutionary rates on successive sites in the genome, CSMET offers a principled way to take into consideration lineage-specific evolution of TFBSs during motif detection, and a readily"
10.1.1.153.8636,FINAL ENVIRONMENTAL ASSESSMENT/REGULATORY IMPACT REVIEW/ INITIAL REGULATORY FLEXIBILITY ANALYSIS for Proposed AMENDMENT 85 to the Fishery Management Plan for Groundfish of the Bering Sea/Aleutian Islands Management Area ALLOCATION OF PACIFIC COD AMONG SEC,,"impacts of revising the separate apportionments of the BSAI Pacific cod ITAC among the fixed gear sectors (hook-and-line catcher processors, ≥60 ’ hook-and-line catcher vessels, pot catcher processors, ≥60 ’ pot catcher vessels, and pot/hook-and-line vessels <60 ' in length), jig sector, and trawl sectors based on recent sector catch histories. This action also proposes to implement a further split of the trawl CP sector allocation between the non-AFA and AFA trawl CP sectors. This action also proposes to increase the BSAI Pacific cod allocation to the western Alaska Community Development Quota Program and to modify that allocation such that it represents a directed fishing allowance. Finally, this amendment evaluates the effects of 1) apportioning halibut and crab prohibited species catch allowances among the Pacific cod trawl sectors, and 2) apportioning the halibut prohibited species catch allowance between the Pacific cod hook-and-line sectors. This amendment is intended to reduce uncertainty by establishing and modifying sector allocations such that they better reflect historic use, by sector, taking into account community and other socio-economic considerations. Prepared by staff of the"
10.1.1.153.7053,Quincunx fundamental refinable functions and quincunx biorthogonal wavelets,1999,"Abstract. We analyze the approximation and smoothness properties of quincunx fundamental refinable functions. In particular, we provide a general way for the construction of quincunx interpolatory refinement masks associated with the quincunx lattice in R 2. Their corresponding quincunx fundamental refinable functions attain the optimal approximation order and smoothness order. In addition, these examples are minimally supported with symmetry. For two special families of such quincunx interpolatory masks, we prove that their symbols are nonnegative. Finally, a general way of constructing quincunx biorthogonal wavelets is presented. Several examples of quincunx interpolatory masks and quincunx biorthogonal wavelets are explicitly computed. 1."
10.1.1.153.6743,Fishery Management Plan for Groundfish of the Bering Sea/Aleutian Islands Management Area ALLOCATION OF PACIFIC COD AMONG SECTORS and APPORTIONMENT OF SECTOR ALLOCATIONS BETWEEN BERING SEA AND ALEUTIAN ISLANDS SUBAREAS Abstract: This Environmental Assessm,,"of revising the separate apportionments of the BSAI Pacific cod ITAC among the fixed gear sectors (hook-and-line catcher processors, ≥60 ’ hook-and-line catcher vessels, pot catcher processors, ≥60 ’ pot catcher vessels, and pot/hook-and-line vessels <60 ' in length), jig sector, and trawl sectors based on recent sector catch histories. This action also proposes to implement a further split of the trawl CP sector allocation between the non-AFA and AFA trawl CP sectors. This action also proposes to increase the BSAI Pacific cod allocation to the western Alaska Community Development Quota Program and to modify that allocation such that it represents a directed fishing allowance. Finally, this amendment evaluates the effects of 1) apportioning halibut and crab prohibited species catch allowances among the Pacific cod trawl sectors, and 2) apportioning the halibut prohibited species catch allowance between the Pacific cod hook-and-line sectors. This amendment is intended to reduce uncertainty by establishing and modifying sector allocations such that they better reflect historic use, by sector, taking into account community and other socio-economic considerations. Prepared by staff of the"
10.1.1.151.7840,3G and 3.5G Wireless Network Performance Measured from Moving Cars and High-Speed Trains,,"In recent years, the world has witnessed the deployment of several 3G and 3.5G wireless networks based on technologies such as CDMA 1x EVolution Data-Only (EVDO), High-Speed Downlink Packet Access (HSDPA), and mobile WiMax (e.g., WiBro). Although 3G and 3.5G wireless networks support enough bandwidth for typical Internet applications, their performance varies greatly due to the wireless link characteristics. We present a measurement analysis of the performance of UDP and TCP over 3G and 3.5G wireless networks. The novelty of our measurement experiments lies in the fact that we took our measurements in a fast moving car on a highway and in a high-speed train running at 300 km/h. Predictably, our results show that mobile nodes experience far worse performance than stationary nodes over the same network. 1."
10.1.1.150.4164,Alternate Affy Gene Expression Summary Methods.................... 3,2009,Date/Publication 2009-10-06 06:56:27 R topics documented:
10.1.1.150.2865,Parameter Synthesis in Nonlinear Dynamical Systems: Application to Systems Biology,,"Abstract. The dynamics of biological processes are often modeled as systems of nonlinear ordinary differential equations (ODE). An important feature of nonlinear ODEs is that seemingly minor changes in initial conditions or parameters can lead to radically different behaviors. This is problematic because in general it is never possible to know/measure the precise state of any biological system due to measurement errors. The parameter synthesis problem is to identify sets of parameters (including initial conditions) for which a given system of nonlinear ODEs does not reach a given set of undesirable states. We present an efficient algorithm for solving this problem that combines sensitivity analysis with an efficient search over initial conditions. It scales to high-dimensional models and is exact if the given model is affine. We demonstrate our method on a model of the acute inflammatory response to bacterial infection, and identify initial conditions consistent with 3 biologically relevant outcomes."
10.1.1.150.2217,Towards Interactivity for TEX,,"Much work has been done to improve the level of interactivity avdable to TEX users. This work is categorized, and probable reasons are discussed why it is not really widespread. A more general view of ""interactivity "" may also lead to other tools. A common prerequisite for all these tools is the need to know about TEX'S functionality. The description of TEX should be formal, since the avdable lnformal descriptions have not given satisfactory results. After an abstract decomposition of TEX, an approach for the formal specification of one subsystem (the macro language) is presented. This specification may be interpreted by a Common Lisp system. The resulting Executable TEX Language Specification (ETLS) can be used as the kernel of a TEX macro debugger. Variations on A Theme ""Interactive TEX is the oldest theme on TUG meetings:"
10.1.1.15.9270,Automated Hoarding for Mobile Computers,1997,"A common problem facing mobile computing is disconnected  operation, or computing in the absence of a network.  Hoarding eases disconnected operation by selecting a subset of the user's files for local storage. We describe a hoarding system that can operate without user intervention, by observing user activity and predicting future needs. The system calculates a new measure, semantic distance, between individual files, and uses this to feed a clustering algorithm that chooses which files should be hoarded. A separate replication system manages the actual transport of data; any of a number of replication systems may be used. We discuss practical problems encountered in the real world and present usage statistics showing that our system outperforms previous approaches by factors that can exceed 10:1.  1 Introduction  The face of computing today is rapidly being changed by the advent of mobility, but the utility of the portable computer is seriously challenged by the problem of disconnect..."
10.1.1.15.6066,A Bibliography of Papers in Lecture Notes in Computer Science (2002) (Part 4 of 4),2002,Version 1.03 Title word cross-reference (2
10.1.1.15.273,Applicability of Reinforcement Learning,,"We describe our experiences in trying to implement  a hierarchical reinforcement learning system,  and follow with conclusions that wehavedrawn  from the difficulties that we encountered. We  present our objectives before we started, the problems  weencountered along the way, the solutions  we devised for some of these problems, and our  conclusions afterward about the class of problems  for which reinforcement learning may be suitable."
10.1.1.15.266,Improving Automatic Indexing through Concept Combination,,"Although indexes may overlap, the output of an automatic indexer is generally presented as a fiat and unstructured list of terms. Our purpose is to exploit term overlap and embedding so as to yield a substantial qualitative and quantitative improvement in automatic indexing through concept combination. The increase in the volume of indexing is 10.5% for free indexing and 52.3% for controlled indexing. The resulting structure of the indexed corpus is a partial conceptual analysis."
10.1.1.15.2559,Run-Time Support for Distributed Sharing in Safe Languages,2002,This paper falls in the latter category
10.1.1.149.8326,MARKOV LOCALIZATION IN THE ROBOCUP SIMULATION LEAGUE 1,,"Abstract: For mobile robots, localization is the process of updating the pose of a robot, given information about its environment and the history of its sensor readings. This paper describes an implementation of the Markov localization method using a probability distribution across a fine-grained grid of robot poses to globally localize a robot even in the presence of noise. In particular, we applied this technique to self-localize a soccer player in the RoboCup Simulation League. This simulation features a highly dynamic environment and inaccurate sensor readings similar to real-world situations. We provide an experimental analysis of this implementation and show results indicating that the robot is able to remain relatively well localized in terms of position."
10.1.1.149.7097,Generalization Analysis of Listwise Learning-to-Rank Algorithms,,"This paper presents a theoretical framework for ranking, and demonstrates how to perform generalization analysis of listwise ranking algorithms using the framework. Many learning-to-rank algorithms have been proposed in recent years. Among them, the listwise approach has shown higher empirical ranking performance when compared to the other approaches. However, there is no theoretical study on the listwise approach as far as we know. In this paper, we propose a theoretical framework for ranking, which can naturally describe various listwise learningto-rank algorithms. With this framework, we prove a theorem which gives a generalization bound of a listwise ranking algorithm, on the basis of Rademacher Average of the class of compound functions. The compound functions take listwise loss functions as outer functions and ranking models as inner functions. We then compute the Rademacher Averages for existing listwise algorithms of ListMLE, ListNet, and RankCosine. We also discuss the tightness of the bounds in different situations with regard to the list length and transformation function."
10.1.1.149.700,T-79.232 Safety Critical Systems Case Study 1: Formal Methods- Introduction,,What’s the purpose of formal methods? They are used in a similar way as prototypes
10.1.1.148.5759,T-79.5303 Safety Critical Systems Case Study 1: Formal Methods- Introduction,,What’s the purpose of formal methods? They are used in a similar way as prototypes
10.1.1.148.2577,Displaying 3D Images: Algorithms for Single Image Random Dot Stereograms,1994,"This paper describes how to generate a single image which, when viewed in the appropriate way, appears to the brain as a 3D scene. The image is a stereogram composed of seemingly random dots. A new, simple and symmetric algorithm for generating such images from a solid model is given, along with the design parameters and their influence on the display. The algorithm improves on previously-described ones in several ways: it is symmetric and hence free from directional (right-to-left or left-to-right) bias, it corrects a slight distortion in the rendering of depth, it removes hidden parts of surfaces, and it also eliminates a type of artifact that we call an “echo”. Random dot stereograms have one remaining problem: difficulty of initial viewing. If a computer screen rather than paper is used for output, the problem can be ameliorated by shimmering, or time-multiplexing of pixel values. We also describe a simple computational technique for determining what is present in a stereogram so that, if viewing is difficult, one can ascertain what to look for. Keywords: Single image random dot stereograms, SIRDS, autostereograms,"
10.1.1.147.3931,Code compression,1997,"Current research in compiler optimization counts mainly CPU time and perhaps the first cache level or two. This view has been important but is becoming myopic, at least from a system-wide viewpoint, as the ratio of network and disk speeds to CPU speeds grows exponentially. For example, we have seen the CPU idle for most of the time during paging, so compressing pages can increase total performance even though the CPU must decompress or interpret the page contents. Another profile shows that many functions are called just once, so reduced paging could pay for their interpretation overhead. This paper describes:. Measurements that show how code compression can save space and total time in some important real-world scenarios.. A compressed executable representation that is roughly the same size as gzipped x86 programs and can be interpreted without decompression. It can also be compiled to high-quality machine code at 2.5 megabytes per second on a 120MHz Pentium processor l A compressed “wire ” representation that must be decompressed before execution but is, for example, roughly 21 % the size of SPARC code when compressing gee."
10.1.1.147.3363,Modeling and Rendering of Weathered Stone,1999,"Stone is widespread in its use as a building material and artistic medium. One of its most remarkable qualities is that it changes appearance as it interacts with the environment. These changes are mainly confined to the surface but involve complex volumetric effects such as erosion and mineral dissolution. This paper presents an approach for the modeling and rendering of changes in the shape and appearance of stone. To represent stone, we introduce a slab data structure, which is a surface-aligned volume confined to a narrow region around the boundary of the stone. Our weathering model employs a simulation of the flow of moisture and the transport, dissolution, and recrystallization of minerals within the porous stone volume. In addition, this model governs the erosion of material from the surface. To render the optical effects of translucency and coloration due to the composition of minerals near the surface, we simulate the scattering of light inside the stone using a general subsurface Monte Carlo ray tracer. These techniques can capture many aspects of the time-dependent appearance of stone. We demonstrate the approach with models of granite and marble statues, as well as a sandstone column."
10.1.1.147.311,FaceTracer: A Search Engine for Large Collections of Images with Faces,,"Abstract. We have created the first image search engine based entirely on faces. Using simple text queries such as “smiling men with blond hair and mustaches, ” users can search through over 3.1 million faces which have been automatically labeled on the basis of several facial attributes. Faces in our database have been extracted and aligned from images downloaded from the internet using a commercial face detector, and the number of images and attributes continues to grow daily. Our classification approach uses a novel combination of Support Vector Machines and Adaboost which exploits the strong structure of faces to select and train on the optimal set of features for each attribute. We show state-of-the-art classification results compared to previous works, and demonstrate the power of our architecture through a functional, large-scale face search engine. Our framework is fully automatic, easy to scale, and computes all labels off-line, leading to fast on-line search performance. In addition, we describe how our system can be used for a number of applications, including law enforcement, social networks, and personal photo management. Our search engine will soon be made publicly available. 1"
10.1.1.147.3014,ACKNOWLEDGEMENT,,"(CIFOR), 2007."
10.1.1.147.1024,An RPC mechanism for transportable agents,1996,"Transportable agents are autonomous programs that migrate from machine to machine, performing complex processing at each step to satisfy client requests. As part of their duties agents often need to communicate with other agents. We propose to use remote procedure call (RPC) along with a exible interface de nition language (IDL), to add structure to inter-agent communication. The real power of our Agent RPC comes from a client-server binding mechanism based on exible IDL matching and from support for multiple simultaneous bindings. Our agents are programmed in Agent Tcl�wedescribe how the Tcl implementation made RPC particularly easy to implement. Finally, although our RPC is designed for Agent Tcl programs, the concepts would also work for standard Tcl programs."
10.1.1.146.4368,CU-TMP: Temporal relation classification using syntactic and semantic features,2007,"We approached the temporal relation identification tasks of TempEval 2007 as pair-wise classification tasks. We introduced a variety of syntactically and semantically motivated features, including temporal-logicbased features derived from running our Task B system on the Task A and C data. We trained support vector machine models and achieved the second highest accuracies on the tasks: 61 % on Task A, 75 % on Task B and 54 % on Task C. 1"
10.1.1.145.7164,Convergence Distance,,"⇒ All messages scanned by security gateways 2. Wireless (WiFi) is spreading (Intel Centrino) 3. More Cell phones than POTS. Smart Cell phones w PDA, email, video, images ⇒ Mobility 4. Broadband Access is growing faster than cell phones Fiber is creeping towards home 5. Ethernet extending from Enterprise to Access to Metro … 6. Wiring more expensive than equipment ⇒ Wireless Access 7. Multi-Protocol Label Switching for traffic engineering 8. Voice over Internet Protocol (VOIP) is in the Mainstream"
10.1.1.145.6870,On smoothed k-CNF formulas and the Walksat algorithm,,"In this paper we study the model of ε-smoothed k-CNF formulas. Starting from an arbitrary instance F with n variables and m = dn clauses, apply the ε-smoothing operation of flipping the polarity of every literal in every clause independently at random with probability ε. Keeping ε and k fixed, and letting the density d = m/n grow, it is rather easy to see that for d ≥ ε −k ln 2, F becomes whp unsatisfiable after smoothing. We show that a lower density that behaves roughly like ε −k+1 suffices for this purpose. We also show that our bound on d is nearly best possible in the sense that there are k-CNF formulas F of slightly lower density that whp remain satisfiable after smoothing. One consequence of our proof is a new lower bound of Ω(2 k /k 2) on the density up to which Walksat solves random k-CNFs in polynomial time whp. We are not aware of any previous rigorous analysis showing that Walksat is successful at densities that are increasing as a function of k. 1"
10.1.1.145.3592,Bichromatic separability with two boxes: a general approach,2009,"Let S be a set of n points on the plane in general position such that its elements are colored red or blue. We study the following problem: Find a largest subset of S which can be enclosed by the union of two, not necessarily disjoint, axis-aligned rectangles R and B such that R (resp. B) contains only red (resp. blue) points. We prove that this problem can be solved in O(n 2 log n) time and O(n) space. Our approach is based on solving some instances of Bentley’s maximum-sum consecutive subsequence problem. We introduce the first known data structure to dynamically maintain the optimal solution of this problem. We show that our techniques can be used to efficiently solve a more general class of problems in data analysis."
10.1.1.144.7257,The future of spin networks,1997,"Since Roger Penrose rst introduced the notion of a spin network as a simple model of discrete quantum geometry, they have reappeared in quantum gauge theories, quantum gravity, topological quantum eld theory and conformal eld theory. The roles that spin networks play in these contexts are brie y described, with an emphasis on the question of the relationships among them. It is also argued that spin networks and their generalizations provide a language which may lead to a uni cation of the di erent approaches to quantum gravity and quantum geometry. This leads to a set of conjectures about the form of a future theory that may be simultaneously an extension of the non-perturbative quantization of general relativity and a non-perturbative formulation of string theory."
10.1.1.144.649,Securing distributed systems with information flow control,2006,"Recent operating systems [12, 21, 26] have shown that decentralized information flow control (DIFC) can secure applications built from mostly untrusted code. This paper extends DIFC to the network. We present DStar, a system that enforces the security requirements of mutually distrustful components through cryptography on the network and local OS protection mechanisms on each host. DStar does not require any fully-trusted processes or machines, and is carefully constructed to avoid covert channels inherent in its interface. We use DStar to build a three-tiered web server that mitigates the effects of untrustworthy applications and compromised machines. 1"
10.1.1.144.2304,Implementing Proof by Pointing without a Structure Editor ,1997," A proof by pointing user interface component allows a user to direct the course of a proof assistant by selecting terms with a mouse. Such a gesture is interpreted as a high-level tactical which triggers a sequence of low-level basic commands for the proof engine. The algorithm inherently relies on a structure-conscious environment; as a novelty we show how proof-by-pointing may easily be integrated into an interface without a structure editor. We discuss in detail the use of nested selectable text regions for user interaction, the modifications necessary to the proofengine output, and the algorithm for interpreting selections as proof commands, with particular reference to a concrete implementation using XEmacs and LEGO."
10.1.1.144.1073,Implementing proof by pointing without a structure editor,1997,"A proof by pointing user interface component allows a user to direct the course of a proof assistant by selecting terms with a mouse. Such a gesture is interpreted as a high-level tactical which triggers a sequence of low-level basic commands for the proof engine. The algorithm inherently relies on a structure-conscious environment; as a novelty we show how proof-by-pointing may easily be integrated into an interface without a structure editor. We discuss in detail the use of nested selectable text regions for user interaction, the modifications necessary to the proofengine output, and the algorithm for interpreting selections as proof commands, with particular reference to a concrete implementation using XEmacs and LEGO.  "
10.1.1.143.7564,A Case for Adapting Channel Width in Wireless Networks,2008,"We study a fundamental yet under-explored facet in wireless communication – the width of the spectrum over which transmitters spread their signals, or the channel width. Through detailed measurements in controlled and live environments, and using only commodity 802.11 hardware, we first quantify the impact of channel width on throughput, range, and power consumption. Taken together, our findings make a strong case for wireless systems that adapt channel width. Such adaptation brings unique benefits. For instance, when the throughput required is low, moving to a narrower channel increases range and reduces power consumption; in fixed-width systems, these two quantities are always in conflict. We then present SampleWidth, a channel width adaptation algorithm for the base case of two communicating nodes. This algorithm is based on a simple search process that builds on top of existing techniques for adapting modulation. Per specified policy, it can maximize throughput or minimize power consumption. Evaluation using a prototype implementation shows that SampleWidth correctly identities the optimal width under a range of scenarios. In our experiments with mobility, it increases throughput by more than 60 % compared to the best fixed-width configuration. "
10.1.1.142.3928,"Gaps in the differential forms spectrum on cyclic coverings, Preprint (arXiv:0708.3981",2007,"Abstract. We are interested in the spectrum of the Hodge-de Rham operator on a Z-covering X over a compact manifold M of dimension n + 1. Let Σ be a hypersurface in M which does not disconnect M and such that M − Σ is a fundamental domain of the covering. If the cohomology group H n/2 (Σ) is trivial, we can construct for each N ∈ N a metric g = gN on M, such that the Hodgede Rham operator on the covering (X, g) has at least N gaps in its (essential) spectrum. If H n/2 (Σ) ̸ = 0, the same statement holds true for the Hodge-de Rham operators on p-forms provided p / ∈ {n/2, n/2 + 1}. 1."
10.1.1.142.3479,Musical instrument identification based on f0-dependent multivariate normal distribution,,"The pitch dependency of timbres has not been fully exploited in musical instrument identification. In this paper, we present a method using an F0-dependent multivariate normal distribution of which mean is represented by a function of fundamental frequency (F0). This F0-dependent mean function represents the pitch dependency of each feature, while the F0-normalized covariance represents the non-pitch dependency. Musical instrument sounds are first analyzed by the F0-dependent multivariate normal distribution, and then identified by using the discriminant function based on the Bayes decision rule. Experimental results of identifying 6,247 solo tones of 19 musical instruments by 10-fold cross validation showed that the proposed method improved the recognition rate at individual-instrument level from 75.73 % to 79.73%, and the recognition rate at category level from 88.20 % to 90.65%. 1."
10.1.1.142.1306,Fingerprint classification by combination of flat and structural approaches,2001,"Abstract. This paper investigates the advantages of the combination of flat and structural approaches for fingerprint classification. A novel structural classification method is described and compared with the “multichannel ” flat method recently proposed by Jain et al. [1]. Performances and complementarity of the two methods are evaluated using NIST-4 Database. A simple approach based on the concept of “metaclassification ” is proposed for the combination of the two fingerprint classification methods. Reported results point out the potential advantages of the combination of flat and structural fingerprint-classification approaches. In particular, such results show that the exploitation of structural information allows increasing classification performances. 1."
10.1.1.141.7968,CSMET: Comparative Genomic Motif Detection via Multi- Resolution,2007,"Functional turnover of transcription factor binding sites (TFBSs), such as whole-motif loss or gain, are common events during genome evolution. Conventional probabilistic phylogenetic shadowing methods model the evolution of genomes only at nucleotide level, and lack the ability to capture the evolutionary dynamics of functional turnover of aligned sequence entities. As a result, comparative genomic search of non-conserved motifs across evolutionarily related taxa remains a difficult challenge, especially in higher eukaryotes, where the cis-regulatory regions containing motifs can be long and divergent; existing methods rely heavily on specialized pattern-driven heuristic search or sampling algorithms, which can be difficult to generalize and hard to interpret based on phylogenetic principles. We propose a new method: Conditional Shadowing via Multi-resolution Evolutionary Trees, or CSMET, which uses a context-dependent probabilistic graphical model that allows aligned sites from different taxa in a multiple alignment to be modeled by either a background or an appropriate motif phylogeny conditioning on the functional specifications of each taxon. The functional specifications themselves are the output of a phylogeny which models the evolution not of individual nucleotides, but of the overall functionality (e.g., functional retention or loss) of the aligned sequence segments over lineages. Combining this method with a hidden Markov model that autocorrelates evolutionary rates on successive sites in the genome, CSMET offers a principled way to take into consideration lineage-specific evolution of TFBSs during motif detection, and a readily"
10.1.1.141.5540,PRISM User’s Manual (Version 1.12),,"The past few years have witnessed a tremendous interest in logic-based probabilistic learning as testified by the number of formalisms and systems and their applications. Logic-based probabilistic learning is a multidisciplinary research area that integrates relational or logic formalisms, probabilistic reasoning mechanisms, and machine learning and data mining principles. Logic-based probabilistic learning has found its way into many application areas including bioinformatics, diagnosis and troubleshooting, stochastic language processing, information retrieval, linkage analysis and discovery, robot control, and probabilistic constraint solving. PRISM (PRogramming In Statistical Modeling) is a logic-based language that integrates logic programming and probabilistic reasoning including parameter learning. It allows for the description of independent probabilistic choices and their consequences in general logic programs. PRISM supports parameter learning, i.e. for a given set of possibly incomplete observed data, PRISM can estimate the probability distributions to best explain the data. This power is suitable for applications such as learning parameters of stochastic grammars, training stochastic models for gene sequence analysis, game record analysis, user modeling, and obtaining probabilistic information for tuning systems performance. PRISM offers incomparable flexibility compared with specific statistical tools such as hidden Markov models (HMMs) [4, 28], probabilistic context free grammars (PCFGs) [4] and discrete Bayesian networks. PRISM employs a proof-theoretic approach to learning. It conducts learning in two phases: the first phase searches for all the explanations for the observed data, and the second phase estimates the probability distributions by using the EM algorithm. Learning from flat explanations can be exponential in both space and time. To speed up learning, the authors proposed learning from explanation graphs and using tabling to reduce redundancy in the construction of explanation graphs. The PRISM programming system is implemented on top of B-Prolog"
10.1.1.141.2567,"The architectural costs of streaming I/O: A comparison of workstations, clusters, and SMPs",1998,"We investigate resource usage while performing streaming I/O by contrasting three architectures, a single workstation, a cluster, and an SMP, under various I/O benchmarks. We derive analytical and empirically-1Introduction based models of resource usage during data transfer, examining the I/O bus, memory bus, network, and processor of each system. By investigating each resource in detail, we assess what comprises a wellbalanced system for these workloads. We find that the architectures we study are not well balanced for streaming I/O applications. Across the platforms, the main limitation to attaining peak performance is the CPU, due to lack of data locality. Increasing processorperformance (especially with improved block operation performance) will be of great aid for these workloads in the future. For a cluster workstation, the I/O bus is a major system bottleneck, because of the increased load placed on it from network communication. A well-balanced cluster workstation should have copious I/O bus bandwidth, perhaps via multiple I/O busses. The SMP suffers from poor memory-system performance; even when there is true parallelism in the benchmark, contention in the shared-memory system leads to reduced performance. As a result, the clustered workstations provide higher absolute performance for streaming I/O workloads."
10.1.1.141.193,Musical instrument identification based on F0-dependent multivariate normal distribution,2003,"The pitch dependency of timbres has not been fully exploited in musical instrument identification. In this paper, we present a method using an F0-dependent multivariate normal distribution of which mean is represented by a function of fundamental frequency (F0). This F0-dependent mean function represents the pitch dependency of each feature, while the F0-normalized covariance represents the non-pitch dependency. Musical instrument sounds are first analyzed by the F0-dependent multivariate normal distribution, and then identified by using the discriminant function based on the Bayes decision rule. Experimental results of identifying 6,247 solo tones of 19 musical instruments by 10-fold cross validation showed that the proposed method improved the recognition rate at individual-instrument level from 75.73 % to 79.73%, and the recognition rate at category level from 88.20 % to 90.65%. 1."
10.1.1.140.6887,  Packing Square Tiles into One Texture,2004,"This paper deals with the packing of square tiles of the same size into one texture. Texture size is constrained by the graphics hardware. In particular, width and height resolutions must be powers of two. To cover the whole texture and avoid space loss, common schemes pack a number of tiles that is a power of two. To cover the"
10.1.1.140.2908,On smoothed k-CNF formulas and the Walksat,2008,algorithm
10.1.1.14.8976,Exploring a Two-Market Genetic Algorithm,2002,"The ordinary genetic algorithm may be  thought of as conducting a single market in  which solutions compete for success, as measured  by the fitness funtion. We introduce  a two-market genetic algorithm, consisting  of two phases, each of which is an ordinary  single-market genetic algorithm. The twomarket  genetic algorithm has a natural interpretation  as a method of solving constrained  optimization problems. Phase 1 is optimality  improvement; it works on the problem without  regard to constraints. Phase 2 is feasibility  improvement; it works on the existing  population of solutions and drives it towards  feasibility. We tested this concept on 14 standard  knapsack test problems for genetic algorithms,  with excellent results. The paper  concludes with discussions of why the twomarket  genetic algorithm is successful and of  how this work can be extended."
10.1.1.14.8651,Revisiting R-tree Construction Principles,2002,"Spatial indexing is a well researched field that benefited computer  science with many outstanding results. Our effort in this paper can  be seen as revisiting some outstanding contributions to spatial indexing,  questioning some paradigms, and designing an access method with globally  improved performance characteristics. In particular, we argue that  dynamic R-tree construction is a typical clustering problem which can be  addressed by incorporating existing clustering algorithms. As a working  example, we adopt the well-known k-means algorithm. Further, we study  the effect of relaxing the ""two-way split procedure and propose a ""multiway  "" split, which inherently is supported by clustering techniques. We  compare our clustering approach to two prominent examples of spatial  access methods, the R- and the R*-tree."
10.1.1.14.7064,Boolean Analyzer - An Algorithm That Uses A Probabilistic Interestingness Measure to find Dependency/Association Rules In A Head Trauma Data,2001,"A new, binary-based technique is presented for finding dependency/association rules called the Boolean Analyzer (BA). With initial guidance from a domain user or domain expert, BA is given one or more metrics to partition the entire data set. This leads to analyzing the implicit domain knowledge and creating weighted rules in the form of boolean expressions. To augment the analysis of the rules produced, we can additionally apply a probabilistic interestingness measure (PIM) to order the generated rules based on event dependency, where events are combinations of primed and unprimed variables."
10.1.1.14.6482,A Logic-based Knowledge Representation for Authorization with Delegation,1999,": We introduce Delegation Logic (DL), a logic-based knowledge representation (i.e., language) that deals with authorization in large-scale, open, distributed systems. Of central importance in any system for deciding whether requests should be authorized in such a system are delegation of authority, negation of authority, and conflicts between authorities. DL's approach to these issues and to the interplay among them borrows from previous work on delegation and trust management in the computer-security literature and previous work on negation and conflict handling in the logic-programming and non-monotonic reasoning literature, but it departs from previous work in some crucial ways. In this introductory paper, we present the syntax and semantics of DL and explain our novel design choices. This first paper focuses on delegation, including explicit treatment of delegation depth and delegation to complex principals; a forthcoming companion paper focuses on negation. Compared to previous lo..."
10.1.1.14.5983,Checking Ownership and Confinement Properties,2002,"A number of formal proposals to manage aliasing in Java-like programming  languages have been advanced over the last five years. Unfortunately,  it is not clear how practical these proposals are, that is, how well they relate to the  kinds of programs currently written in those languages. We have analysed heap  dumps from a corpus of Java programs to identify their implicit aliasing structures,  including object ownership, confinement, and uniqueness. Understanding  the kinds of aliasing present in programs should help us to design formalisms to  make explicit the kinds of aliasing implicit in object-oriented programs."
10.1.1.139.5197,Sensitivity profiles from an array of coils for encoding and reconstruction,2000,"A new parallel imaging technique was implemented which can result in reduced image acquisition times in MRI. MR data is acquired in parallel using an array of receiver coils and then reconstructed simultaneously with multiple processors. The method requires the initial estimation of the 2D sensitivity profile of each coil used in the receiver array. These sensitivity profiles are then used to partially encode the images of interest. A fraction of the total number of k-space lines is consequently acquired and used in a parallel reconstruction scheme, allowing for a substantial reduction in scanning and display times. This technique is in the family of parallel acquisition schemes such as simultaneous acquisition of spatial harmonics (SMASH) and sensitivity encoding (SENSE). It extends the use of the SMASH method to allow the placement of the receiver coil array around the object of interest, enabling imaging of any plane within the volume of interest. In addition, this technique permits the arbitrary choice of the set of k-space lines used in the reconstruction and lends itself to parallel reconstruction, hence allowing for real-time rendering. Simulated results with a 16-fold increase in temporal resolution are shown, as are experimental results with a 4-fold increase in temporal resolution. Magn"
10.1.1.139.2637,  Depth estimation via sampling,2008,"  In this chapter, we introduce a “trivial” but yet powerful idea. Given a set S of objects, a point p that is contained in some of the objects, and let its weight be the number of objects that contains it. We can estimate the depth/weight of p by counting the number of objects that contains it in a random sample of the objects. In fact, by considering points induced by the sample, we can bound the number of “light ” vertices induced by S. This idea can be extended to bounding the number of “light” configurations induced by a set of objects. This approach leads to a sequence of short, beautiful, elegant and correct  proofs of several hallmark results in discrete geometry. While the results in this chapter are not directly related to approximation algorithms, the insights and general approach would be useful for us later, or so one hopes. 8.1 The at most k-levels Let L be a set of n lines in the plane. A point p ∈  � ℓ∈L ℓ is of level k, if there are k lines of L strictly below it. The k-level is the closure of set of points of level k. Namely, the k-level is an x-monotone curve along the lines of L."
10.1.1.138.7886,Simulating a lesion in a basis function model of spatial representations: comparison with hemineglect,2001,"The basis function theory of spatial representations explains how neurons i n the parietal cortex can perform nonlinear transformations from sensory to motor coordinates. The authors present computer simulations showing that unilateral parietal lesions leading to a neuronal gradient in basis function maps can account for the behavior of patients with hemineglect, including (a) neglect in line cancellation and line bisection experiments; (b) neglect in multiple frames of reference simultaneously; (c) relative neglect, a form of what is sometime called object-centered neglect; and (d) neglect without optic ataxia. Contralateral neglect arises in the model because the lesion produces an imbalance in the salience of stimuli that is modulated by the orientation of the body in space. These results strongly support the basis function theory for spatial representations in humans and provide a computational model of hemineglect at the single-cell level. A unilateral lesion of the parieto-occipital cortex in humans often produces hemineglect (Heilman, Watson,  & Valenstein, 1985; Pouget & Driver, 1999; Vallar, 1998), a neurologic syndrome characterized by a conspicuous inability to react or respond to stimuli presented in the hemispace contralateral to the lesion. For example, when asked to"
10.1.1.138.622,AUTOMATIC GEOMETRIC AND RADIOMETRIC REGISTRATION OF LANDSAT-TM IMAGES USING MUTUAL INFORMATION,,"Abstract: This work is on development of a method for automatic registration of satellite images acquired on different dates, for both geometric and radiometric correction with respect to a reference image. Mutual information statistics is used as the similarity metric of geometric and radiometric registration. Affine and linear transformations are used in geometric and radiometric correction respectively. Powell's method is applied in iterative optimization to find the best transformation parameters for both types of registration, based on the maximum mutual information between images. The method is validated using Landsat's Thematic Mapper (TM) sensor, bands 3, 4 and 5 images, obtained on five separate dates for scene 231-062 in the Central Amazon. Key words: radiometric registration, geometric registration, mutual information. 1."
10.1.1.138.4254,Trading Memory for Randomness \Lambda,,"Abstract Strategies in repeated games can be classified as towhether or not they use memory and/or randomization. We consider Markov decision processes and 2-player graphgames, both of the deterministic and probabilistic varieties. We characterize when memory and/or randomization arerequired for winning with respect to various classes of!-regular objectives, noting particularly when the use of memory can be traded for the use of randomization. In partic-ular, we show that Markov decision processes allow randomized memoryless optimal strategies for all M&quot;uller ob-jectives. Furthermore, we show that 2-player probabilistic graph games allow randomized memoryless strategies forwinning with probability 1 those M&quot;uller objectives which are upward-closed. Upward-closure means that if a set ff ofinfinitely repeating vertices is winning, then all supersets of ff are also winning. 1"
10.1.1.138.3372,Memory ordering in modern microprocessors,2005,"has supported a large number of SMP systems based on a variety of CPUs since the 2.0 kernel. Linux has done an excellent job of abstracting away differences among these CPUs, even in kernel code. One important difference is how CPUs allow memory accesses to be reordered in SMP systems. SMMP Hardware Memory accesses are among the slowest of a CPU’s operations, due to the fact that Moore’s law has increased CPU instruction performance at a much greater rate than it has increased memory performance. This difference in performance increase means that memory operations have been getting increasingly expensive compared to simple register-to-register instructions. Modern CPUs sport increasingly large caches in order to reduce the"
10.1.1.138.245,X-trace: A pervasive network tracing framework,2007,"Modern Internet systems often combine different applications (e.g., DNS, web, and database), span different administrative domains, and function in the context of network mechanisms like tunnels, VPNs, NATs, and overlays. Diagnosing these complex systems is a daunting challenge. Although many diagnostic tools exist, they are typically designed for a specific layer (e.g., traceroute) or application, and there is currently no tool for reconstructing a comprehensive view of service behavior. In this paper we propose X-Trace, a tracing framework that provides such a comprehensive view for systems that adopt it. We have implemented X-Trace in several protocols and software systems, and we discuss how it works in three deployed scenarios: DNS resolution, a three-tiered photo-hosting website, and a service accessed through an overlay network. 1"
10.1.1.138.1549,Accepted dd/mm/yyyy,,"This paper presents an overview of our work towards building humanoid robots that can work alongside people as cooperative teammates. We present our theoretical framework based on a novel combination of Joint Intention Theory and Collaborative Discourse Theory, and demonstrate how it can be applied to allow a human to work cooperatively with a humanoid robot on a joint task using speech, gesture, and expressive cues. Such issues must be addressed to enable many new and exciting applications for humanoid robots that require them assist ordinary people in daily activities or to work as capable members of human-robot teams. Keywords: Human-robot interaction; teamwork; dialog and gesture; collaboration; social robots. 1."
10.1.1.138.1117,Analyzing stripped device-driver executables,2008,"Abstract. This paper sketches the design and implementation of Device-Driver Analyzer for x86 (DDA/x86), a prototype analysis tool for finding bugs in stripped Windows device-driver executables (i.e., when neither source code nor symbol-table/debugging information is available), and presents a case study. DDA/x86 was able to find known bugs (previously discovered by source-code-based analysis tools) along with useful error traces, while having a reasonably low false-positive rate. This work represents the first known application of automatic program verification/analysis to stripped industrial executables, and allows one to check that an executable does not violate known API usage rules (rather than simply trusting that the implementation is correct). 1"
10.1.1.137.8479,Advanced Polymorphic Worms: Evading IDS by Blending in with Normal Traffic,2004,"Abstract. Normal traffic can provide worms with a very good source of information to camouflage themselves. In this paper, we explore the concept of polymorphic worms that mutate based on normal traffic. We assume that a worm has already penetrated a system and is trying to hide its presence and propagation attempts from an IDS. We focus on stealthy worms that cannot be reliably detected by increases in traffic because of their low propagation factor. We first give an example of a simple polymorphic worm. Such worms can evade a signature-based IDS but not necessarily an anomaly-based IDS. We then show that it is feasible for an advanced polymorphic worm to gather a normal traffic profile and use it to evade an anomaly-based IDS. We tested the advanced worm implementation with three anomaly IDS approaches: NETAD, PAYL and Service-specific IDS. None of the three IDS approaches were able to detect the worm reliably. We found that Payload Execution. The goal of this paper is to advance the science of IDS by analyzing techniques polymorphic worms can use to hide themselves. By showing that polymorphic worms are a practical threat, we hope to stimulate further research to improve existing IDS. 1"
10.1.1.137.2115,Inductance 101: Analysis and design issues,2001,"With operating frequencies approaching the gigahertz range, inductance is becoming an increasingly important consideration in the design and analysis of on-chip interconnect. In this paper, we give a tutorial overview of the analysis and design issues related to on-chip inductance effects.We explain the complexity of the current flow in VLSI circuits. We discuss the applicability of the PEEC approach in a detailed circuit model of the signal and power grid interconnect, switching devices, power pads and the package. Further, we explain techniques that can be used to speed-up simulation of the large PEEC model. We then discuss a simplified model that uses the so-called loop inductance approach, and compare it with the detailed model.We present experimental results, obtained from simulations of industrial circuits, for both the PEEC and loop models. We also cover design techniques that can help tackle the on-chip inductance issues."
10.1.1.135.8369,Smart videoconferencing,2000,"The combination of acoustical and video processing to achieve a smart audio and video feed from a set of microphones and cameras is a task that might conventionally be accomplished by camerapersons and control room staff. However, in the context of videoconferencing this process needs to be performed by control software. We discuss the use of a multi-camera multi-microphone set up for unattended videoconferencing, and present details of a prototype implementation being developed. 1."
10.1.1.134.4609,Data movement and control substrate for parallel scientific computing,1997,"In this paper, we describe the design and implementation of a datamovement and control substrate (DMCS) for network-based, homogeneous communication within a single multiprocessor. DMCS is an implementation of an API for communication and computation that has been proposed by the PORTS consortium. One of the goals of this consortium is to de ne an API that can support heterogeneous computing without undue performance penalties for homogeneous computing. Preliminary results in our implementation suggest that this is quite feasible. The DMCS implementation seeks to minimize the assumptions made about the homogeneous nature of its target architecture. Finally, we present some extensions to the API for PORTS that will improve the performance of sparse, adaptive and irregular type of numeric computations."
10.1.1.133.788,Displaying 3D Images: Algorithms for Single Image Random Dot Stereograms,1994,"This paper describes how to generate a single image which, when viewed in the appropriate way, appears to the brain as a 3D scene. The image is a stereogram composed of seemingly random dots. A new, simple and symmetric algorithm for generating such images from a solid model is given, along with the design parameters and their influence on the display. The algorithm improves on previously-described ones in several ways: it is symmetric and hence free from directional (right-to-left or left-to-right) bias, it corrects a slight distortion in the rendering of depth, it removes hidden parts of surfaces, and it also eliminates a type of artifact that we call an “echo.” Random dot stereograms have one remaining problem: difficulty of initial viewing. If a computer screen rather than paper is used for output, the problem can be ameliorated by shimmering, or time-multiplexing of pixel values. We also describe a simple computational technique for determining what is present in a stereogram so that, if viewing is difficult, one can ascertain what to look for."
10.1.1.133.6667,An overview of data warehousing and OLAP technology,1997,"Data warehousing and on-line analytical processing (OLAP) are essential elements of decision support, which has increasingly become a focus of the database industry. Many commercial products and services are now available, and all of the principal database management system vendors now have offerings in these areas. Decision support places some rather different requirements on database technology compared to traditional on-line transaction processing applications. This paper provides an overview of data warehousing and OLAP technologies, with an emphasis on their new requirements. We describe back end tools for extracting, cleaning and loading data into a data warehouse; multidimensional data models typical of OLAP; front end client tools for querying and data analysis; server extensions for efficient query processing; and tools for metadata management and for managing the warehouse. In addition to surveying the state of the art, this paper also identifies some promising research issues, some of which are related to problems that the database research community has worked on for years, but others are only just beginning to be addressed. This overview is based on a tutorial that the authors presented at the VLDB Conference, 1996. 1."
10.1.1.133.6460,and Reasoning about Programs,,"In this paper, we present a new algorithm for partial program verification that runs in polynomial time and space. We are interested in checking that a program satisfies a given temporal safety property. Our insight is that by accurately modeling only those branches in a program for which the property-related behavior differs along the arms of the branch, we can design an algorithm that is accurate enough to verify the program with respect to the given property, without paying the potentially exponential cost of full pathsensitive analysis. We have implemented this “property simulation ” algorithm as part of a partial verification tool called ESP. We present the results of applying ESP to the problem of verifying the file I/O behavior of a version of the GNU C compiler (gcc, 140,000 LOC). We are able to prove that all of the 646 calls to fprintf in the source code of gcc are guaranteed to print to valid, open files. Our results show that property simulation scales to large programs and is accurate enough to verify meaningful properties."
10.1.1.133.4474,A Mechanical Verification of the Alternating Bit Protocol,1981,"The Alternating Bit Protocol has been modeled via a straighforward application of the Gypsy methodology. A safety property was stated for its service specification and a procedural protocol specification was written using Gypsy procedure definitions. Mechanical verification was carried out, including proofs of the supporting lemmas. A unique aspect of this verification effort is the cooperative proof strategy that was employed, making use of two separate verification systems. The combined capabilities of both the Gypsy system and the Affirm system were utilized to achieve this result. 1."
10.1.1.133.4113,CyberCut: An Internet-based CAD/CAM System,,"“CyberCut TM ” is a testbed for an Internet-based CAD/CAM system. It was specifically designed to be a networked, automated system, with a seamless communication flow from a client-side designer to a server-side machining service. The creation of CyberCut required several new software modules. These include: a) a Web-based design tool in which Design-for-Manufacturing information and machining rules constrain the designer to manufacturable parts; b) a geometric representation called SIF-DSG, for unambiguous communication between the client-side designer and the server-side process planner; c) an automated process planning system with several sub-modules that convert an incoming design to a set of tool-paths for execution on a 3-axis CNC milling machine. Using this software-pipeline, a CyberCut service, modeled on the MOSIS service for VLSI chips, has been now been launched for limited student-use at a group of cooperating universities."
10.1.1.131.559,1. Motivation: Implementation of Target Recognition Applications Using Pipelined Reconfigurable Hardware,,"Intelligence, Surveillance and Reconnaissance (ISR) systems present unique challenges to digital designers. Computational hardware for ISR systems needs to be able to process large amounts of data, often in real time, while meeting stringent physical constraints. These data processing requirements are increasing rapidly as new sensors come online and increasing amounts of automation are desired. Commodity processors can be used in some cases, but their power requirements are high and they are often very inefficient for the highly parallel and repetitive algorithms common in ISR applications. ASICs can be used to meet performance and physical requirements in many cases, but they also have many drawbacks, especially high design cost, long design turnaround time, and inflexibility. Programmable logic devices (PLDs), such as FPGAs, are increasingly being used in place of ASICs because of their short design cycle, performance, and flexibility. However, implementing applications on an FPGA can be difficult and requires hardware design skills. FPGAs are also expensive and take a long time to be reconfigured for different applications. Further, FPGA designs are not scalable. An application designed for one particular size FPGA cannot be run on a FPGA with more resources and get"
10.1.1.131.4580,Using Prior Knowledge with Adaptive Probing,,"When searching a tree to find the best leaf, complete search methods such as depth-first search and depth-bounded discrepancy search use a fixed deterministic order that may or may not be appropriate for the tree at hand. Adaptive probing is a recently-proposed stochastic method that attempts to adjust its sampling on-line to focus on areas of the tree that seem to contain good solutions. While effective on a variety of trees, adaptive probing wastes time learning basic features of the problem that are built into other algorithms, such as the fact that the heuristic is often helpful. In this paper, we investigate two simple methods for adding such prior knowledge to adaptive probing. The first simply reuses the model learned during a previous run on a similar problem. The second uses a heuristically biased policy at the start of the search, gradually deferring to learned information in later iterations. Empirical results on two different representations of number partitioning confirm that these methods can allow adaptive probing to search efficiently from the very start of a run. However, reusing previous models seems to more frequently preserve the ability of the algorithm to adapt to the search space."
10.1.1.130.4585,Formal proofs for the security of signcryption,2002,"Abstract. Signcryption is a public key or asymmetric cryptographic method that provides simultaneously both message confidentiality and unforgeability at a lower computational and communication overhead. In this paper, we propose a sound security model for signcryption that admits rigorous formal proofs for the confidentiality and unforgeablity of signcryption. A conclusion that comes out naturally from this work is that, as an asymmetric encryption scheme, signcryption is secure against adaptive chosen ciphertext attack in the random oracle model relative to, quite interestingly, the Gap Diffie-Hellman problem, and as a digital signature scheme, signcryption is existentially unforgeable against adaptive chosen message attack in the random oracle model relative to the discrete logarithm problem. 1"
10.1.1.13.9903,Computer Security is Not a Science (but it should be),,"Introduction  Security research is sometimes referred to as the ""Humanities of Computer Science"" because, too frequently, ""secure"" systems are built using equal measures of folklore and black arts. Despite the humorous intention, there is a kernel of truth in this jest--- computer security, at least ""security in the large"", is not currently a science.  This claim may seem unfair, given the progress made in security over the past decades. However, our present tools and methodologies are at most adequate for understanding systems security on a small scale. Cryptography, for example, is perhaps the most thoroughly studied and most rigorously modeled aspect of security. Despite its tremendous importance, cryptography alone is not sufficient for building secure systems. Indeed, the vast majority of all security flaws arise because of faulty software (e.g., the ubiquitous buffer overflow problem). Such security holes cannot be avoided by cryptographic techniques, and despite widely known and"
10.1.1.13.7597,Thread Transparency in Information Flow Middleware,2001,"Applications that process continuous information flows are  challenging to write because the application programmer must deal with  flow-specific concurrency and timing requirements, necessitating the explicit  management of threads, synchronization, scheduling and timing."
10.1.1.13.619,Functional Abstraction driven Design Space Exploration of Heterogeneous Programmable Architectures,2001,"Rapid Design Space Exploration (DSE) of a programmable architecture is feasible using an automatic toolkit (compiler, simulator, assembler) generation methodology driven byan  Architecture Description Language (ADL). While many contemporary ADLs can effectively capture one class of architecture, they are typically unable to capture a wide spectrum of processor and memory features present in DSP, VLIW, EPIC and Superscalar processors. The main bottleneck has been the lack of an abstraction underlying the ADL (covering a diverse set of architectural features) that permits reuse of the abstraction primitives to compose the heterogeneous architectures. We present in this paper the functional abstraction needed to capture such wide varietyof programmable architectures. We illustrate the usefulness of this approachby specifying two very different architectures using functional abstraction. Our DSE results demonstrate the power of reuse in composing heterogeneous architectures using functional abstraction primitives allowing for a reduction in the time for specification and exploration by at least an order of magnitude."
10.1.1.13.4427,Formal Proofs for the Security of Signcryption,2002,Signcryption is a public key or asymmetric cryptographic  method that provides simultaneously both message confidentiality and  unforgeability at a lower computational and communication overhead.
10.1.1.129.8583,PESA: Phrase Pair Extraction as Sentence Splitting,2005,"Most statistical machine translation systems use phrase-to-phrase translations to capture local context information, leading to better lexical choice and more reliable local reordering. The quality of the phrase alignment is crucial to the quality of the resulting translations. Here, we propose a new phrase alignment method, not based on the Viterbi path of word alignment models. Phrase alignment is viewed as a sentence splitting task. For a given spitting of the source sentence (source phrase, left segment, right segment) find a splitting for the target sentence, which optimizes the overall sentence alignment probability. Experiments on different translation tasks show that this phrase alignment method leads to highly competitive translation results. 1"
10.1.1.129.6712,The dangers of replication and a solution,1996, Update anywhere-anytime-anyway transactional replication has unstable behavior as the workload scales up: a ten-fold increase in nodes and traffi c gives a thousand fold increase in deadlocks or reconciliations. Master copy replica-tion (primary copyj schemes reduce this problem. A simple analytic model demonstrates these results. A new two-tier replication algorithm is proposed that allows mobile (disconnected) applications to propose tentative update transactions that are later applied to a master copy. Commutative update transactions avoid the instability of other replication schemes. 
10.1.1.129.3184,Symbolic Computational Techniques for Solving Games,,Software Tools for Technology Transfer manuscript No. (will be inserted by the editor)
10.1.1.129.1554,NuMA influences higher order chromatin organization in human mammary epithelium,2007,"The coiled-coil protein NuMA is an important contributor to mitotic spindle formation and stabilization. A potential role for NuMA in nuclear organization or gene regulation is suggested by the observations that its pattern of nuclear distribution depends upon cell phenotype and that it interacts and/or colocalizes with transcription factors. To date, the precise contribution of NuMA to nuclear function remains unclear. Previously, we observed that antibody-induced alteration of NuMA distribution in growth-arrested and differentiated mammary epithelial structures (acini) in threedimensional culture triggers the loss of acinar differentiation. Here, we show that in mammary epithelial cells, NuMA is present in both the nuclear matrix and chromatin compartments. Expression of a portion of the C terminus of NuMA that shares sequence similarity with the chromatin regulator HPC2 is sufficient to inhibit acinar differentiation and results in the redistribution of NuMA, chromatin markers acetyl-H4 and H4K20m, and regions of deoxyribonuclease I-sensitive chromatin compared with control cells. Short-term alteration of NuMA distribution with anti-NuMA C-terminus antibodies in live acinar cells indicates that changes in NuMA and chromatin organization precede loss of acinar differentiation. These findings suggest that NuMA has a role in mammary epithelial differentiation by influencing the organization of chromatin."
10.1.1.128.9063,'One is a Lonely Number': on the logic of communication,2002,"Logic is not just about single-agent notions like reasoning, or zero-agent notions like truth, but also about communication between two or more people. What we tell and ask each other can be just as 'logical' as what we infer in Olympic solitude. We show how such interactive phenomena can be studied systematically by merging epistemic and dynamic logic.  "
10.1.1.128.4185,NuMA influences higher order chromatin organization in human mammary epithelium,2007,"The coiled-coil protein NuMA is an important contributor to mitotic spindle formation and stabilization. A potential role for NuMA in nuclear organization or gene regulation is suggested by the observations that its pattern of nuclear distribution depends upon cell phenotype and that it interacts and/or colocalizes with transcription factors. To date, the precise contribution of NuMA to nuclear function remains unclear. Previously, we observed that antibody-induced alteration of NuMA distribution in growth-arrested and differentiated mammary epithelial structures (acini) in threedimensional culture triggers the loss of acinar differentiation. Here, we show that in mammary epithelial cells, NuMA is present in both the nuclear matrix and chromatin compartments. Expression of a portion of the C terminus of NuMA that shares sequence similarity with the chromatin regulator HPC2 is sufficient to inhibit acinar differentiation and results in the redistribution of NuMA, chromatin markers acetyl-H4 and H4K20m, and regions of deoxyribonuclease I-sensitive chromatin compared with control cells. Short-term alteration of NuMA distribution with anti-NuMA C-terminus antibodies in live acinar cells indicates that changes in NuMA and chromatin organization precede loss of acinar differentiation. These findings suggest that NuMA has a role in mammary epithelial differentiation by influencing the organization of chromatin."
10.1.1.128.4116,● VHDL-200X is being developed in a time phased effort.,2004,"With all the media hype about languages such as Verilog/SystemVerilog, Vera, and specman e where does the future of VHDL lie? 2003 has heard many claims about VHDL being &quot;The New Latin &quot; and it is dead. Fortunately these claims were made by people or companies who have very little interest (or market share) in VHDL and are looking to push the market in their direction. The VHDL designer and vendor community are actively working on revisions to both VHDL and the packages that support the language. These revisions are integrating the newest features of verification languages and assertion languages as well as adding features such as a programming language interface (VHPI) and a simulation control interface. In addition, changes are being made to improve performance and ease of use. To ensure wide support of the new features, EDA vendors are being actively engaged to participate in the standardization efforts and only features that have both designer and vendor support will be integrated into the language."
10.1.1.126.4776,QCDOC: A 10 Teraflops Computer for Tightly-coupled Calculations,2004,"Numerical simulations of the strong nuclear force, known as quantum chromodynamics or QCD, have proven to be a demanding, forefront problem in high-performance computing. In this report, we describe a new computer, QCDOC (QCD On a Chip), designed for optimal price/performance in the study of QCD. QCDOC uses a six-dimensional, low-latency mesh network to connect processing nodes, each of which includes a single custom ASIC, designed by our collaboration and built by IBM, plus DDR SDRAM. Each node has a peak speed of 1 Gigaflops and two 12,288 node, 10+ Teraflops machines are to be completed in the fall of 2004. Currently, a 512 node machine is running, delivering efficiencies as high as 45 % of peak on the conjugate gradient solvers that dominate our calculations and a 4096-node machine with a cost of $1.6M is under construction. This should give us a price/performance less than $1 per sustained Megaflops. 1"
10.1.1.126.230,"Effect of Genre, Speaker, and Word Class on the Realization of Given and New Information",,"There is much evidence in the literature that speakers tend to deaccent discourse-given entities, while accenting new ones. However, speakers do not always follow this simple strategy and the causes for such variation are not yet well understood. In this paper, we describe several new forms of variability in the relationship between given/new information and accenting behavior, variation due to individual differences and to word class. We present results indicating that different speakers have different strategies for making new words prominent. We analyze two word-classes – nouns and verbs – in a corpus of spontaneous and read direction-giving monologues, and show that speakers use different combinations of pitch, intensity and inter-word pauses to distinguish between given and new information. Most interestingly, we find that in both genres all speakers tend to produce given verbs with higher intensity than new verbs. Index Terms: prosody, information status, given/new information, accenting."
10.1.1.126.1146,Mitigating Soft Error Failures for Multimedia Applications by Selective Data Protection ∗,,"With advances in process technology, soft errors (SE) are becoming an increasingly critical design concern. Owing to their large area and high density, caches are worst hit by soft errors. Although Error Correction Code based mechanisms protect the data in caches, they have high performance and power overheads. Since multimedia applications are increasingly being used in mission-critical embedded systems where both reliability and energy are a major concern, there is a definite need to improve reliability in embedded systems, without too much energy overhead. We observe that while a soft error in multimedia data may only result in a minor loss in QoS, a soft error in a variable that controls the execution flow of the program may be fatal. Consequently, we propose to partition the data space into failure critical"
10.1.1.125.9986,ABSTRACT A New Approach To Real-Time Checkpointing,,"The progress towards programming methodologies that simplify the work of the programmer involves automating, whenever possible, activities that are secondary to the main task of designing algorithms and developing applications. Automatic memory management, using garbage collection, and automatic persistence, using checkpointing, are both examples of mechanisms that operate behind the scenes, simplifying the work of the programmer. Implementing such mechanisms in the presence of real-time constraints, however, is particularly difficult. In this paper we review the behavior of traditional copy-on-write implementations of checkpointing in the context of real-time systems, and we show how such implementations may, in pathological cases, seriously impair the ability of the user code to meet its deadlines. We discuss the source of the problem, supply benchmarks, and discuss possible remedies. We subsequently propose a novel approach that does not rely on copy-on-write and that, while more expensive in terms of CPU time overhead, is unaffected by pathological user code. We also describe our implementation of the proposed solution, based on the Ovm RTSJ Java Virtual Machine, and we discuss our experimental results."
10.1.1.125.3564,The internet worm incident,1991,"... worm program. That program exploited flaws in utility programs in systems based on BSD-derived versions of UNIX. The flaws allowed the program to break into those machines and copy itself, thus infecting those systems. This program eventually spread to thousands of machines, and disrupted normal activities and Internet connectivity for many days. This paper explains why this program was a worm (as opposed to a virus), and provides a brief chronology of both the spread and eradication of the program. That is followed by discussion of some specific issues raised by the community’s reaction and subsequent discussion of the event. Included are some interesting lessons learned from"
10.1.1.125.2602,Qualitative Reasoning Group,,"The finite representation of infinite behaviors provided by total envisionments has been shown to be useful for tasks such as model development, explanation, and monitoring. Unfortunately, generating total envisionments of complex systems can be intractable, or at least excessive for highly-focused tasks. Such shortcomings have encouraged some researchers to favor alternative schemes, such as attainable envisioning and history generation. However, we argue in this paper for an incremental means of envisioning that can realizemany of the practical advantages oftotal envisioningwhile supporting more-focused search to address issues of tractability and relevance. In this paper, we describe our theory of incremental envisioning and compare it with QPC [3] and QPE [9]. We emphasize how IQE attempts to reason about states at low (i.e., abstract levels of detail when that is sufficient for the overall task. Such reasoning helps avoid the inefficiency of making state distinctions that are irrelevant for the overall task- a problemcommon among most other current qualitative simulators, including QPC and QPE. 1"
10.1.1.124.4029,Mitigating Soft Error Failures for Multimedia Applications by Selective Data Protection ,2006,"With advances in process technology, soft errors (SE) are becoming an increasingly critical design concern. Due to their large area and high density, caches are worst hit by soft errors. Although Error Correction Code based mechanisms protect the data in caches, they have high performance and power overheads. Since multimedia applications are increasingly being used in mission-critical embedded systems where both reliability and energy are a major concern, there is a definite need to improve reliability in embedded systems, without too much energy overhead. We observe that while a soft error in multimedia data may only result in a minor loss in QoS, a soft error in a variable that controls the execution flow of the program may be fatal. Consequently, we propose to partition the data space into failure critical and"
10.1.1.123.7551,AUTOMATIC GEOMETRIC AND RADIOMETRIC REGISTRATION OF LANDSAT-TM IMAGES USING MUTUAL INFORMATION,,"Abstract: This work is on development of a method for automatic registration of satellite images acquired on different dates, for both geometric and radiometric correction with respect to a reference image. Mutual information statistics is used as the similarity metric of geometric and radiometric registration. Affine and linear transformations are used in geometric and radiometric correction respectively. Powell's method is applied in iterative optimization to find the best transformation parameters for both types of registration, based on the maximum mutual information between images. The method is validated using Landsat's Thematic Mapper (TM) sensor, bands 3, 4 and 5 images, obtained on five separate dates for scene 231-062 in the Central Amazon. Key words: radiometric registration, geometric registration, mutual information. 1."
10.1.1.122.8989,V4 DRAFT 1/9/05,2005,"self-inflicted credibility problem. What is your profession? Computer science. Oh? Is that a science? Sure, it is the science of information processes and their interactions with the world. I’ll accept that what you do is technology; but not science. Science deals with fundamental laws of nature. Computers are man-made. Their principles come from other fields such as physics and electronics engineering. Hold on. There are many natural information processes. Computers are tools to implement, study, and predict them. In North America alone, nearly 200 academic departments recognize this; some have been granting CS degrees for 40 years. They are wrong. But it’s not your fault. The pioneers of your field genuinely believed in the 1950s that their new field was science. They were mistaken. There is no"
10.1.1.122.1668,1 Introduction Code Checking and Visualization of an Architecture Design,,"Computer graphics has be successfully applied to architecture design. There is more demand to new applications. One of them, to be addressed in this work, is"
10.1.1.121.9958,Accepted dd/mm/yyyy,,"This paper presents an overview of our work towards building humanoid robots that can work alongside people as cooperative teammates. We present our theoretical framework based on a novel combination of Joint Intention Theory and Collaborative Discourse Theory, and demonstrate how it can be applied to allow a human to work cooperatively with a humanoid robot on a joint task using speech, gesture, and expressive cues. Such issues must be addressed to enable many new and exciting applications for humanoid robots that require them assist ordinary people in daily activities or to work as capable members of human-robot teams. Keywords: Human-robot interaction; teamwork; dialog and gesture; collaboration; social robots. 1."
10.1.1.121.8750,Ten benchmark database queries for location-based services,2003,"Location-based services (l-services for short) compose an emerging application involving spatiotemporal databases. In this paper, we discuss this type of application, in terms of database requirements, and provide a set of ten benchmark database queries (plus two operations for loading and updating data). The list includes selection queries on stationary and moving reference objects, join queries and unary operations on trajectories of moving objects. We also survey recent work in query processing for those query types, with emphasis on indexing of moving objects, and suggest candidates for efficiently supporting databases for l-services."
10.1.1.121.7316,Adaptive Routing for Intermittently Connected Mobile Ad Hoc Networks,2005,"The vast majority of mobile ad hoc networking research makes a very large assumption: that communication can only take place between nodes that are simultaneously accessible within in the same connected cloud (i.e., that communication is synchronous). In reality, this assumption is likely to be a poor one, particularly for sparsely or irregularly populated environments. In this paper, we present the Context-Aware Routing (CAR) algorithm. CAR is a novel approach to the provision of asynchronous communication in partially-connected mobile ad hoc networks, based on the intelligent placement of messages. We discuss the details of the algorithm, and then present simulation results demonstrating that it is possible for nodes to exploit context information in making local decisions that lead to good delivery ratios and latencies with small overheads. 1"
10.1.1.120.8615,– Personal statement,,– What do you want to do? • Research universities based on your goals – Academic reputation – Quality of the specific program in your area of interest
10.1.1.120.8540,– Personal statement,,– What do you want to do? • Research universities based on your goals – Academic reputation – Quality of the specific program in your area of interest
10.1.1.120.6524,"Montana smart pointers: They're smart, and they're pointers",1997,"The Montana C++ programming environment provides an API interface to the compiler, which allows the compilation process to be extended through programmersupplied tools. This paper investigates the feasibility of that interface, using smart pointers as an example. Smart pointers are a powerful feature of the C++ language that enable a variety of applications, such as garbage collection, persistence, and distributed objects. However, while smart pointers can be used in much the same way as built-in pointers, they are not interchangeable. Using the Montana API, smart pointer functionality can be introduced for built-in pointers, thus enabling built-in pointers that act like smart pointers. We provide an overview of the Montana programming environment and describes how smart pointers can be implemented using the Montana API. Acknowledgements I would like to thank my supervisor, Dr. Jacob Slonim, and Dr. Gordon Cormack, for their time and energy spent in providing guidance, advice, and comments throughout"
10.1.1.120.3110,Efficient Blind Search: Optimal Power of Detection under Computational Cost Constraints,2007,"Some astronomy projects require a blind search through a vast number of hypotheses to detect objects of interest. The number of hypotheses to test can be in the billions. A naive blind search over every single hypothesis would be far too costly computationally. We propose a hierarchical scheme for blind search, using various ‘reso-lution ’ levels. At lower resolution levels, ‘regions ’ of interest in the search space are singled out with a low computational cost. These regions are refined at intermediate resolution levels and only the most promising candidates are finally tested at the original fine resolution. The optimal search strategy is found by dynamic programming. We demonstrate the procedure for pulsar search from satellite gamma-ray observations and show that the power of the naive blind search can almost be matched with the hierarchical scheme while reducing the computational burden by more than three orders of magnitude. 1 1"
10.1.1.119.9102,Review Methodologies for target selection in structural genomics Abstract,,"As the number of complete genomes that have been sequenced keeps growing, unknown areas of the protein space are revealed and new horizons open up. Most of this information will be fully appreciated only when the structural information about the encoded proteins becomes available. The goal of structural genomics is to direct large-scale efforts of protein structure determination, so as to increase the impact of these efforts. This review focuses on current approaches in structural genomics aimed at selecting representative proteins as targets for structure determination. We will discuss the concept of representative structures/folds, the current methodologies for identifying those proteins, and computational techniques for identifying proteins which are expected to adopt new structural folds. # 2000 Elsevier Science Ltd. All rights reserved."
10.1.1.117.5978,MEAD - a platform for multidocument multilingual text summarization,2004,"This paper describes the functionality of MEAD, a comprehensive, public domain, open source, multidocument multilingual summarization environment that has been thus far downloaded by more than 500 organizations. MEAD has been used in a variety of summarization applications ranging from summarization for mobile devices to Web page summarization within a search engine and to novelty detection. 1."
10.1.1.117.3806,Analyzing stripped device-driver executables,2008,"Abstract. This paper sketches the design and implementation of Device-Driver Analyzer for x86 (DDA/x86), a prototype analysis tool for finding bugs in stripped Windows device-driver executables (i.e., when neither source code nor symbol-table/debugging information is available), and presents a case study. DDA/x86 was able to find known bugs (previously discovered by source-code-based analysis tools) along with useful error traces, while having a reasonably low false-positive rate. This work represents the first known application of automatic program verification/analysis to stripped industrial executables, and allows one to check that an executable does not violate known API usage rules (rather than simply trusting that the implementation is correct). 1"
10.1.1.117.3285,"A Secure, Publisher-Centric Web Caching Infrastructure",,"The current web caching infrastructure, though it has a number of performance benefits for clients and network providers, does not meet publishers’ requirements. We argue that to satisfy these requirements, caches should be enhanced in both the data and control planes. In the data plane, caches will dynamically generate content for clients by running code provided by publishers. In the control plane, caches will return logs of client accesses to publishers. In this paper, we introduce Gemini, a system which has both of these capabilities, and discuss two of its key components: security and incremental deployment. Since Gemini caches are deeply involved in content preparation and logging, ensuring that they perform correctly is vital. Traditional end-to-end security mechanisms are not sufficient to protect clients and publishers, so we introduce a new security model which consists of two pieces: an authorization mechanism and a verification mechanism. The former allows a publisher to authorize a set of caches to run its code and serve its content, while the latter allows clients and publishers to probabilistically verify that authorized caches are operating correctly. Because it is unrealistic to assume that Gemini caches will be deployed everywhere simultaneously, we have designed the system to be incrementally deployable and to coexist with legacy clients, caches, and servers. Finally, we describe our implementation of Gemini and present preliminary performance results. I."
10.1.1.116.8518,Eigenvalues of random power law graphs,2003,"Many graphs arising in various information networks exhibit the “power law ” behavior – the number of vertices of degree k is proportional to k −β for some positive β. We show that if β>2.5, the largest eigenvalue of a random power law graph is almost surely (1 + o(1))  √ m where m is the maximum degree. Moreover, the k largest eigenvalues of a random power law graph with exponent β have power law distribution with exponent 2β − 1 if the maximum degree is sufficiently large, where k is a function depending on β,m and d, the average degree. When 2 <β<2.5, the largest eigenvalue is heavily concentrated at cm 3−β for some constant c depending on β and the average degree. This result follows from a more general theorem which shows that the largest eigenvalue of a random graph with a given expected degree sequence is determined by m, the maximum degree, and ˜ d, the weighted average of the squares of the expected degrees. We show that the k-th largest eigenvalue is almost surely (1 + o(1))  √ m k where mk is the k-th largest expected degree provided mk is large enough. These results have implications on the usage of spectral techniques in many areas related to pattern detection and information retrieval. 1"
10.1.1.116.4564,"Montana smart pointers: They're smart, and they're pointers",1997,"Abstract: The Montana C++ programming environment provides an API interface to the compiler, which allows the compilation process to be extended through programmer-supplied tools. This paper investigates the feasibility of that interface, using smart pointers as an example. Smart pointers are a powerful feature of the C++ language that enable a variety of applications, such as garbage collection, persistence, and distributed objects. However, while smart pointers can be used in much the same way as built-in pointers, they are not interchangeable. Using the Montana API, smart pointer functionality can be introduced for built-in pointers, thus enabling built-in pointers that act like smart pointers. We provide an overview of the Montana programming environment and describes how smart pointers can be implemented using the Montana API."
10.1.1.116.1182,The dangers of replication and a solution,1996,Update anywhere-anytime-anyway transactional replication has unstable behavior as the workload scales up: a ten-fold increase in nodes and traflc gives a thousand fold increase in deadlocks or reconciliations. Master copy replication (primary copyj schemes reduce this problem. A simple analytic model demonstrates these results. A new two-tier replication algorithm is proposed that allows mobile (disconnected) applications to propose tentative update transactions that are later applied to a master copy. Commutative update transactions avoid the instability of other replication schemes.
10.1.1.115.5442,Trust-based mechanism design,2004,"developed powerful tools for analyzing decision making in systems with multiple autonomous actors. These tools, when tailored to computational settings, provide a foundation for building multiagent software systems. This tailoring gives rise to the field of computational-mechanism design, which applies economic principles to computer systems design."
10.1.1.115.3869,PostgreSQL 7.1 Programmer’s Guide The PostgreSQL Global Development Group PostgreSQL 7.1 Programmer’s Guide by The PostgreSQL Global Development Group Copyright © 1996-2001 by PostgreSQL Global Development Group Legal Notice,,is Copyright © 1996-2001 by the PostgreSQL Global Development Group and is distributed under the terms of the license of the University of
10.1.1.114.4368,Optimistic Active Learning using Mutual Information,,"An “active learning system ” will sequentially decide which unlabeled instance to label, with the goal of efficiently gathering the information necessary to produce a good classifier. Some such systems greedily select the next instance based only on properties of that instance and the few currently labeled points — e.g., selecting the one closest to the current classification boundary. Unfortunately, these approaches ignore the valuable information contained in the other unlabeled instances, which can help identify a good classifier much faster. For the previous approaches that do exploit this unlabeled data, this information is mostly used in a conservative way. One common property of the approaches in the literature is that the active learner sticks to one single query selection criterion in the whole process. We propose a system, MM+M, that selects the query instance that is able to provide the maximum conditional mutual information about the labels of the unlabeled instances, given the labeled data, in an optimistic way. This approach implicitly exploits the discriminative partition information contained in the unlabeled data. Instead of using one selection criterion, MM+M also employs a simple on-line method that changes its selection rule when it encounters an “unexpected label”. Our empirical results demonstrate that this new approach works effectively. 1"
10.1.1.113.7513,Abstract Type-Based Race Detection for Java,,"This paper presents a static race detection analysis for multithreaded Java programs. Our analysis is based on a formal type system that is capable of capturing many common synchronization patterns. These patterns include classes with internal synchronization, classes that require client-side synchronization, and thread-local classes. Experience checking over 40,000 lines of Java code with the type system demonstrates that it is an effective approach for eliminating races conditions. On large examples, fewer than 20 additional type annotations per 1000 lines of code were required by the type checker, and we found a number of races in the standard Java libraries and other test programs. 1"
10.1.1.113.5719,Securing distributed systems with information flow control,2006,"decentralized information flow control (DIFC) can secure applications built from mostly untrusted code. This paper extends DIFC to the network. We present DStar, a system that enforces the security requirements of mutually distrustful components through cryptography on the network and local OS protection mechanisms on each host. DStar does not require any fully-trusted processes or machines, and is carefully constructed to avoid covert channels inherent in its interface. We use DStar to build a three-tiered web server that mitigates the effects of untrustworthy applications and compromised machines. 1"
10.1.1.113.3824,Exploratory study of a new model of evolving networks,2006,"The study of social networks has gained new importance with the recent rise of large online communities. Most current approaches focus on deterministic (descriptive) models and are usually restricted to a preset number of people. Moreover, the dynamic aspect is often treated as an addendum to the static model. Taking inspiration from reallife friendship formation patterns, we propose a new generative model of evolving social networks that allows for birth and death of social links and addition of new people. Each person has a distribution over social interaction spheres, which we term ”contexts. ” We study the robustness of our model by examining statistical properties of simulated networks relative to well known properties of real social networks. We discuss the shortcomings of this model and problems that arise during learning. Several extensions are proposed. 1."
10.1.1.112.816,“access/refractory ” and “degraded-store”,,semantic impairments
10.1.1.112.5553,On flat-state connectivity of chains with fixed acute angles,2002,"We prove that two classes of fixed-angle, open chains with acute angles are “flat-state connected. ” A chain is flatstate connected if it can be reconfigured between any two of its planar realizations without self-crossing. In a companion paper (under preparation) [ADD +], several fixed-angle linkages will be proved flat-state connected or disconnected. In particular, all orthogonal or obtuse-angle open chains are flat-state connected. But it remains open whether this holds for acute-angle open chains. In this paper, we prove that two classes of such chains are indeed flat-state connected: those with equal acute angles, and those with equal edge lengths and angles in (60 ◦ , 90 ◦]. We claim, but do not prove, an extension of the latter result to the range [45 ◦ , 90 ◦ ] without length restriction. 1"
10.1.1.112.3446,Paravirtualization for HPC Systems,2006," In this work, we investigate the efficacy of using paravirtualizing software for performance-critical HPC kernels and applications. We present a comprehensive performance evaluation of Xen, a low-overhead, Linux-based, virtual machine monitor, for paravirtualization of HPC cluster systems at LLNL. We investigate subsystem and overall performance using a wide range of benchmarks and applications. We employ statistically sound methods to compare the performance of a paravirtualized kernel against three Linux operating systems: RedHat Enterprise 4 for build versions 2.6.9 and 2.6.12 and the LLNL CHAOS kernel. Our results indicate that Xen is very efficient and practical for HPC systems. "
10.1.1.112.2448,Joint nonparametric alignment for analyzing spatial gene expression patterns in drosophila imaginal discs,,"To compare spatial patterns of gene expression, one must analyze a large number of images as current methods are only able to measure a small number of genes at a time. Bringing images of corresponding tissues into alignment is a critical first step in making a meaningful comparative analysis of these spatial patterns. Significant image noise and variability in the shapes make it hard to pick a canonical shape model. In this paper, we address these problems by combining segmentation and unsupervised shape learning algorithms. We first segment images to acquire structures of interest, then jointly align the shapes of these acquired structures using an unsupervised nonparametric maximum likelihood algorithm along the lines of ‘congealing’ [12], while simultaneously learning the underlying shape model and associated transformations. The learned transformations are applied to corresponding images to bring them into alignment in one step. We demonstrate the results for images of various classes of Drosophila imaginal discs and discuss the methodology used for a quantitative analysis of spatial gene expression patterns. 1."
10.1.1.112.2074,Traceability in a Collaborative Systems Development from Lifecycle Perspective,2002,"The aim of this position paper is to discuss the features of state-of-the-art and outstanding issues of the traceability between product fragments in collaborative system development. A lot of research has been done in the pre-traceability area. Recently, researchers ’ attitudes towards interrelation of requirements and architecture elements have increased. Several approaches to tackle this problem have been proposed. Nevertheless, to the author’s knowledge, the solution for traceability between various product fragments through the lifetime of the system does not exist. Central repository for the traceability relationship and distributed repositories for the model fragments storage and interchange between developers are proposed. Usage of ontology is proposed to interrelate different product fragments and establish the traceability relations between them. 1."
10.1.1.112.1050,1 Introduction Code Checking and Visualization of an Architecture Design,,"Computer graphics has be successfully applied to architecture design. There is more demand to new applications. One of them, to be addressed in this work, is"
10.1.1.111.9945,CU-TMP: Temporal Relation Classification Using Syntactic and Semantic Features,,"We approached the temporal relation identification tasks of TempEval 2007 as pair-wise classification tasks. We introduced a variety of syntactically and semantically motivated features, including temporal-logicbased features derived from running our Task B system on the Task A and C data. We trained support vector machine models and achieved the second highest accuracies on the tasks: 61 % on Task A, 75 % on Task B and 54 % on Task C. 1"
10.1.1.111.4196,Computing partial data cubes for parallel data warehousing applications,2001,"Abstract. In this paper, we focus on an approach to On Line Analytical Processing (OLAP) that is based on a database operator and data structure called the datacube. The datacube is a relational operator that is used to construct all possible views of an given data set. Efficient algorithms for computing the entire datacube — both sequentially and in parallel — have recently been proposed. However, due to space and time constraints, the assumption that all 2 d (where d = dimensions) views should be computed is often not valid in practice. As a result, algorithms for computing partial datacube are required. In this paper, we describe a parallel algorithm for computing partial datacubes and preliminary experimental results. 1"
10.1.1.111.4125,● VHDL-200X is being developed in a time phased effort.,2004,"With all the media hype about languages such as Verilog/SystemVerilog, Vera, and specman e where does the future of VHDL lie? 2003 has heard many claims about VHDL being &quot;The New Latin &quot; and it is dead. Fortunately these claims were made by people or companies who have very little interest (or market share) in VHDL and are looking to push the market in their direction. The VHDL designer and vendor community are actively working on revisions to both VHDL and the packages that support the language. These revisions are integrating the newest features of verification languages and assertion languages as well as adding features such as a programming language interface (VHPI) and a simulation control interface. In addition, changes are being made to improve performance and ease of use. To ensure wide support of the new features, EDA vendors are being actively engaged to participate in the standardization efforts and only features that have both designer and vendor support will be integrated into the language."
10.1.1.111.3397,Trust-based mechanism design,2004,"developed powerful tools for analyzing decision making in systems with multiple autonomous actors. These tools, when tailored to computational settings, provide a foundation for building multiagent software systems. This tailoring gives rise to the field of computational-mechanism design, which applies economic principles to computer systems design."
10.1.1.110.2611,Gaps in the differential forms spectrum on cyclic coverings ,2007," We are interested in the spectrum of the Hodge-de Rham operator on a Z-covering X over a compact manifold M of dimension n + 1. Let Σ be a hypersurface in M which does not disconnect M and such that M − Σ is a fundamental domain of the covering. If the cohomology group H n/2 (Σ) is trivial, we can construct for each N ∈ N a metric g = gN on M, such that the Hodge-de Rham operator on the covering (X, g) has at least N gaps in its (essential) spectrum. If H n/2 (Σ) � = 0, the same statement holds true for the Hodge-de Rham operators on p-forms provided p / ∈ {n/2, n/2 + 1}.  "
10.1.1.11.9004,"Misperception, Communication and Diversity",2002,"It is commonly agreed upon that misperception is  detrimental. However, misperception might have a  beneficial effect from a collective viewpoint when  individuals mispercept incoming information that promotes  a specific kind of behavior, which leads to an increase in  diversity. First, this paper proposes our hypothesis regarding  adaptive property of misperception based on the argument  of the relationship between misperception and behavioral  diversity, and the effects of communication on diversity."
10.1.1.11.7636,A Guide to Complexity Theory in Operations Research,1995,"It is a well-known fact that there exists an ever increasing number of problems for which,  despite the efforts of many inventive and persistent researchers, it seems virtually impossible to find  efficient algorithms. In this situation, the theory of computational complexity may provide helpful  insight into how probable the existence of such algorithms is at all. Unluckily, some of its concepts  can still be found to be used erroneously, if at all. For instance, it is a common misunderstanding that  any problem that generalizes an NP-complete problem is NP-complete or NP-hard itself; indeed any  such generalization could as well be exponential in the worst case, i.e. solvable with effort exponentially  increasing in the size of the instances attempted. In this work we develop the basic concepts of  complexity theory. While doing so, we aim at presenting the material in a way that emphasizes the  correspondences between the kind of problems considered in operations research and the formal  problem classes which are studied in complexity theory."
10.1.1.109.2101,Checking ownership and confinement properties,2002,"A number of formal proposals to manage aliasing in Java-like programming languages have been advanced over the last five years. Unfortunately, it is not clear how practical these proposals are, that is, how well they relate to the kinds of programs currently written in those languages. We have analysed heap dumps from a corpus of Java programs to identify their implicit aliasing structures, including object ownership, confinement, and uniqueness. Understanding the kinds of aliasing present in programs should help us to design formalisms to make explicit the kinds of aliasing implicit in object-oriented programs."
10.1.1.108.8478,Reference Ontologies — Application Ontologies: Either/Or or Both/And?,,"The distinction between reference ontologies and application ontologies crept rather unobtrusively into the recent literature on knowledge engineering. A lot of the discourse surrounding this distinction – notably, the one framing the workshop generating this collection of papers – suggests the two types of ontologies are in some sort of opposition to one another. Thus, Borge et al. [3] characterize reference ontologies (more recently, foundational ontologies) as rich, axiomatic theories whose focus is to clarify the intended meanings of terms used in specific domains. Application ontologies, by contrast, provide a minimal terminological structure to fit the needs of a specific community. Reflecting their minimal nature, Masolo et al. [7] refer to such ontologies as “lightweight ” ontologies. An application ontology can be lightweight in a second respect as well, namely, that it may not necessarily take the form of fully-fledged axiomatic theory. Rather, it might only be a taxonomy of the relevant domain, a division of the domain into a salient collection of classes, perhaps ordered by the subclass relation. Importantly, though, for an application ontology to “fit the needs of a specific community ” needn’t require representational accuracy. In the “worst ” case (from a reference ontology perspective), to fit the needs of a community is just to represent uncritically what people in that community think about the ontology’s domain."
10.1.1.107.4781,User Participation in Standards Setting,1998,"This paper explores the views of members of standards setting organisations in the field of electronic communications. It focuses in particular on their experiences of, and attitudes towards, user participation in standards setting."
10.1.1.106.769,An FPGA Design Flow for Reconfigurable Network-Based Multi-Processor Systems on Chip,,"Multi-Processor System on Chip (MPSoC) platforms are becoming increasingly more heterogeneous and are shifting towards a more communication-centric methodology. Networks on Chip (NoC) have emerged as the design paradigm for scalable on-chip communication architectures. As the system complexity grows, the problem emerges as how to design and instantiate such a NoC-based MPSoC platform in a systematic and automated way. In this paper we present an integrated flow to automatically generate a highly configurable NoC-based MPSoC for FPGA instantiation. The system specification is done on a high level of abstraction, relieving the designer of errorprone and time consuming work. The flow uses the state-ofthe-art Æthereal NoC, and Silicon Hive processing cores, both configurable at design- and run-time. We use this flow to generate a range of sample designs whose functionality has been verified on a Celoxica RC300E development board. The board, equipped with a Xilinx Virtex II 6000, also offers a huge number of peripherals, and we show how their insertion is automated in the design for easy debugging and prototyping. 1."
10.1.1.106.6333,Using phrasal patterns to identify discourse relations,2006,"This paper describes a system which identifies discourse relations between two successive sentences in Japanese. On top of the lexical information previously proposed, we used phrasal pattern information. Adding phrasal information improves the system's accuracy 12%, from 53 % to 65%. 1"
10.1.1.106.430,The chromatic number of random regular graphs,2004,"Abstract. Given any integer d ≥ 3, let k be the smallest integer such that d < 2k log k. We prove that with high probability the chromatic number of a random d-regular graph is k, k + 1, or k + 2. 1"
10.1.1.105.3558,1. Motivation: Implementation of Target Recognition Applications Using Pipelined Reconfigurable Hardware,,"Intelligence, Surveillance and Reconnaissance (ISR) systems present unique challenges to digital designers. Computational hardware for ISR systems needs to be able to process large amounts of data, often in real time, while meeting stringent physical constraints. These data processing requirements are increasing rapidly as new sensors come online and increasing amounts of automation are desired. Commodity processors can be used in some cases, but their power requirements are high and they are often very inefficient for the highly parallel and repetitive algorithms common in ISR applications. ASICs can be used to meet performance and physical requirements in many cases, but they also have many drawbacks, especially high design cost, long design turnaround time, and inflexibility. Programmable logic devices (PLDs), such as FPGAs, are increasingly being used in place of ASICs because of their short design cycle, performance, and flexibility. However, implementing applications on an FPGA can be difficult and requires hardware design skills. FPGAs are also expensive and take a long time to be reconfigured for different applications. Further, FPGA designs are not scalable. An application designed for one particular size FPGA cannot be run on a FPGA with more resources and get"
10.1.1.105.2500,Robust and efficient skeletal graphs,2000,"There has recently been significant interest in using representations based on abstractions of Blum’s skeleton into a graph, for qualitative shape matching. The application of these techniques to large databases of shapes hinges on the availability of numerical algorithms for computing the medial axis. Unfortunately, this computation can be extremely subtle. Approaches based on Voronoi techniques preserve topology, but heuristic pruning measures are introduced to remove unwanted edges. Methods based on Euclidean distance functions can localize skeletal points accurately, but often at the cost of altering the object’s topology. In this paper we introduce a new algorithm for computing subpixel skeletons which is robust and accurate, has low computational complexity, and preserves topology. The key idea is to measure the net outward flux of a vector field per unit area, and to detect locations where a conservation of energy principle is violated. This is done in conjunction with a thinning process applied in a rectangular lattice. We illustrate the approach with several examples of skeletal graphs for biological and man-made silhouettes. 1."
10.1.1.105.1942,Abstract Plutarch: An Argument for Network Pluralism,,"It is widely accepted that the current Internet architecture is insufficient for the future: problems such as address space scarcity, mobility and non-universal connectivity are already with us, and stand to be exacerbated by the explosion of wireless, ad-hoc and sensor networks. Furthermore, it is far from clear that the ubiquitous use of standard transport and name resolution protocols will remain practicable or even desirable. In this paper we propose Plutarch, a new internetworking architecture. It subsumes existing architectures such as that defined by the Internet Protocol suite, but makes explicit the heterogeneity that contemporary inter-networking schemes attempt to mask. To handle this heterogeneity, we introduce the notions of context and interstitial function, and describe a supporting architecture. We discuss the benefits, present some potential scenarios, and consider the research challenges posed. 1"
10.1.1.104.9511,QCDOC: A 10 Teraflops Computer for Tightly-coupled Calculations,2004,"Numerical simulations of the strong nuclear force, known as quantum chromodynamics or QCD, have proven to be a demanding, forefront problem in high-performance computing. In this report, we describe a new computer, QCDOC (QCD On a Chip), designed for optimal price/performance in the study of QCD. QCDOC uses a six-dimensional, low-latency mesh network to connect processing nodes, each of which includes a single custom ASIC, designed by our collaboration and built by IBM, plus DDR SDRAM. Each node has a peak speed of 1 Gigaflops and two 12,288 node, 10+ Teraflops machines are to be completed in the fall of 2004. Currently, a 512 node machine is running, delivering efficiencies as high as 45 % of peak on the conjugate gradient solvers that dominate our calculations and a 4096-node machine with a cost of $1.6M is under construction. This should give us a price/performance less than $1 per sustained Megaflops. 1"
10.1.1.104.611,V4 DRAFT 1/9/05,2005,"self-inflicted credibility problem. What is your profession? Computer science. Oh? Is that a science? Sure, it is the science of information processes and their interactions with the world. I’ll accept that what you do is technology; but not science. Science deals with fundamental laws of nature. Computers are man-made. Their principles come from other fields such as physics and electronics engineering. Hold on. There are many natural information processes. Computers are tools to implement, study, and predict them. In North America alone, nearly 200 academic departments recognize this; some have been granting CS degrees for 40 years. They are wrong. But it’s not your fault. The pioneers of your field genuinely believed in the 1950s that their new field was science. They were mistaken. There is no"
10.1.1.104.3108,Genetic Process Mining: A Basic Approach and its Challenges,2006,"Abstract. One of the aims of process mining is to retrieve a process model from a given event log. However, current techniques have problems when mining processes that contain non-trivial constructs and/or when dealing with the presence of noise in the logs. To overcome these problems, we try to use genetic algorithms to mine process models. The non-trivial constructs are tackled by choosing an internal representation that supports them. The noise problem is naturally tackled by the genetic algorithm because, per definition, these algorithms are robust to noise. The definition of a good fitness measure is the most critical challenge in a genetic approach. This paper presents the current status of our research and the pros and cons of the fitness measure that we used so far. Experiments show that the fitness measure leads to the mining of process models that can reproduce all the behavior in the log, but these mined models may also allow for extra behavior. In short, the current version of the genetic algorithm can already be used to mine process models, but future research is necessary to always ensure that the mined models do not allow for extra behavior. Thus, this paper also discusses some ideas for future research that could ensure that the mined models will always only reflect the behavior in the log."
10.1.1.104.2958,A Performance Monitoring Interface for OpenMP,2002,"Abstract. This paper reports on efforts to define a performance monitoring interface for OpenMP that merges the OMPI and POMP prototype interfaces developed in the past year. The primary goal is to define a clear and portable API for OpenMP that makes execution events visible to runtime monitoring tools, primarily tools for performance measurement. The proposed specification is presented, covering many relevant design issues and the result of discussions among the involved groups. Both successful convergence of ideas, leading to agreement on proposed specifications, as well as differences in opinion and remaining open issues are documented from our many discussions. The paper is intended to serve as a preliminary proposal for consideration by the OpenMP Architecture Review Board and recently formed Tools sub-committee. 1"
10.1.1.103.4081,ABSTRACT Boosting for Document Routing,,"RankBoost is a recently proposed algorithm for learning ranking functions. It is simple to implement and has strong justifications from computational learning theory. We describe the algorithm and present experimental results on applying it to the document routing problem. The first set of results applies RankBoost to a text representation produced using modern term weighting methods. Performance of RankBoost is somewhat inferior to that of a state-of-the-art routing algorithm which is, however, more complex and less theoretically justified than RankBoost. RankBoost achieves comparable performance to the state-of-the-art algorithm when combined with feature or example selection heuristics. Our second set of results examines the behavior of RankBoost when it has to learn not only a ranking function but also all aspects of term weighting from raw data. Performance is usually, though not always, less good here, but the term weighting functions implicit in the resulting ranking functions are intriguing, and the approach could easily be adapted to mixtures of textual and nontextual data."
10.1.1.101.1032,A Temporal Data Model for Multimedia Database Systems,1997,"Abstract: The presented data model is a novel approach for integrating temporal concepts into a multimedia database system. Multimedia objects are extended with the traditional time dimensions valid time and transaction time. In addition a new time dimension specifically tailored for multimedia data types is presented with semantics that are completely orthogonal to the already established time dimensions, valid time and transaction time, i.e., the model supports a 3D time for multimedia data. This new time dimension, the play time dimension, places the building blocks of multimedia data in a temporal structure for multimedia presentation. This model is currently being implemented in a MMDBS for distance education at UNIK, University of Oslo. 1."
10.1.1.100.8982,Some demonstrations of the effects of structural descriptions in mental imagery,1979,"A visual imagery task is presented which is beyond the limits of normal human ability, and some of the factors contributing to its difficulty are isolated by comparing the difficulty of related tasks. It is argu~,cl that complex objects are assigned hierarchical structural descriptions by being parsed into parts, each of which has its own local system of significant directions. Two quite different schemas for a wire-frame cube are used to illustrate this theory, and some striking perceptual differences to which they give rise are described. The difficulty of certain mental imagery tasks is shown to depend on which of the alternative structural descriptions of an object is used, and this is interpreted as evidence that structural descriptions are an important component of mental images. Finally, it is argued that analog transformations like mental folding involve changing the values of continuous variables in a structural description. 1."
10.1.1.100.8313,Traceability in a Collaborative Systems Development from Lifecycle Perspective,2002,"The aim of this position paper is to discuss the features of state-of-the-art and outstanding issues of the traceability between product fragments in collaborative system development. A lot of research has been done in the pre-traceability area. Recently, researchers ’ attitudes towards interrelation of requirements and architecture elements have increased. Several approaches to tackle this problem have been proposed. Nevertheless, to the author’s knowledge, the solution for traceability between various product fragments through the lifetime of the system does not exist. Central repository for the traceability relationship and distributed repositories for the model fragments storage and interchange between developers are proposed. Usage of ontology is proposed to interrelate different product fragments and establish the traceability relations between them. 1."
10.1.1.100.6547,Functional discrimination of gene expression patterns in terms of the gene ontology,2003,"The ever-growing amount of experimental data in molecular biology and genetics requires its automated analysis, by employing sophisticated knowledge discovery tools. We use an Inductive Logic Programming (ILP) learner to induce functional discrimination rules between genes studied using microarrays and found to be differentially expressed in three recently discovered subtypes of adenocarcinoma of the lung. The discrimination rules involve functional annotations from the Proteome HumanPSD database in terms of the Gene Ontology, whose hierarchical structure is essential for this task. While most of the lower levels of gene expression data (pre)processing have been automated, our work can be seen as a step toward automating the higher level functional analysis of the data. We view our application not just as a prototypical example of applying more sophisticated machine learning techniques to the functional analysis of genes, but also as an incentive for developing increasingly more sophisticated functional annotations and ontologies, that can be automatically processed by such learning algorithms. 1 Introduction and"
10.1.1.100.2647,A Performance Monitoring Interface for OpenMP,2002,"Abstract. This paper reports on efforts to define a performance monitoring interface for OpenMP that merges the OMPI and POMP prototype interfaces developed in the past year. The primary goal is to define a clear and portable API for OpenMP that makes execution events visible to runtime monitoring tools, primarily tools for performance measurement. The proposed specification is presented, covering many relevant design issues and the result of discussions among the involved groups. Both successful convergence of ideas, leading to agreement on proposed specifications, as well as differences in opinion and remaining open issues are documented from our many discussions. The paper is intended to serve as a preliminary proposal for consideration by the OpenMP Architecture Review Board and recently formed Tools sub-committee. 1"
10.1.1.10.8604,Managing Deadline Miss Ratio and Sensor Data Freshness in Real-Time Databases,2004,"The demand for real-time data services is increasing in many applications including e-commerce, agile manufacturing, and  telecommunications network management. In these applications, it is desirable to execute transactions within their deadlines, i.e., before  the real-world status changes, using fresh (temporally consistent) data. However, meeting these fundamental requirements is  challenging due to dynamic workloads and data access patterns in these applications. Further, transaction timeliness and data freshness  requirements may conflict. In this paper, we define average/transient deadline miss ratio and new data freshness metrics to let a  database administrator specify the desired quality of real-time data services for a specific application. We also present a novel QoS  management architecture for real-time databases to support the desired QoS even in the presence of unpredictable workloads and  access patterns. To prevent overload and support the desired QoS, the presented architecture applies feedback control, admission  control, and flexible freshness management schemes. A simulation study shows that our QoS-aware approach can achieve a near zero  miss ratio and perfect freshness, meeting basic requirements for real-time transaction processing. In contrast, baseline approaches fail  to support the desired miss ratio and/or freshness in the presence of unpredictable workloads and data access patterns."
10.1.1.1.9330,Integration of Advice in an Action-Selection Architecture,2002,"The introduction of a coach competition in the RoboCup2001 simulation league raised many questions concerning the development of a ""coachable"" team. This paper addresses the issues of dealing with conflicting advice and knowing when to listen to advice. An action-selection architecture is proposed to support the integration of advice into an agent's set of beliefs. The results from the coach competition are discussed and provide a basis for experiments. Results are provided to support the claim that the architecture is well-suited for such a task."
10.1.1.1.9247,Fully Dynamic Transitive Closure: Breaking Through The O(n²)  Barrier,2000,"In this paper we introduce a general framework for casting fully dynamic transitive closure into the problem of reevaluating polynomials over matrices. With this technique, we improve the best known bounds for fully dynamic transitive closure. In particular, we devise a deterministic algorithm for general directed graphs that achieves O(n²) amortized time for updates, while preserving unit worstcase cost for queries. In case of deletions only, our algorithm performs updates faster in O(n) amortized time. Our"
10.1.1.1.5567,GenIc: A Single Pass Generalized Incremental Algorithm for Clustering,2004,In this paper we introduce a new single pass clustering algorithm called GenIc designed with the objective of having low overall cost. We examine some of the properties of GenIc and compare it to windowed k-means. We also study its performance using experimental data sets obtained from network monitoring.
10.1.1.1.4174,Hoard: A Scalable Memory Allocator for Multithreaded Applications,2000,"Parallel, multithreaded C and C++ programs such as web servers, database managers, news servers, and scientific applications are becoming increasingly prevalent. For these applications, the memory allocator is often a bottleneck that severely limits program performance and scalability on multiprocessor systems. Previous allocators suffer from problems that include poor performance and scalability, and heap organizations that introduce false sharing. Worse, many allocators exhibit a dramatic increase in memory consumption when confronted with a producer-consumer pattern of object allocation and freeing. This increase in memory consumption can range from a factor of P (the number of processors) to unbounded memory consumption."
10.1.1.1.2490,Reference Ontologies — Application Ontologies:  Either/Or or Both/And?,2003,"The distinction between reference ontologies and application ontologies crept rather unobtrusively into the recent literature on knowledge engineering. A lot of the discourse surrounding this distinction – notably, the one framing the workshop generating this collection of papers – suggests the two types of ontologies are in some sort of opposition to one another. Thus, Borge et al. [3] characterize reference ontologies (more recently, foundational ontologies) as rich, axiomatic theories whose focus is to clarify the intended meanings of terms used in speciﬁc domains. Application ontologies, by contrast, provide a minimal terminological structure to ﬁt the needs of a speciﬁc community. 
Reﬂecting their minimal nature, Masolo et al. [7] refer to such ontologies as “lightweight” ontologies. An application ontology can be lightweight in a second respect as well, namely, that it may not necessarily take the form of fully-ﬂedged axiomatic theory. Rather, it might only be a taxonomy 
of the relevant domain, a division of the domain into a salient collection of classes, perhaps ordered by the subclass relation. Importantly, though, for an application ontology to “ﬁt the needs of a speciﬁc community” needn’t require representational accuracy. In the “worst” case (from a reference 
ontology perspective), to ﬁt the needs of a community is just to represent uncritically what people in that community think about the ontology’s domain. 
The preceding paragraph is perhaps a good ﬁrst cut, but it strikes me that the distinction between reference ontologies and application ontologies has not been clearly made. To clarify the distinction and its signiﬁcance is my goal in this short paper. I conclude that, in fact, the distinction is not really 
an opposition; rather, reference ontologies and application ontologies reﬂect different aspects of a single methodology for ontology development. "
